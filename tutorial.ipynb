{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[ 0.1784, -0.6993,  0.1194],\n",
      "        [-0.3298, -0.3424,  0.1192],\n",
      "        [ 0.4631, -0.5193, -0.3089]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征映射到新的特征空间。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[-0.5624,  0.0032,  0.5491,  ...,  1.1661,  0.5079, -0.2542],\n",
      "         [-0.1031,  0.2404,  0.3230,  ...,  0.1743, -0.7406, -0.0555],\n",
      "         [-0.2318, -0.0089,  1.3721,  ..., -0.0148,  0.5539,  0.1288],\n",
      "         ...,\n",
      "         [-0.6158,  0.4170,  0.3829,  ...,  0.0249,  0.3260, -1.0052],\n",
      "         [ 1.9146, -0.2386,  1.0197,  ...,  0.4866,  1.6184, -0.3478],\n",
      "         [ 0.1907, -0.3370, -1.2368,  ..., -0.5970,  0.0251, -1.0617]],\n",
      "\n",
      "        [[ 0.0539,  0.5592,  0.1958,  ..., -0.4255,  0.6925, -0.7155],\n",
      "         [ 0.5924, -0.8995, -0.5179,  ...,  0.3685, -0.4286,  0.0962],\n",
      "         [-0.2297, -0.1499,  0.9091,  ...,  0.8226, -0.4116, -0.6720],\n",
      "         ...,\n",
      "         [-0.2549,  0.0671,  0.4572,  ...,  0.6250,  0.3556, -0.2483],\n",
      "         [-0.1086,  0.5611,  0.4977,  ..., -0.0023, -0.0528, -0.4180],\n",
      "         [ 0.3888, -1.0604, -0.0389,  ...,  0.4054,  0.8530, -0.0224]],\n",
      "\n",
      "        [[ 0.4801,  0.2982,  0.0407,  ..., -0.1195,  0.7620, -0.4842],\n",
      "         [ 0.3492,  1.0675, -0.5719,  ...,  0.2386,  0.0107, -0.2456],\n",
      "         [-0.0154,  0.0777, -1.3775,  ..., -0.0762, -0.4710, -0.7656],\n",
      "         ...,\n",
      "         [-0.4288, -0.8509,  0.9592,  ..., -0.0306,  0.5050, -0.2375],\n",
      "         [-0.3524,  1.0223, -0.3520,  ...,  0.4737, -0.0025, -0.0112],\n",
      "         [-0.2353, -1.4301,  0.4524,  ...,  0.1605, -0.3977, -0.0316]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4760, -0.4442,  0.0109,  ...,  0.6373,  0.8236,  0.8091],\n",
      "         [ 0.2907,  0.1676,  0.6068,  ...,  0.2445,  0.4391, -0.9943],\n",
      "         [ 0.6477,  0.2806,  0.2846,  ...,  0.5929, -0.6609,  0.0583],\n",
      "         ...,\n",
      "         [ 0.0112, -0.4074,  0.1731,  ..., -0.0744,  0.6459, -0.2608],\n",
      "         [-0.4144,  0.2559,  0.1455,  ..., -0.1096,  1.2815,  0.3020],\n",
      "         [ 1.0981, -0.9485,  0.2769,  ...,  0.8321,  0.2828,  0.1800]],\n",
      "\n",
      "        [[ 0.0682, -0.3929, -0.7390,  ...,  0.5510,  0.2755, -0.6868],\n",
      "         [-0.2665,  1.1607,  0.4220,  ..., -0.0021,  0.3882, -0.5741],\n",
      "         [ 0.6627, -0.1431,  0.3261,  ...,  0.5718, -0.5290, -0.6092],\n",
      "         ...,\n",
      "         [ 0.1947, -0.2705,  0.2019,  ..., -1.1304,  0.4840, -0.5524],\n",
      "         [-0.5533, -0.3040,  0.2819,  ...,  0.7568,  0.2351, -0.7519],\n",
      "         [-0.6493, -0.9724,  0.9026,  ...,  0.1522, -0.2951,  0.8334]],\n",
      "\n",
      "        [[-0.7298, -0.0391,  0.5353,  ...,  0.3224, -0.3733, -0.3301],\n",
      "         [-1.1085,  0.5865,  0.8565,  ..., -0.0363, -0.0791, -0.4795],\n",
      "         [ 0.6482,  0.1365,  0.9076,  ...,  0.4534, -0.0852, -0.4712],\n",
      "         ...,\n",
      "         [-0.5033, -0.2178, -0.4378,  ..., -0.7931,  0.0164,  0.3977],\n",
      "         [ 0.0672,  0.0935, -0.3566,  ...,  0.9356, -0.7232, -0.1062],\n",
      "         [ 0.1359,  1.0368, -0.1075,  ..., -0.6107, -0.5730, -0.4245]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[ 0.3953,  0.3677,  0.0110,  ...,  0.5793,  0.8494,  0.8609],\n",
      "         [-0.7482, -1.0107, -0.7512,  ..., -0.0718,  0.5739,  0.4940],\n",
      "         [-0.8599, -0.3216, -0.0651,  ..., -0.2874, -0.5969,  0.8664],\n",
      "         ...,\n",
      "         [ 0.1597,  1.3888,  0.5258,  ..., -0.1830,  0.2650, -0.1084],\n",
      "         [-0.5799,  0.6775, -0.9523,  ..., -1.4671, -0.5917,  0.5802],\n",
      "         [-0.1519,  2.0380,  0.4099,  ..., -0.3131, -0.5655, -0.7218]],\n",
      "\n",
      "        [[-0.2804, -0.5397,  0.2886,  ..., -0.1712, -0.4554,  0.0257],\n",
      "         [-1.4752, -0.0596,  0.0203,  ...,  0.7826, -1.0647,  1.4168],\n",
      "         [-0.4477, -0.1351,  0.0746,  ..., -1.1885,  0.5727,  0.1952],\n",
      "         ...,\n",
      "         [-0.8086,  0.6436,  0.3647,  ...,  1.2065, -0.7912,  0.0086],\n",
      "         [-0.9920,  0.2846,  0.0162,  ..., -0.1770, -0.5830,  0.3358],\n",
      "         [ 0.0188, -0.0680,  1.1035,  ...,  0.1710, -0.1770,  0.3293]],\n",
      "\n",
      "        [[-0.0387,  0.0166, -0.3628,  ..., -0.0647, -0.3950,  0.3115],\n",
      "         [-0.4050, -1.0950, -1.2449,  ..., -1.0938,  0.4112,  1.1643],\n",
      "         [ 0.5285,  1.3334,  0.0496,  ..., -0.2146, -0.9731, -0.3368],\n",
      "         ...,\n",
      "         [-1.9862,  1.4475,  0.5332,  ..., -0.7197,  0.2818,  0.4829],\n",
      "         [-0.0497,  0.6261,  0.0502,  ..., -0.0275, -0.3302,  0.4560],\n",
      "         [ 0.9363,  0.7185,  0.8892,  ..., -0.6506, -0.5976,  0.1833]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6501,  0.3298, -0.0970,  ..., -0.0068, -1.4588,  0.2412],\n",
      "         [-0.3563,  0.0122, -0.7176,  ..., -0.0473, -0.4454,  0.5186],\n",
      "         [-0.8439,  0.1837, -0.9965,  ...,  0.2815,  0.9468,  1.2173],\n",
      "         ...,\n",
      "         [-0.0439,  0.2037,  1.2204,  ..., -0.8382,  0.1525,  0.4213],\n",
      "         [-0.8262,  0.1852,  1.2503,  ..., -1.2399, -0.3638,  1.7566],\n",
      "         [-0.0099,  0.4743,  0.8401,  ..., -0.0487, -0.6692, -0.2324]],\n",
      "\n",
      "        [[-1.2670,  0.2414,  0.2684,  ...,  0.8238, -0.6760,  0.1015],\n",
      "         [-0.2859,  0.2519, -0.8753,  ...,  0.9423, -0.0247,  1.0610],\n",
      "         [-0.2417,  1.4860, -0.7733,  ...,  0.1816, -0.1765, -0.5684],\n",
      "         ...,\n",
      "         [ 0.0832,  0.1182,  0.5494,  ..., -0.7713, -0.5050, -0.2501],\n",
      "         [-0.9924, -0.2447,  0.8739,  ...,  0.4991, -1.3061,  1.5215],\n",
      "         [-0.0765,  0.3019,  1.0875,  ...,  0.2087, -0.0915,  0.6020]],\n",
      "\n",
      "        [[ 0.3821,  0.4682,  1.1134,  ..., -1.3505,  0.5300,  0.5455],\n",
      "         [-0.5180,  0.5718,  0.6937,  ..., -1.0087, -0.0959, -0.1908],\n",
      "         [ 0.1347,  0.3361, -0.1778,  ..., -0.9424, -0.3254,  0.3582],\n",
      "         ...,\n",
      "         [-0.9741, -0.6309,  0.7328,  ...,  1.0617, -0.2287,  0.7504],\n",
      "         [-0.1989,  0.3702,  1.1890,  ..., -0.1855, -0.0622,  0.6678],\n",
      "         [-0.4119,  1.4932,  0.8676,  ..., -0.2312,  0.0716, -0.3034]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[-0.2114, -0.8342, -0.1700,  ...,  0.2811,  0.9752,  0.2780],\n",
      "         [-1.2753, -0.9862, -0.4521,  ...,  1.0710,  0.3949, -0.2081],\n",
      "         [-0.6172, -0.5541, -0.7753,  ..., -0.0448,  0.9737, -0.5744],\n",
      "         ...,\n",
      "         [-0.1374,  0.1236,  0.0409,  ...,  0.4996,  0.0459, -0.1561],\n",
      "         [-0.1155,  0.8637, -1.1799,  ...,  0.2430,  1.0173, -0.4931],\n",
      "         [-1.4836, -0.5726,  0.4765,  ...,  2.1992,  0.4265, -0.0936]],\n",
      "\n",
      "        [[-0.4374, -0.5459,  0.0985,  ...,  0.6828,  1.0115, -0.6093],\n",
      "         [-0.3803, -0.7193, -0.9969,  ...,  0.0879,  0.4811, -0.3434],\n",
      "         [-0.5474,  0.1292,  0.5170,  ...,  1.1314,  0.2452,  0.1806],\n",
      "         ...,\n",
      "         [-0.6252, -0.1586, -0.2599,  ..., -0.3794,  0.2509,  0.9545],\n",
      "         [ 0.2646,  0.4040, -0.3090,  ...,  0.2350, -0.0331,  0.1252],\n",
      "         [ 0.1005,  0.2161, -1.2203,  ...,  0.7361,  0.2991, -0.0673]],\n",
      "\n",
      "        [[ 0.1882, -0.5809,  0.3112,  ...,  0.8436,  0.4688, -0.1500],\n",
      "         [ 0.1666, -1.1033, -0.9760,  ...,  1.1682, -0.3454,  0.2452],\n",
      "         [-0.5923,  0.0809, -0.8638,  ...,  1.2173,  0.4049, -0.1534],\n",
      "         ...,\n",
      "         [ 0.2915,  0.0309, -0.9692,  ...,  0.9190,  0.0874,  0.5110],\n",
      "         [-0.1265,  0.1860,  0.0742,  ..., -0.6362, -0.4089,  0.3845],\n",
      "         [ 0.6323,  0.0697,  0.6249,  ...,  0.2054,  0.8667, -0.0494]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5147, -0.1682,  0.3593,  ...,  1.0334,  1.2519, -0.2673],\n",
      "         [-0.8073, -0.6747, -1.4629,  ...,  0.6286,  1.0235, -0.2303],\n",
      "         [-0.6530, -0.3647, -1.1181,  ...,  0.3675,  1.3970,  0.3453],\n",
      "         ...,\n",
      "         [ 0.1722,  0.2267, -1.3930,  ..., -0.4020,  0.3468, -0.1140],\n",
      "         [-0.0957, -0.2818, -0.4901,  ...,  0.7085,  0.7480, -0.5325],\n",
      "         [-0.9096,  0.2075, -0.2246,  ..., -0.1997,  0.0034,  0.7446]],\n",
      "\n",
      "        [[ 1.0070, -0.8399,  0.4331,  ..., -0.0868,  0.4366,  0.1449],\n",
      "         [-1.5205, -0.7861, -0.1578,  ...,  0.2162,  0.7523,  0.0127],\n",
      "         [-0.3469, -0.2894, -1.0568,  ...,  0.2950,  1.8924,  0.3204],\n",
      "         ...,\n",
      "         [-1.0132,  0.6999, -0.3198,  ..., -0.3406,  0.4967,  0.2298],\n",
      "         [ 0.2980,  0.7123, -1.0817,  ...,  0.6183, -0.1680,  0.0041],\n",
      "         [-0.0793, -0.0754, -0.1075,  ...,  0.2026, -0.0353,  0.5321]],\n",
      "\n",
      "        [[-0.3024,  0.3378, -0.7744,  ...,  2.6293,  0.7079, -1.0958],\n",
      "         [-0.2283, -0.2814, -0.3230,  ...,  0.4470, -0.2570,  0.1913],\n",
      "         [-0.8446, -0.4680,  0.0352,  ...,  0.4003,  0.9192, -0.9540],\n",
      "         ...,\n",
      "         [ 0.3613, -0.1325, -0.5762,  ..., -0.2450,  0.0520, -0.8636],\n",
      "         [-0.6581, -0.7264,  0.1971,  ...,  0.6566, -0.1322,  0.5452],\n",
      "         [ 0.1992, -0.3095, -0.0488,  ...,  1.2826,  0.8392, -0.3185]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **分割多头的目的**\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n",
    "\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim\n",
    "#　将`num_heads`维度提到前面，方便后续并行计算。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0145, 0.0134, 0.0188,  ..., 0.0268, 0.0104, 0.0072],\n",
      "          [0.0144, 0.0141, 0.0494,  ..., 0.0128, 0.0135, 0.0195],\n",
      "          [0.0230, 0.0256, 0.0182,  ..., 0.0241, 0.0086, 0.0305],\n",
      "          ...,\n",
      "          [0.0271, 0.0205, 0.0163,  ..., 0.0285, 0.0124, 0.0118],\n",
      "          [0.0270, 0.0243, 0.0227,  ..., 0.0161, 0.0124, 0.0191],\n",
      "          [0.0167, 0.0207, 0.0206,  ..., 0.0294, 0.0165, 0.0132]],\n",
      "\n",
      "         [[0.0190, 0.0289, 0.0166,  ..., 0.0294, 0.0333, 0.0164],\n",
      "          [0.0214, 0.0333, 0.0103,  ..., 0.0248, 0.0115, 0.0274],\n",
      "          [0.0122, 0.0168, 0.0106,  ..., 0.0607, 0.0150, 0.0266],\n",
      "          ...,\n",
      "          [0.0134, 0.0091, 0.0078,  ..., 0.0612, 0.0162, 0.0162],\n",
      "          [0.0196, 0.0166, 0.0071,  ..., 0.0304, 0.0100, 0.0176],\n",
      "          [0.0125, 0.0166, 0.0066,  ..., 0.0259, 0.0107, 0.0183]],\n",
      "\n",
      "         [[0.0147, 0.0163, 0.0125,  ..., 0.0072, 0.0181, 0.0146],\n",
      "          [0.0375, 0.0353, 0.0091,  ..., 0.0066, 0.0200, 0.0131],\n",
      "          [0.0231, 0.0236, 0.0097,  ..., 0.0165, 0.0489, 0.0158],\n",
      "          ...,\n",
      "          [0.0224, 0.0398, 0.0123,  ..., 0.0195, 0.0107, 0.0222],\n",
      "          [0.0320, 0.0108, 0.0129,  ..., 0.0304, 0.0304, 0.0123],\n",
      "          [0.0151, 0.0271, 0.0112,  ..., 0.0310, 0.0490, 0.0065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0451, 0.0145, 0.0233,  ..., 0.0181, 0.0233, 0.0190],\n",
      "          [0.0166, 0.0056, 0.0214,  ..., 0.0137, 0.0101, 0.0189],\n",
      "          [0.0100, 0.0045, 0.0083,  ..., 0.0420, 0.0140, 0.0364],\n",
      "          ...,\n",
      "          [0.0176, 0.0147, 0.0162,  ..., 0.0254, 0.0195, 0.0216],\n",
      "          [0.0144, 0.0180, 0.0233,  ..., 0.0288, 0.0111, 0.0104],\n",
      "          [0.0205, 0.0173, 0.0205,  ..., 0.0281, 0.0176, 0.0120]],\n",
      "\n",
      "         [[0.0114, 0.0137, 0.0155,  ..., 0.0249, 0.0296, 0.0177],\n",
      "          [0.0120, 0.0167, 0.0167,  ..., 0.0339, 0.0207, 0.0104],\n",
      "          [0.0240, 0.0212, 0.0209,  ..., 0.0246, 0.0216, 0.0175],\n",
      "          ...,\n",
      "          [0.0326, 0.0227, 0.0233,  ..., 0.0134, 0.0210, 0.0149],\n",
      "          [0.0438, 0.0367, 0.0292,  ..., 0.0093, 0.0146, 0.0126],\n",
      "          [0.0227, 0.0221, 0.0308,  ..., 0.0089, 0.0100, 0.0134]],\n",
      "\n",
      "         [[0.0208, 0.0246, 0.0203,  ..., 0.0572, 0.0139, 0.0533],\n",
      "          [0.0186, 0.0098, 0.0120,  ..., 0.0201, 0.0256, 0.0188],\n",
      "          [0.0216, 0.0099, 0.0200,  ..., 0.0165, 0.0136, 0.0096],\n",
      "          ...,\n",
      "          [0.0260, 0.0094, 0.0224,  ..., 0.0150, 0.0122, 0.0205],\n",
      "          [0.0265, 0.0248, 0.0226,  ..., 0.0106, 0.0079, 0.0119],\n",
      "          [0.0247, 0.0161, 0.0219,  ..., 0.0190, 0.0089, 0.0331]]],\n",
      "\n",
      "\n",
      "        [[[0.0111, 0.0112, 0.0181,  ..., 0.0170, 0.0304, 0.0201],\n",
      "          [0.0237, 0.0122, 0.0157,  ..., 0.0191, 0.0229, 0.0144],\n",
      "          [0.0308, 0.0252, 0.0316,  ..., 0.0133, 0.0136, 0.0106],\n",
      "          ...,\n",
      "          [0.0306, 0.0186, 0.0173,  ..., 0.0161, 0.0171, 0.0216],\n",
      "          [0.0236, 0.0238, 0.0182,  ..., 0.0134, 0.0206, 0.0135],\n",
      "          [0.0411, 0.0106, 0.0160,  ..., 0.0138, 0.0253, 0.0217]],\n",
      "\n",
      "         [[0.0455, 0.0209, 0.0267,  ..., 0.0123, 0.0159, 0.0398],\n",
      "          [0.0109, 0.0107, 0.0372,  ..., 0.0209, 0.0124, 0.0239],\n",
      "          [0.0239, 0.0133, 0.0234,  ..., 0.0196, 0.0228, 0.0135],\n",
      "          ...,\n",
      "          [0.0121, 0.0359, 0.0264,  ..., 0.0243, 0.0154, 0.0158],\n",
      "          [0.0232, 0.0347, 0.0187,  ..., 0.0193, 0.0111, 0.0264],\n",
      "          [0.0198, 0.0311, 0.0263,  ..., 0.0215, 0.0249, 0.0203]],\n",
      "\n",
      "         [[0.0231, 0.0232, 0.0256,  ..., 0.0126, 0.0134, 0.0178],\n",
      "          [0.0230, 0.0246, 0.0436,  ..., 0.0166, 0.0111, 0.0237],\n",
      "          [0.0126, 0.0171, 0.0293,  ..., 0.0157, 0.0260, 0.0490],\n",
      "          ...,\n",
      "          [0.0099, 0.0183, 0.0254,  ..., 0.0175, 0.0218, 0.0241],\n",
      "          [0.0113, 0.0179, 0.0247,  ..., 0.0157, 0.0161, 0.0277],\n",
      "          [0.0129, 0.0180, 0.0093,  ..., 0.0299, 0.0142, 0.0462]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0166, 0.0133, 0.0125,  ..., 0.0179, 0.0272, 0.0215],\n",
      "          [0.0339, 0.0153, 0.0217,  ..., 0.0419, 0.0195, 0.0127],\n",
      "          [0.0214, 0.0077, 0.0213,  ..., 0.0151, 0.0294, 0.0330],\n",
      "          ...,\n",
      "          [0.0182, 0.0225, 0.0142,  ..., 0.0252, 0.0150, 0.0212],\n",
      "          [0.0434, 0.0301, 0.0193,  ..., 0.0180, 0.0097, 0.0253],\n",
      "          [0.0208, 0.0092, 0.0225,  ..., 0.0285, 0.0245, 0.0203]],\n",
      "\n",
      "         [[0.0139, 0.0081, 0.0123,  ..., 0.0106, 0.0138, 0.0135],\n",
      "          [0.0250, 0.0120, 0.0357,  ..., 0.0064, 0.0164, 0.0178],\n",
      "          [0.0172, 0.0099, 0.0238,  ..., 0.0101, 0.0112, 0.0113],\n",
      "          ...,\n",
      "          [0.0268, 0.0148, 0.0301,  ..., 0.0099, 0.0228, 0.0240],\n",
      "          [0.0151, 0.0190, 0.0191,  ..., 0.0113, 0.0367, 0.0344],\n",
      "          [0.0117, 0.0203, 0.0213,  ..., 0.0122, 0.0259, 0.0167]],\n",
      "\n",
      "         [[0.0229, 0.0229, 0.0228,  ..., 0.0410, 0.0119, 0.0238],\n",
      "          [0.0209, 0.0285, 0.0227,  ..., 0.0220, 0.0217, 0.0211],\n",
      "          [0.0190, 0.0132, 0.0160,  ..., 0.0199, 0.0350, 0.0251],\n",
      "          ...,\n",
      "          [0.0136, 0.0411, 0.0120,  ..., 0.0266, 0.0081, 0.0151],\n",
      "          [0.0218, 0.0239, 0.0101,  ..., 0.0335, 0.0172, 0.0105],\n",
      "          [0.0184, 0.0194, 0.0233,  ..., 0.0240, 0.0143, 0.0162]]],\n",
      "\n",
      "\n",
      "        [[[0.0095, 0.0148, 0.0051,  ..., 0.0101, 0.0130, 0.0380],\n",
      "          [0.0077, 0.0096, 0.0148,  ..., 0.0176, 0.0139, 0.0242],\n",
      "          [0.0220, 0.0165, 0.0128,  ..., 0.0143, 0.0173, 0.0155],\n",
      "          ...,\n",
      "          [0.0307, 0.0140, 0.0109,  ..., 0.0103, 0.0159, 0.0326],\n",
      "          [0.0291, 0.0346, 0.0291,  ..., 0.0213, 0.0203, 0.0181],\n",
      "          [0.0092, 0.0212, 0.0203,  ..., 0.0152, 0.0150, 0.0111]],\n",
      "\n",
      "         [[0.0088, 0.0132, 0.0182,  ..., 0.0149, 0.0205, 0.0137],\n",
      "          [0.0138, 0.0175, 0.0118,  ..., 0.0290, 0.0241, 0.0113],\n",
      "          [0.0138, 0.0215, 0.0114,  ..., 0.0278, 0.0222, 0.0099],\n",
      "          ...,\n",
      "          [0.0095, 0.0234, 0.0224,  ..., 0.0112, 0.0126, 0.0146],\n",
      "          [0.0174, 0.0109, 0.0186,  ..., 0.0205, 0.0248, 0.0149],\n",
      "          [0.0121, 0.0100, 0.0157,  ..., 0.0116, 0.0375, 0.0218]],\n",
      "\n",
      "         [[0.0198, 0.0436, 0.0063,  ..., 0.0096, 0.0262, 0.0362],\n",
      "          [0.0170, 0.0204, 0.0154,  ..., 0.0125, 0.0346, 0.0286],\n",
      "          [0.0136, 0.0201, 0.0101,  ..., 0.0257, 0.0201, 0.0468],\n",
      "          ...,\n",
      "          [0.0219, 0.0206, 0.0143,  ..., 0.0131, 0.0187, 0.0260],\n",
      "          [0.0375, 0.0220, 0.0079,  ..., 0.0121, 0.0208, 0.0370],\n",
      "          [0.0133, 0.0356, 0.0092,  ..., 0.0181, 0.0264, 0.0511]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0150, 0.0139, 0.0172,  ..., 0.0156, 0.0333, 0.0293],\n",
      "          [0.0125, 0.0165, 0.0155,  ..., 0.0266, 0.0155, 0.0221],\n",
      "          [0.0215, 0.0132, 0.0217,  ..., 0.0086, 0.0259, 0.0147],\n",
      "          ...,\n",
      "          [0.0116, 0.0170, 0.0216,  ..., 0.0157, 0.0119, 0.0342],\n",
      "          [0.0220, 0.0194, 0.0222,  ..., 0.0237, 0.0187, 0.0253],\n",
      "          [0.0158, 0.0243, 0.0327,  ..., 0.0412, 0.0141, 0.0123]],\n",
      "\n",
      "         [[0.0252, 0.0284, 0.0228,  ..., 0.0198, 0.0134, 0.0259],\n",
      "          [0.0148, 0.0069, 0.0184,  ..., 0.0105, 0.0116, 0.0232],\n",
      "          [0.0137, 0.0127, 0.0132,  ..., 0.0095, 0.0252, 0.0104],\n",
      "          ...,\n",
      "          [0.0148, 0.0168, 0.0176,  ..., 0.0160, 0.0198, 0.0206],\n",
      "          [0.0121, 0.0110, 0.0133,  ..., 0.0107, 0.0246, 0.0166],\n",
      "          [0.0229, 0.0200, 0.0181,  ..., 0.0107, 0.0167, 0.0148]],\n",
      "\n",
      "         [[0.0129, 0.0167, 0.0138,  ..., 0.0286, 0.0081, 0.0212],\n",
      "          [0.0189, 0.0168, 0.0241,  ..., 0.0214, 0.0197, 0.0175],\n",
      "          [0.0190, 0.0184, 0.0277,  ..., 0.0253, 0.0133, 0.0158],\n",
      "          ...,\n",
      "          [0.0206, 0.0453, 0.0202,  ..., 0.0238, 0.0324, 0.0208],\n",
      "          [0.0104, 0.0144, 0.0186,  ..., 0.0225, 0.0070, 0.0103],\n",
      "          [0.0134, 0.0208, 0.0143,  ..., 0.0107, 0.0126, 0.0167]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0240, 0.0070, 0.0165,  ..., 0.0112, 0.0165, 0.0173],\n",
      "          [0.0195, 0.0083, 0.0233,  ..., 0.0366, 0.0267, 0.0176],\n",
      "          [0.0168, 0.0090, 0.0157,  ..., 0.0268, 0.0264, 0.0145],\n",
      "          ...,\n",
      "          [0.0219, 0.0306, 0.0218,  ..., 0.0219, 0.0230, 0.0204],\n",
      "          [0.0252, 0.0130, 0.0131,  ..., 0.0269, 0.0139, 0.0141],\n",
      "          [0.0188, 0.0260, 0.0224,  ..., 0.0256, 0.0201, 0.0250]],\n",
      "\n",
      "         [[0.0259, 0.0079, 0.0171,  ..., 0.0173, 0.0214, 0.0159],\n",
      "          [0.0365, 0.0276, 0.0138,  ..., 0.0163, 0.0183, 0.0371],\n",
      "          [0.0474, 0.0107, 0.0171,  ..., 0.0123, 0.0162, 0.0314],\n",
      "          ...,\n",
      "          [0.0302, 0.0100, 0.0220,  ..., 0.0242, 0.0195, 0.0208],\n",
      "          [0.0410, 0.0148, 0.0175,  ..., 0.0207, 0.0290, 0.0289],\n",
      "          [0.0227, 0.0156, 0.0108,  ..., 0.0133, 0.0147, 0.0348]],\n",
      "\n",
      "         [[0.0181, 0.0175, 0.0083,  ..., 0.0210, 0.0245, 0.0156],\n",
      "          [0.0567, 0.0153, 0.0148,  ..., 0.0508, 0.0125, 0.0255],\n",
      "          [0.0233, 0.0320, 0.0161,  ..., 0.0334, 0.0142, 0.0180],\n",
      "          ...,\n",
      "          [0.0174, 0.0219, 0.0207,  ..., 0.0355, 0.0184, 0.0101],\n",
      "          [0.0250, 0.0208, 0.0117,  ..., 0.0107, 0.0222, 0.0146],\n",
      "          [0.0335, 0.0200, 0.0130,  ..., 0.0103, 0.0181, 0.0285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0381, 0.0146, 0.0188,  ..., 0.0193, 0.0235, 0.0148],\n",
      "          [0.0292, 0.0089, 0.0179,  ..., 0.0176, 0.0196, 0.0158],\n",
      "          [0.0284, 0.0156, 0.0244,  ..., 0.0162, 0.0318, 0.0166],\n",
      "          ...,\n",
      "          [0.0267, 0.0185, 0.0191,  ..., 0.0380, 0.0238, 0.0195],\n",
      "          [0.0169, 0.0499, 0.0085,  ..., 0.0225, 0.0331, 0.0162],\n",
      "          [0.0171, 0.0301, 0.0132,  ..., 0.0218, 0.0340, 0.0161]],\n",
      "\n",
      "         [[0.0124, 0.0206, 0.0159,  ..., 0.0127, 0.0114, 0.0189],\n",
      "          [0.0095, 0.0122, 0.0217,  ..., 0.0246, 0.0193, 0.0238],\n",
      "          [0.0294, 0.0099, 0.0130,  ..., 0.0194, 0.0112, 0.0095],\n",
      "          ...,\n",
      "          [0.0080, 0.0154, 0.0305,  ..., 0.0126, 0.0046, 0.0270],\n",
      "          [0.0096, 0.0219, 0.0167,  ..., 0.0112, 0.0091, 0.0148],\n",
      "          [0.0153, 0.0152, 0.0226,  ..., 0.0223, 0.0154, 0.0125]],\n",
      "\n",
      "         [[0.0157, 0.0217, 0.0225,  ..., 0.0140, 0.0235, 0.0143],\n",
      "          [0.0093, 0.0178, 0.0084,  ..., 0.0131, 0.0109, 0.0210],\n",
      "          [0.0263, 0.0234, 0.0228,  ..., 0.0089, 0.0103, 0.0231],\n",
      "          ...,\n",
      "          [0.0148, 0.0233, 0.0100,  ..., 0.0117, 0.0114, 0.0218],\n",
      "          [0.0090, 0.0134, 0.0074,  ..., 0.0165, 0.0199, 0.0152],\n",
      "          [0.0107, 0.0242, 0.0152,  ..., 0.0207, 0.0142, 0.0182]]],\n",
      "\n",
      "\n",
      "        [[[0.0105, 0.0136, 0.0280,  ..., 0.0102, 0.0151, 0.0145],\n",
      "          [0.0211, 0.0128, 0.0113,  ..., 0.0122, 0.0313, 0.0389],\n",
      "          [0.0221, 0.0124, 0.0082,  ..., 0.0272, 0.0171, 0.0280],\n",
      "          ...,\n",
      "          [0.0267, 0.0155, 0.0104,  ..., 0.0253, 0.0174, 0.0224],\n",
      "          [0.0188, 0.0120, 0.0153,  ..., 0.0266, 0.0197, 0.0182],\n",
      "          [0.0252, 0.0104, 0.0102,  ..., 0.0146, 0.0273, 0.0130]],\n",
      "\n",
      "         [[0.0183, 0.0260, 0.0125,  ..., 0.0215, 0.0150, 0.0229],\n",
      "          [0.0107, 0.0111, 0.0158,  ..., 0.0122, 0.0110, 0.0329],\n",
      "          [0.0347, 0.0122, 0.0161,  ..., 0.0126, 0.0165, 0.0147],\n",
      "          ...,\n",
      "          [0.0279, 0.0232, 0.0206,  ..., 0.0108, 0.0094, 0.0242],\n",
      "          [0.0197, 0.0076, 0.0173,  ..., 0.0073, 0.0085, 0.0122],\n",
      "          [0.0125, 0.0070, 0.0115,  ..., 0.0152, 0.0074, 0.0186]],\n",
      "\n",
      "         [[0.0167, 0.0109, 0.0167,  ..., 0.0139, 0.0153, 0.0166],\n",
      "          [0.0387, 0.0300, 0.0229,  ..., 0.0135, 0.0239, 0.0189],\n",
      "          [0.0120, 0.0259, 0.0182,  ..., 0.0203, 0.0279, 0.0153],\n",
      "          ...,\n",
      "          [0.0166, 0.0141, 0.0281,  ..., 0.0089, 0.0190, 0.0185],\n",
      "          [0.0100, 0.0151, 0.0146,  ..., 0.0201, 0.0152, 0.0209],\n",
      "          [0.0169, 0.0050, 0.0229,  ..., 0.0127, 0.0112, 0.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0158, 0.0178, 0.0167,  ..., 0.0170, 0.0183, 0.0213],\n",
      "          [0.0200, 0.0153, 0.0175,  ..., 0.0103, 0.0124, 0.0334],\n",
      "          [0.0236, 0.0106, 0.0193,  ..., 0.0227, 0.0194, 0.0477],\n",
      "          ...,\n",
      "          [0.0302, 0.0128, 0.0113,  ..., 0.0316, 0.0293, 0.0140],\n",
      "          [0.0316, 0.0236, 0.0278,  ..., 0.0164, 0.0124, 0.0260],\n",
      "          [0.0240, 0.0198, 0.0180,  ..., 0.0267, 0.0372, 0.0134]],\n",
      "\n",
      "         [[0.0135, 0.0091, 0.0165,  ..., 0.0145, 0.0232, 0.0152],\n",
      "          [0.0385, 0.0287, 0.0222,  ..., 0.0167, 0.0182, 0.0180],\n",
      "          [0.0233, 0.0212, 0.0247,  ..., 0.0131, 0.0128, 0.0276],\n",
      "          ...,\n",
      "          [0.0220, 0.0169, 0.0152,  ..., 0.0139, 0.0131, 0.0106],\n",
      "          [0.0771, 0.0120, 0.0270,  ..., 0.0093, 0.0180, 0.0064],\n",
      "          [0.0162, 0.0088, 0.0312,  ..., 0.0101, 0.0272, 0.0056]],\n",
      "\n",
      "         [[0.0086, 0.0127, 0.0384,  ..., 0.0170, 0.0070, 0.0200],\n",
      "          [0.0119, 0.0125, 0.0164,  ..., 0.0384, 0.0130, 0.0272],\n",
      "          [0.0443, 0.0303, 0.0190,  ..., 0.0109, 0.0432, 0.0191],\n",
      "          ...,\n",
      "          [0.0110, 0.0065, 0.0200,  ..., 0.0157, 0.0197, 0.0155],\n",
      "          [0.0222, 0.0145, 0.0125,  ..., 0.0210, 0.0182, 0.0190],\n",
      "          [0.0162, 0.0193, 0.0206,  ..., 0.0120, 0.0407, 0.0188]]],\n",
      "\n",
      "\n",
      "        [[[0.0195, 0.0271, 0.0408,  ..., 0.0079, 0.0187, 0.0175],\n",
      "          [0.0156, 0.0438, 0.0182,  ..., 0.0250, 0.0170, 0.0294],\n",
      "          [0.0216, 0.0350, 0.0098,  ..., 0.0095, 0.0283, 0.0182],\n",
      "          ...,\n",
      "          [0.0190, 0.0199, 0.0186,  ..., 0.0144, 0.0150, 0.0240],\n",
      "          [0.0188, 0.0264, 0.0098,  ..., 0.0175, 0.0164, 0.0117],\n",
      "          [0.0251, 0.0452, 0.0155,  ..., 0.0086, 0.0150, 0.0245]],\n",
      "\n",
      "         [[0.0368, 0.0187, 0.0166,  ..., 0.0272, 0.0268, 0.0176],\n",
      "          [0.0334, 0.0255, 0.0152,  ..., 0.0204, 0.0262, 0.0170],\n",
      "          [0.0270, 0.0125, 0.0149,  ..., 0.0158, 0.0163, 0.0153],\n",
      "          ...,\n",
      "          [0.0101, 0.0066, 0.0096,  ..., 0.0121, 0.0095, 0.0131],\n",
      "          [0.0155, 0.0124, 0.0174,  ..., 0.0219, 0.0223, 0.0290],\n",
      "          [0.0171, 0.0292, 0.0114,  ..., 0.0273, 0.0143, 0.0187]],\n",
      "\n",
      "         [[0.0282, 0.0285, 0.0157,  ..., 0.0204, 0.0169, 0.0165],\n",
      "          [0.0260, 0.0211, 0.0243,  ..., 0.0144, 0.0089, 0.0172],\n",
      "          [0.0231, 0.0238, 0.0280,  ..., 0.0197, 0.0202, 0.0153],\n",
      "          ...,\n",
      "          [0.0249, 0.0144, 0.0214,  ..., 0.0143, 0.0141, 0.0119],\n",
      "          [0.0247, 0.0220, 0.0252,  ..., 0.0221, 0.0274, 0.0148],\n",
      "          [0.0151, 0.0179, 0.0169,  ..., 0.0118, 0.0112, 0.0124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0253, 0.0199, 0.0216,  ..., 0.0257, 0.0200, 0.0197],\n",
      "          [0.0302, 0.0116, 0.0123,  ..., 0.0358, 0.0316, 0.0297],\n",
      "          [0.0327, 0.0130, 0.0119,  ..., 0.0231, 0.0197, 0.0265],\n",
      "          ...,\n",
      "          [0.0231, 0.0204, 0.0210,  ..., 0.0263, 0.0146, 0.0151],\n",
      "          [0.0122, 0.0268, 0.0250,  ..., 0.0230, 0.0140, 0.0112],\n",
      "          [0.0130, 0.0355, 0.0145,  ..., 0.0411, 0.0285, 0.0174]],\n",
      "\n",
      "         [[0.0142, 0.0099, 0.0116,  ..., 0.0286, 0.0134, 0.0090],\n",
      "          [0.0115, 0.0082, 0.0139,  ..., 0.0171, 0.0131, 0.0223],\n",
      "          [0.0132, 0.0051, 0.0099,  ..., 0.0176, 0.0136, 0.0194],\n",
      "          ...,\n",
      "          [0.0179, 0.0160, 0.0218,  ..., 0.0202, 0.0106, 0.0090],\n",
      "          [0.0226, 0.0130, 0.0189,  ..., 0.0149, 0.0130, 0.0172],\n",
      "          [0.0267, 0.0206, 0.0325,  ..., 0.0205, 0.0225, 0.0138]],\n",
      "\n",
      "         [[0.0159, 0.0221, 0.0193,  ..., 0.0232, 0.0440, 0.0409],\n",
      "          [0.0121, 0.0209, 0.0158,  ..., 0.0157, 0.0277, 0.0156],\n",
      "          [0.0192, 0.0257, 0.0552,  ..., 0.0132, 0.0147, 0.0197],\n",
      "          ...,\n",
      "          [0.0093, 0.0164, 0.0211,  ..., 0.0121, 0.0065, 0.0196],\n",
      "          [0.0178, 0.0180, 0.0156,  ..., 0.0115, 0.0125, 0.0335],\n",
      "          [0.0186, 0.0330, 0.0289,  ..., 0.0159, 0.0137, 0.0185]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-1.6458e-01, -2.3871e-01, -7.4233e-01,  ..., -2.8825e-01,\n",
      "           -2.7900e-01,  1.9989e-02],\n",
      "          [-1.1793e-01, -3.2437e-01, -7.7370e-01,  ..., -3.0512e-01,\n",
      "           -2.6440e-01,  1.5022e-02],\n",
      "          [-1.7897e-01, -2.7836e-01, -7.6252e-01,  ..., -3.1575e-01,\n",
      "           -3.3100e-01,  3.7315e-02],\n",
      "          ...,\n",
      "          [-1.6517e-01, -2.3494e-01, -7.7010e-01,  ..., -2.8893e-01,\n",
      "           -2.9343e-01, -3.1390e-03],\n",
      "          [-1.5670e-01, -2.5454e-01, -7.9921e-01,  ..., -2.9428e-01,\n",
      "           -3.6396e-01,  6.8471e-03],\n",
      "          [-2.0672e-01, -1.9219e-01, -7.4919e-01,  ..., -3.2988e-01,\n",
      "           -2.9385e-01,  1.4005e-02]],\n",
      "\n",
      "         [[ 5.2237e-02, -3.9978e-01, -2.0427e-01,  ..., -3.4041e-01,\n",
      "            4.3268e-02,  6.3622e-01],\n",
      "          [ 1.0566e-02, -3.8578e-01, -2.2503e-01,  ..., -4.1149e-01,\n",
      "            3.3213e-02,  5.7943e-01],\n",
      "          [ 5.7272e-02, -3.7270e-01, -2.2454e-01,  ..., -3.4588e-01,\n",
      "            9.8815e-02,  6.1478e-01],\n",
      "          ...,\n",
      "          [ 8.6910e-02, -3.9082e-01, -2.3046e-01,  ..., -2.8730e-01,\n",
      "            4.2541e-02,  6.1709e-01],\n",
      "          [-6.9852e-02, -3.2695e-01, -3.3970e-01,  ..., -4.1270e-01,\n",
      "            1.3753e-01,  6.1062e-01],\n",
      "          [ 3.2495e-02, -4.1060e-01, -2.5616e-01,  ..., -4.2441e-01,\n",
      "            1.1827e-01,  6.1146e-01]],\n",
      "\n",
      "         [[-3.5626e-01,  2.4218e-01,  2.2223e-02,  ...,  3.4802e-02,\n",
      "            1.3542e-01,  2.8892e-01],\n",
      "          [-3.7694e-01,  3.0654e-01, -8.7439e-03,  ...,  1.3692e-01,\n",
      "            1.4804e-01,  2.6776e-01],\n",
      "          [-3.6463e-01,  3.4828e-01,  5.6671e-02,  ...,  1.1644e-01,\n",
      "            1.6501e-01,  3.2234e-01],\n",
      "          ...,\n",
      "          [-3.3098e-01,  3.3107e-01, -5.4127e-03,  ...,  9.6998e-02,\n",
      "            1.5385e-01,  3.7717e-01],\n",
      "          [-3.2397e-01,  2.6012e-01, -1.6212e-02,  ...,  7.0080e-02,\n",
      "            1.0794e-01,  3.2705e-01],\n",
      "          [-2.9968e-01,  3.9625e-01,  4.7974e-02,  ...,  1.4329e-01,\n",
      "            1.4209e-01,  2.8949e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.3375e-01, -4.7059e-01, -4.5447e-01,  ..., -5.2304e-02,\n",
      "            2.4488e-01, -4.4614e-01],\n",
      "          [-6.4926e-01, -4.6636e-01, -5.1117e-01,  ..., -5.4320e-02,\n",
      "            2.6572e-01, -4.9391e-01],\n",
      "          [-7.2191e-01, -4.9454e-01, -5.4409e-01,  ..., -6.5338e-02,\n",
      "            2.8934e-01, -5.4839e-01],\n",
      "          ...,\n",
      "          [-6.3386e-01, -4.3064e-01, -5.3772e-01,  ..., -2.6337e-02,\n",
      "            2.9101e-01, -4.7428e-01],\n",
      "          [-6.5381e-01, -4.2419e-01, -5.1528e-01,  ..., -2.5921e-02,\n",
      "            2.9568e-01, -4.9644e-01],\n",
      "          [-6.0726e-01, -4.6629e-01, -5.6456e-01,  ..., -2.3931e-02,\n",
      "            2.3469e-01, -5.4434e-01]],\n",
      "\n",
      "         [[-3.8573e-01,  3.5377e-03, -4.3980e-01,  ...,  4.0122e-01,\n",
      "           -2.4574e-01, -4.1814e-02],\n",
      "          [-3.6783e-01, -3.4651e-02, -4.6911e-01,  ...,  3.7679e-01,\n",
      "           -2.8165e-01,  5.1005e-02],\n",
      "          [-3.5298e-01,  1.7763e-02, -4.7370e-01,  ...,  3.6401e-01,\n",
      "           -2.7426e-01,  1.5131e-02],\n",
      "          ...,\n",
      "          [-4.3674e-01,  3.8888e-02, -4.4992e-01,  ...,  3.6064e-01,\n",
      "           -2.9573e-01,  1.4383e-02],\n",
      "          [-3.6304e-01,  3.2157e-02, -5.0614e-01,  ...,  2.7442e-01,\n",
      "           -3.9679e-01,  9.8830e-03],\n",
      "          [-3.8236e-01,  1.7689e-02, -4.6118e-01,  ...,  3.8029e-01,\n",
      "           -3.1253e-01,  1.3749e-02]],\n",
      "\n",
      "         [[-2.4798e-01, -2.0910e-01, -1.5792e-01,  ...,  5.4005e-01,\n",
      "            3.3731e-01, -9.6270e-02],\n",
      "          [-2.4521e-01, -2.0222e-01, -2.3801e-01,  ...,  5.8084e-01,\n",
      "            3.1780e-01, -2.0839e-01],\n",
      "          [-2.5934e-01, -2.0558e-01, -1.8269e-01,  ...,  6.1333e-01,\n",
      "            3.1855e-01, -1.8348e-01],\n",
      "          ...,\n",
      "          [-2.6026e-01, -2.7857e-01, -1.9621e-01,  ...,  4.9772e-01,\n",
      "            3.5144e-01, -1.2783e-01],\n",
      "          [-3.4639e-01, -1.8390e-01, -2.4444e-01,  ...,  5.5365e-01,\n",
      "            3.2386e-01, -1.4214e-01],\n",
      "          [-1.9760e-01, -2.2238e-01, -2.2070e-01,  ...,  5.4102e-01,\n",
      "            4.1714e-01, -1.8870e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5915e-01, -1.8442e-01, -7.0667e-01,  ..., -5.3084e-01,\n",
      "           -3.2406e-01, -3.5429e-02],\n",
      "          [-1.7156e-01, -1.8862e-01, -7.7050e-01,  ..., -5.4476e-01,\n",
      "           -2.8580e-01, -1.8871e-01],\n",
      "          [-2.1946e-01, -1.7161e-01, -7.0574e-01,  ..., -5.5781e-01,\n",
      "           -2.5135e-01, -1.7826e-01],\n",
      "          ...,\n",
      "          [-2.0447e-01, -1.8046e-01, -7.6515e-01,  ..., -5.4670e-01,\n",
      "           -2.9552e-01, -2.1144e-01],\n",
      "          [-1.6836e-01, -1.0911e-01, -6.9846e-01,  ..., -5.5286e-01,\n",
      "           -3.5871e-01, -2.0210e-01],\n",
      "          [-2.3317e-01, -2.4181e-01, -7.5741e-01,  ..., -5.6433e-01,\n",
      "           -2.2481e-01, -1.6688e-01]],\n",
      "\n",
      "         [[-1.0526e-01, -3.1422e-01, -1.5401e-01,  ..., -3.2349e-01,\n",
      "           -8.4790e-02,  6.0135e-01],\n",
      "          [-9.5363e-02, -3.9314e-01, -1.3322e-01,  ..., -2.5008e-01,\n",
      "           -4.2240e-02,  6.1376e-01],\n",
      "          [-2.1556e-02, -3.1532e-01, -1.4075e-01,  ..., -2.4642e-01,\n",
      "            4.1117e-04,  6.4835e-01],\n",
      "          ...,\n",
      "          [-5.3662e-02, -3.2460e-01, -1.7636e-01,  ..., -2.9641e-01,\n",
      "            2.8522e-02,  7.0355e-01],\n",
      "          [-1.6412e-02, -3.3420e-01, -2.0053e-01,  ..., -3.7582e-01,\n",
      "            5.6455e-03,  5.8497e-01],\n",
      "          [-3.3281e-02, -3.5695e-01, -1.9893e-01,  ..., -2.8601e-01,\n",
      "           -4.4058e-02,  6.4099e-01]],\n",
      "\n",
      "         [[-5.7388e-02,  3.5501e-01, -1.2598e-02,  ...,  1.6002e-01,\n",
      "            1.1846e-01,  1.8949e-01],\n",
      "          [-1.8523e-01,  3.1066e-01, -7.1212e-02,  ...,  1.3581e-01,\n",
      "            1.4281e-01,  2.4585e-01],\n",
      "          [-1.4925e-01,  3.6107e-01, -9.1334e-03,  ...,  7.4611e-02,\n",
      "            1.5227e-01,  2.9200e-01],\n",
      "          ...,\n",
      "          [-1.0462e-01,  3.9790e-01,  4.9039e-02,  ...,  9.7725e-02,\n",
      "            1.2251e-01,  1.8907e-01],\n",
      "          [-5.2747e-02,  4.0475e-01, -1.5157e-03,  ...,  1.7619e-01,\n",
      "            1.3681e-01,  2.0879e-01],\n",
      "          [-3.4593e-02,  4.7468e-01,  6.5064e-02,  ...,  1.3764e-01,\n",
      "            1.0820e-01,  2.1778e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.7669e-01, -1.9967e-01, -4.2757e-01,  ..., -7.1608e-03,\n",
      "            2.7874e-01, -4.2186e-01],\n",
      "          [-6.5501e-01, -1.3566e-01, -4.5663e-01,  ...,  8.3810e-02,\n",
      "            2.9271e-01, -3.5599e-01],\n",
      "          [-6.2534e-01, -1.7941e-01, -3.7297e-01,  ...,  3.3688e-02,\n",
      "            2.9801e-01, -3.2911e-01],\n",
      "          ...,\n",
      "          [-6.1479e-01, -2.0706e-01, -3.7095e-01,  ...,  6.6193e-02,\n",
      "            3.1452e-01, -3.5433e-01],\n",
      "          [-5.8707e-01, -2.2540e-01, -4.6513e-01,  ...,  9.8449e-03,\n",
      "            2.2144e-01, -3.4494e-01],\n",
      "          [-6.6676e-01, -1.4200e-01, -4.0841e-01,  ...,  7.9723e-02,\n",
      "            3.4080e-01, -2.9826e-01]],\n",
      "\n",
      "         [[-5.2541e-01,  1.6268e-01, -4.2352e-01,  ...,  3.4596e-01,\n",
      "           -2.9587e-01, -7.0665e-02],\n",
      "          [-5.3610e-01,  6.2337e-02, -4.1553e-01,  ...,  3.6849e-01,\n",
      "           -3.4086e-01, -1.9429e-02],\n",
      "          [-5.2625e-01,  7.6051e-02, -3.5975e-01,  ...,  2.9835e-01,\n",
      "           -3.3211e-01,  9.1216e-03],\n",
      "          ...,\n",
      "          [-5.9097e-01,  1.8084e-02, -3.6408e-01,  ...,  3.6485e-01,\n",
      "           -2.6669e-01,  1.6067e-02],\n",
      "          [-5.3518e-01,  2.2941e-02, -3.5103e-01,  ...,  3.4785e-01,\n",
      "           -2.4402e-01,  7.7496e-02],\n",
      "          [-5.3339e-01,  6.3447e-02, -3.9577e-01,  ...,  3.4432e-01,\n",
      "           -2.9000e-01,  1.6930e-02]],\n",
      "\n",
      "         [[-3.4272e-01, -1.1414e-01, -2.5691e-01,  ...,  4.1404e-01,\n",
      "            2.8770e-01, -1.6704e-01],\n",
      "          [-3.3850e-01, -1.1788e-01, -1.6495e-01,  ...,  4.4829e-01,\n",
      "            2.8469e-01, -1.9975e-01],\n",
      "          [-3.6605e-01, -1.4896e-01, -2.0310e-01,  ...,  4.5812e-01,\n",
      "            2.6333e-01, -1.8137e-01],\n",
      "          ...,\n",
      "          [-3.8204e-01, -5.8457e-02, -2.3780e-01,  ...,  4.3654e-01,\n",
      "            2.8757e-01, -2.3984e-01],\n",
      "          [-3.3659e-01, -1.8647e-01, -2.4888e-01,  ...,  4.5574e-01,\n",
      "            2.3117e-01, -2.1886e-01],\n",
      "          [-3.4944e-01, -1.1250e-01, -2.4231e-01,  ...,  4.2827e-01,\n",
      "            2.6374e-01, -1.6318e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0227e-01, -1.3798e-01, -8.8465e-01,  ..., -5.6444e-01,\n",
      "           -2.4315e-01, -4.3996e-02],\n",
      "          [-1.3545e-01, -8.4332e-02, -9.4934e-01,  ..., -5.6394e-01,\n",
      "           -2.3560e-01, -9.6019e-02],\n",
      "          [-1.4791e-01, -1.0505e-01, -9.7888e-01,  ..., -5.0187e-01,\n",
      "           -1.5307e-01, -2.7189e-02],\n",
      "          ...,\n",
      "          [-1.6568e-01, -1.8265e-01, -8.9020e-01,  ..., -5.0290e-01,\n",
      "           -2.5395e-01, -8.4858e-02],\n",
      "          [-1.1482e-01, -1.4542e-01, -9.3328e-01,  ..., -4.5485e-01,\n",
      "           -1.7222e-01, -9.5330e-02],\n",
      "          [-2.5921e-02, -2.6018e-01, -1.0024e+00,  ..., -4.5548e-01,\n",
      "           -2.3125e-01, -4.6669e-02]],\n",
      "\n",
      "         [[-9.1234e-02, -3.8652e-01, -2.8197e-01,  ..., -2.0897e-01,\n",
      "            3.1392e-01,  4.9708e-01],\n",
      "          [-1.9745e-01, -3.5493e-01, -1.6630e-01,  ..., -1.0736e-01,\n",
      "            3.0309e-01,  5.5037e-01],\n",
      "          [-1.2504e-01, -4.2263e-01, -2.1996e-01,  ..., -1.6610e-01,\n",
      "            2.7044e-01,  5.1045e-01],\n",
      "          ...,\n",
      "          [-1.3168e-01, -4.0309e-01, -2.5523e-01,  ..., -2.1722e-01,\n",
      "            2.9882e-01,  5.3262e-01],\n",
      "          [-1.6063e-01, -4.4447e-01, -2.7651e-01,  ..., -1.7894e-01,\n",
      "            3.0449e-01,  5.5861e-01],\n",
      "          [-1.1137e-01, -4.0643e-01, -2.2144e-01,  ..., -1.1686e-01,\n",
      "            3.4025e-01,  5.7530e-01]],\n",
      "\n",
      "         [[-2.4735e-01,  3.3904e-01,  1.3289e-02,  ..., -3.8472e-02,\n",
      "            2.1734e-01,  4.9678e-01],\n",
      "          [-1.7170e-01,  3.3265e-01, -4.5050e-02,  ..., -9.2367e-02,\n",
      "            2.2836e-01,  3.9875e-01],\n",
      "          [-1.9774e-01,  2.8918e-01, -4.6547e-02,  ..., -1.0293e-01,\n",
      "            2.8043e-01,  4.1281e-01],\n",
      "          ...,\n",
      "          [-2.3430e-01,  3.3168e-01,  1.9791e-02,  ..., -7.1038e-02,\n",
      "            2.3411e-01,  4.5825e-01],\n",
      "          [-2.4637e-01,  3.3988e-01, -7.5834e-02,  ..., -1.0990e-01,\n",
      "            2.0920e-01,  4.0817e-01],\n",
      "          [-2.7610e-01,  3.4324e-01,  3.7405e-02,  ..., -6.6053e-02,\n",
      "            2.8330e-01,  4.8575e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.3461e-01, -2.0917e-01, -3.8437e-01,  ..., -2.4474e-01,\n",
      "            3.6406e-01, -4.6234e-01],\n",
      "          [-6.3606e-01, -2.1943e-01, -4.0417e-01,  ..., -2.2318e-01,\n",
      "            3.7391e-01, -4.6688e-01],\n",
      "          [-6.7341e-01, -2.3605e-01, -3.5293e-01,  ..., -2.6267e-01,\n",
      "            2.9563e-01, -5.0554e-01],\n",
      "          ...,\n",
      "          [-7.1937e-01, -2.4400e-01, -3.8060e-01,  ..., -1.4275e-01,\n",
      "            4.0864e-01, -5.0210e-01],\n",
      "          [-6.5402e-01, -2.1928e-01, -4.0502e-01,  ..., -2.8487e-01,\n",
      "            3.6210e-01, -4.7313e-01],\n",
      "          [-5.9411e-01, -2.0261e-01, -4.2957e-01,  ..., -2.9709e-01,\n",
      "            3.8047e-01, -5.0218e-01]],\n",
      "\n",
      "         [[-3.1848e-01,  1.8352e-01, -4.5948e-01,  ...,  2.7359e-01,\n",
      "           -3.4865e-01,  2.1498e-01],\n",
      "          [-1.5687e-01,  1.2131e-01, -5.0593e-01,  ...,  2.2348e-01,\n",
      "           -3.5334e-01,  1.9697e-01],\n",
      "          [-2.5547e-01,  2.2148e-01, -4.1534e-01,  ...,  2.9784e-01,\n",
      "           -4.6168e-01,  2.2592e-01],\n",
      "          ...,\n",
      "          [-1.6139e-01,  2.8220e-01, -4.9122e-01,  ...,  2.5225e-01,\n",
      "           -4.0494e-01,  1.8844e-01],\n",
      "          [-2.0919e-01,  1.9652e-01, -3.5247e-01,  ...,  2.7078e-01,\n",
      "           -3.9739e-01,  1.7354e-01],\n",
      "          [-2.6285e-01,  2.0873e-01, -3.6280e-01,  ...,  2.5932e-01,\n",
      "           -3.8564e-01,  1.9852e-01]],\n",
      "\n",
      "         [[-5.5784e-01, -2.0826e-01, -1.4888e-01,  ...,  5.3804e-01,\n",
      "            3.0635e-01, -2.0058e-01],\n",
      "          [-5.0277e-01, -1.0450e-01, -1.4490e-01,  ...,  5.0741e-01,\n",
      "            3.7652e-01, -1.8308e-01],\n",
      "          [-4.9145e-01, -9.8001e-02, -1.4850e-01,  ...,  4.9129e-01,\n",
      "            3.1526e-01, -2.0269e-01],\n",
      "          ...,\n",
      "          [-5.3036e-01, -1.1664e-01, -2.0474e-01,  ...,  4.7823e-01,\n",
      "            3.3889e-01, -1.9789e-01],\n",
      "          [-5.5553e-01, -2.2474e-01, -1.5041e-01,  ...,  5.7719e-01,\n",
      "            2.5287e-01, -2.2152e-01],\n",
      "          [-5.4232e-01, -1.7577e-01, -1.2679e-01,  ...,  5.0647e-01,\n",
      "            3.4884e-01, -2.2634e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7869e-01, -2.4811e-01, -7.7546e-01,  ..., -2.8069e-01,\n",
      "           -1.9373e-01, -1.6542e-01],\n",
      "          [-2.2933e-01, -2.2514e-01, -8.0953e-01,  ..., -2.8165e-01,\n",
      "           -2.0745e-01, -1.3809e-01],\n",
      "          [-2.4480e-01, -2.3830e-01, -8.0521e-01,  ..., -2.9597e-01,\n",
      "           -2.2561e-01, -1.5038e-01],\n",
      "          ...,\n",
      "          [-2.0619e-01, -2.8381e-01, -8.1431e-01,  ..., -1.9910e-01,\n",
      "           -2.4229e-01, -1.5225e-01],\n",
      "          [-1.9602e-01, -2.5426e-01, -8.0213e-01,  ..., -2.9834e-01,\n",
      "           -2.4523e-01, -1.7725e-01],\n",
      "          [-2.6215e-01, -2.7242e-01, -8.1850e-01,  ..., -2.0463e-01,\n",
      "           -2.0099e-01, -1.0384e-01]],\n",
      "\n",
      "         [[-2.1894e-01, -5.5204e-01, -3.5006e-01,  ..., -2.4308e-01,\n",
      "            1.9343e-01,  5.6448e-01],\n",
      "          [-1.7361e-01, -4.1521e-01, -2.8192e-01,  ..., -2.4197e-01,\n",
      "            2.4429e-01,  5.6905e-01],\n",
      "          [-1.8253e-01, -5.4851e-01, -3.2534e-01,  ..., -2.2687e-01,\n",
      "            2.7198e-01,  5.6596e-01],\n",
      "          ...,\n",
      "          [-2.0599e-01, -4.6947e-01, -3.1114e-01,  ..., -2.2936e-01,\n",
      "            2.1697e-01,  5.9902e-01],\n",
      "          [-2.4304e-01, -5.1069e-01, -2.9145e-01,  ..., -2.2054e-01,\n",
      "            2.2150e-01,  5.5275e-01],\n",
      "          [-1.8394e-01, -4.1988e-01, -2.7888e-01,  ..., -2.4373e-01,\n",
      "            2.4490e-01,  5.3277e-01]],\n",
      "\n",
      "         [[-3.3315e-01,  3.9788e-01,  6.6146e-02,  ..., -3.6048e-02,\n",
      "            1.2134e-01,  1.6692e-01],\n",
      "          [-2.1234e-01,  3.2624e-01,  2.5119e-03,  ...,  3.0887e-02,\n",
      "            1.2723e-01,  2.5116e-01],\n",
      "          [-1.9406e-01,  2.9597e-01,  1.7500e-02,  ..., -3.7642e-02,\n",
      "            1.7605e-01,  1.7275e-01],\n",
      "          ...,\n",
      "          [-2.8640e-01,  3.5194e-01,  5.9702e-02,  ..., -8.8316e-02,\n",
      "            1.6263e-01,  2.2442e-01],\n",
      "          [-3.3475e-01,  3.8744e-01,  6.1205e-03,  ..., -1.5286e-02,\n",
      "            1.2547e-01,  1.8301e-01],\n",
      "          [-2.8920e-01,  3.8874e-01,  1.6087e-02,  ..., -4.9524e-02,\n",
      "            1.1267e-01,  2.0266e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.0591e-01, -2.8982e-01, -4.7109e-01,  ..., -1.5625e-01,\n",
      "            2.5817e-01, -5.1488e-01],\n",
      "          [-6.4714e-01, -2.2679e-01, -4.5558e-01,  ..., -1.3643e-01,\n",
      "            3.5915e-01, -5.1827e-01],\n",
      "          [-7.0356e-01, -2.1786e-01, -4.0202e-01,  ..., -1.3008e-01,\n",
      "            2.9439e-01, -5.3951e-01],\n",
      "          ...,\n",
      "          [-6.4630e-01, -2.9537e-01, -4.9846e-01,  ..., -1.7091e-01,\n",
      "            3.0767e-01, -5.2492e-01],\n",
      "          [-6.5800e-01, -2.7954e-01, -4.9432e-01,  ..., -1.3685e-01,\n",
      "            2.8180e-01, -4.9139e-01],\n",
      "          [-7.2384e-01, -1.8750e-01, -4.1239e-01,  ..., -1.3164e-01,\n",
      "            3.7820e-01, -5.6491e-01]],\n",
      "\n",
      "         [[-5.2492e-01,  9.8887e-02, -4.5393e-01,  ...,  3.4137e-01,\n",
      "           -3.6850e-01,  1.2766e-01],\n",
      "          [-6.0721e-01,  9.4812e-02, -4.3649e-01,  ...,  2.4114e-01,\n",
      "           -2.3341e-01,  1.6083e-01],\n",
      "          [-5.3352e-01,  1.5039e-01, -4.3094e-01,  ...,  3.1892e-01,\n",
      "           -3.0542e-01,  2.5255e-01],\n",
      "          ...,\n",
      "          [-5.9661e-01,  1.1299e-01, -4.5597e-01,  ...,  3.3709e-01,\n",
      "           -3.5037e-01,  1.8077e-01],\n",
      "          [-5.8404e-01,  1.0830e-01, -4.4443e-01,  ...,  4.0164e-01,\n",
      "           -3.8595e-01,  1.5692e-01],\n",
      "          [-5.9184e-01,  5.0597e-02, -4.4891e-01,  ...,  3.2565e-01,\n",
      "           -2.6337e-01,  2.0126e-01]],\n",
      "\n",
      "         [[-4.6091e-01, -1.6187e-01, -1.8491e-01,  ...,  6.2224e-01,\n",
      "            4.2366e-01, -2.3543e-01],\n",
      "          [-4.7712e-01, -2.0378e-01, -6.3265e-02,  ...,  6.5008e-01,\n",
      "            4.0293e-01, -2.4834e-01],\n",
      "          [-4.6493e-01, -1.1483e-01, -1.2521e-01,  ...,  6.2893e-01,\n",
      "            4.6360e-01, -2.1345e-01],\n",
      "          ...,\n",
      "          [-4.7402e-01, -1.5003e-01, -1.2138e-01,  ...,  6.4430e-01,\n",
      "            4.3797e-01, -2.4433e-01],\n",
      "          [-4.6219e-01, -1.5431e-01, -1.0154e-01,  ...,  6.6025e-01,\n",
      "            3.9240e-01, -2.7937e-01],\n",
      "          [-4.6753e-01, -7.0843e-02, -1.8785e-01,  ...,  6.2059e-01,\n",
      "            3.7903e-01, -2.7429e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.6373e-01, -1.8727e-01, -8.2857e-01,  ..., -3.9829e-01,\n",
      "           -1.6443e-01,  1.0176e-01],\n",
      "          [-1.4301e-01, -1.5488e-01, -8.3307e-01,  ..., -3.7438e-01,\n",
      "           -1.9657e-01,  1.0429e-01],\n",
      "          [-1.4811e-01, -1.4957e-01, -7.7102e-01,  ..., -3.7459e-01,\n",
      "           -1.7961e-01,  3.6398e-02],\n",
      "          ...,\n",
      "          [-1.3517e-01, -2.3644e-01, -8.1376e-01,  ..., -3.9053e-01,\n",
      "           -1.6554e-01,  7.9206e-02],\n",
      "          [-1.2075e-01, -1.4083e-01, -7.8133e-01,  ..., -3.8485e-01,\n",
      "           -2.1407e-01,  4.0044e-02],\n",
      "          [-2.0131e-01, -1.5825e-01, -8.1102e-01,  ..., -3.9614e-01,\n",
      "           -1.8578e-01,  3.4497e-02]],\n",
      "\n",
      "         [[-1.3606e-03, -4.0697e-01, -3.3730e-01,  ..., -2.0582e-01,\n",
      "            1.4322e-01,  7.0259e-01],\n",
      "          [-2.5217e-02, -3.7848e-01, -3.7720e-01,  ..., -1.2391e-01,\n",
      "            7.6574e-02,  6.7112e-01],\n",
      "          [ 1.1235e-01, -4.0261e-01, -3.5544e-01,  ..., -2.2928e-01,\n",
      "            1.6239e-01,  7.0520e-01],\n",
      "          ...,\n",
      "          [ 1.3159e-02, -3.5581e-01, -4.0605e-01,  ..., -2.2711e-01,\n",
      "            9.4914e-02,  6.7869e-01],\n",
      "          [-8.0463e-02, -4.7259e-01, -4.3674e-01,  ..., -1.5942e-01,\n",
      "            3.0711e-02,  7.0846e-01],\n",
      "          [ 1.2472e-01, -3.9488e-01, -4.0265e-01,  ..., -2.0039e-01,\n",
      "            2.1677e-01,  6.8719e-01]],\n",
      "\n",
      "         [[-2.8923e-01,  4.2712e-01,  2.9292e-02,  ...,  1.9956e-01,\n",
      "            6.9695e-02,  4.1627e-01],\n",
      "          [-3.2818e-01,  3.8888e-01,  4.4843e-02,  ...,  2.1744e-01,\n",
      "            6.6663e-02,  3.8352e-01],\n",
      "          [-2.6643e-01,  3.8744e-01,  8.3168e-02,  ...,  2.7569e-01,\n",
      "           -2.0602e-02,  3.7081e-01],\n",
      "          ...,\n",
      "          [-3.1768e-01,  3.6871e-01,  7.4852e-02,  ...,  2.6313e-01,\n",
      "            1.0490e-01,  4.5695e-01],\n",
      "          [-3.4080e-01,  4.1592e-01,  7.6465e-02,  ...,  2.1802e-01,\n",
      "            8.1738e-02,  4.2180e-01],\n",
      "          [-3.6320e-01,  4.6065e-01,  2.9725e-02,  ...,  2.2620e-01,\n",
      "            1.1857e-01,  4.4426e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7064e-01, -2.4466e-01, -4.0693e-01,  ..., -1.6572e-01,\n",
      "            2.6828e-01, -4.2462e-01],\n",
      "          [-5.8793e-01, -1.7637e-01, -5.1646e-01,  ..., -2.4021e-01,\n",
      "            1.3861e-01, -4.6777e-01],\n",
      "          [-6.6436e-01, -2.1052e-01, -4.5280e-01,  ..., -1.7079e-01,\n",
      "            2.3874e-01, -4.5955e-01],\n",
      "          ...,\n",
      "          [-7.1842e-01, -2.0921e-01, -3.6424e-01,  ..., -1.0707e-01,\n",
      "            3.2843e-01, -4.3311e-01],\n",
      "          [-6.6203e-01, -2.4409e-01, -3.8002e-01,  ..., -1.6135e-01,\n",
      "            2.6014e-01, -4.5297e-01],\n",
      "          [-6.8231e-01, -2.5272e-01, -4.2615e-01,  ..., -2.2661e-01,\n",
      "            2.2818e-01, -3.4241e-01]],\n",
      "\n",
      "         [[-2.1423e-01, -4.9434e-03, -4.2251e-01,  ...,  2.6525e-01,\n",
      "           -2.2001e-01, -8.5056e-04],\n",
      "          [-2.9711e-01, -8.2728e-02, -4.2633e-01,  ...,  1.7727e-01,\n",
      "           -2.7017e-01,  8.9028e-02],\n",
      "          [-3.1351e-01, -1.6832e-01, -3.7731e-01,  ...,  2.2064e-01,\n",
      "           -2.0223e-01,  7.4383e-02],\n",
      "          ...,\n",
      "          [-2.2672e-01, -1.3369e-04, -3.8173e-01,  ...,  2.4241e-01,\n",
      "           -3.4287e-01,  5.0570e-04],\n",
      "          [-2.6465e-01,  4.5536e-03, -4.4839e-01,  ...,  2.6065e-01,\n",
      "           -2.6011e-01,  4.9504e-02],\n",
      "          [-2.4498e-01, -1.6150e-02, -3.5505e-01,  ...,  2.9928e-01,\n",
      "           -2.2753e-01,  2.1646e-02]],\n",
      "\n",
      "         [[-3.8100e-01, -1.8184e-01, -2.4935e-02,  ...,  6.4298e-01,\n",
      "            3.2531e-01, -8.8724e-02],\n",
      "          [-3.0731e-01, -1.7369e-01, -1.5160e-01,  ...,  6.1534e-01,\n",
      "            2.8138e-01, -1.6401e-01],\n",
      "          [-4.2751e-01, -1.5869e-01, -9.5355e-02,  ...,  5.7031e-01,\n",
      "            2.8952e-01, -1.3410e-01],\n",
      "          ...,\n",
      "          [-3.3029e-01, -2.0826e-01, -8.4047e-02,  ...,  6.8941e-01,\n",
      "            3.3797e-01, -1.9113e-01],\n",
      "          [-4.0686e-01, -1.4901e-01, -9.8646e-02,  ...,  6.0853e-01,\n",
      "            2.7193e-01, -1.2570e-01],\n",
      "          [-3.3448e-01, -2.2381e-01, -1.3966e-01,  ...,  6.1178e-01,\n",
      "            2.8258e-01, -2.0955e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.7943e-01, -1.8318e-01, -7.3117e-01,  ..., -4.1891e-01,\n",
      "           -3.5357e-01, -8.2352e-02],\n",
      "          [-2.0245e-01, -1.1632e-01, -7.1332e-01,  ..., -4.4538e-01,\n",
      "           -3.5193e-01, -1.0482e-01],\n",
      "          [-1.6156e-01, -1.5070e-01, -7.9867e-01,  ..., -4.3893e-01,\n",
      "           -2.7666e-01, -1.1910e-01],\n",
      "          ...,\n",
      "          [-2.4249e-01, -1.3563e-01, -7.5211e-01,  ..., -4.0928e-01,\n",
      "           -3.0719e-01, -1.1128e-01],\n",
      "          [-2.3002e-01, -1.4850e-01, -7.9764e-01,  ..., -4.6150e-01,\n",
      "           -2.6549e-01, -1.1261e-01],\n",
      "          [-1.2616e-01, -1.1666e-01, -7.1529e-01,  ..., -4.4561e-01,\n",
      "           -2.6207e-01, -1.1658e-01]],\n",
      "\n",
      "         [[-7.3469e-02, -2.7195e-01, -1.3253e-01,  ..., -2.1433e-01,\n",
      "            2.7171e-01,  5.4321e-01],\n",
      "          [-8.0980e-02, -3.3794e-01, -1.6746e-01,  ..., -2.2097e-01,\n",
      "            2.2941e-01,  5.9228e-01],\n",
      "          [-4.2577e-02, -3.6799e-01, -1.5778e-01,  ..., -2.4668e-01,\n",
      "            3.0588e-01,  5.1380e-01],\n",
      "          ...,\n",
      "          [-5.8969e-02, -3.7616e-01, -1.3082e-01,  ..., -1.6364e-01,\n",
      "            2.5567e-01,  4.9533e-01],\n",
      "          [-5.0745e-02, -3.6059e-01, -1.8028e-01,  ..., -1.9169e-01,\n",
      "            2.6089e-01,  5.5552e-01],\n",
      "          [-9.9881e-02, -3.4477e-01, -1.5810e-01,  ..., -1.8374e-01,\n",
      "            1.9795e-01,  6.1829e-01]],\n",
      "\n",
      "         [[-2.7809e-01,  3.7615e-01,  5.9153e-03,  ...,  2.4739e-02,\n",
      "            3.0246e-01,  4.0077e-01],\n",
      "          [-2.9028e-01,  3.6277e-01,  4.7417e-02,  ...,  4.7862e-02,\n",
      "            2.9722e-01,  4.4428e-01],\n",
      "          [-3.0063e-01,  4.2112e-01,  4.8565e-02,  ...,  6.0903e-02,\n",
      "            2.8635e-01,  3.9343e-01],\n",
      "          ...,\n",
      "          [-3.0156e-01,  3.7740e-01, -4.1963e-02,  ...,  8.7918e-02,\n",
      "            2.6773e-01,  4.0524e-01],\n",
      "          [-3.3685e-01,  4.8529e-01, -1.2403e-02,  ...,  1.1206e-01,\n",
      "            2.5244e-01,  4.3229e-01],\n",
      "          [-2.9103e-01,  3.6542e-01,  4.4633e-02,  ...,  5.6267e-02,\n",
      "            2.6799e-01,  4.2921e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.1677e-01, -1.6901e-01, -4.5830e-01,  ..., -2.5177e-02,\n",
      "            3.3369e-01, -4.0196e-01],\n",
      "          [-6.5059e-01, -1.0956e-01, -4.7287e-01,  ..., -1.2656e-03,\n",
      "            3.8881e-01, -4.2191e-01],\n",
      "          [-6.4240e-01, -1.8328e-01, -3.9870e-01,  ..., -6.1385e-03,\n",
      "            3.1818e-01, -4.3835e-01],\n",
      "          ...,\n",
      "          [-6.6164e-01, -1.5940e-01, -3.8462e-01,  ..., -6.2845e-02,\n",
      "            2.8086e-01, -4.3449e-01],\n",
      "          [-6.8819e-01, -2.0099e-01, -3.9848e-01,  ..., -3.6751e-03,\n",
      "            3.0185e-01, -4.3813e-01],\n",
      "          [-7.0739e-01, -7.4751e-02, -4.2717e-01,  ...,  1.9814e-03,\n",
      "            3.9005e-01, -3.6349e-01]],\n",
      "\n",
      "         [[-3.7615e-01,  1.0281e-01, -4.5054e-01,  ...,  3.0154e-01,\n",
      "           -3.4318e-01,  9.6030e-02],\n",
      "          [-4.2906e-01,  5.4819e-02, -5.0366e-01,  ...,  3.3674e-01,\n",
      "           -3.0089e-01,  5.1275e-02],\n",
      "          [-4.0381e-01,  1.3386e-01, -4.5359e-01,  ...,  2.5925e-01,\n",
      "           -2.6691e-01,  8.7824e-02],\n",
      "          ...,\n",
      "          [-4.3774e-01, -5.1228e-04, -5.0059e-01,  ...,  3.2215e-01,\n",
      "           -2.5029e-01,  7.8241e-02],\n",
      "          [-4.3787e-01,  2.7132e-02, -4.8674e-01,  ...,  2.4813e-01,\n",
      "           -2.3501e-01,  1.0841e-01],\n",
      "          [-4.4224e-01, -5.6176e-02, -4.8775e-01,  ...,  3.1055e-01,\n",
      "           -2.6685e-01,  6.7338e-02]],\n",
      "\n",
      "         [[-5.4552e-01, -3.8689e-02, -4.2807e-02,  ...,  5.5073e-01,\n",
      "            2.8039e-01, -3.1363e-02],\n",
      "          [-5.3432e-01,  1.3425e-02, -8.3471e-02,  ...,  4.7213e-01,\n",
      "            2.7906e-01, -7.5216e-02],\n",
      "          [-4.8993e-01,  1.2960e-02, -7.6315e-02,  ...,  4.7712e-01,\n",
      "            3.1721e-01, -1.6823e-01],\n",
      "          ...,\n",
      "          [-4.8879e-01, -3.8355e-02, -1.0562e-01,  ...,  4.2812e-01,\n",
      "            2.6302e-01, -4.3885e-02],\n",
      "          [-4.4948e-01, -3.2469e-03, -7.2929e-02,  ...,  4.8312e-01,\n",
      "            2.8813e-01, -1.0709e-01],\n",
      "          [-5.1132e-01,  4.6294e-02, -6.9811e-02,  ...,  4.7039e-01,\n",
      "            2.7675e-01, -8.1119e-02]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要进行残差连接？\n",
    "也许你已经注意到，前馈神经网络的输出`ffn_output 的形状： torch.Size([32, 50, 512])`与预处理之后的数据`x = preprocessor(input_ids)`、多头注意力的输出`拼接后的 output 的形状： torch.Size([32, 50, 512])`的形状一致。这让我们想到也许能够将其进行相加之类的操作。\n",
    "\n",
    "**（1）保留原始信息**\n",
    "- 多头注意力机制已经捕捉了序列中元素之间的关系。\n",
    "- 残差连接确保这些信息不会被前馈神经网络完全覆盖，保留原始特征。\n",
    "\n",
    "**（2）缓解梯度消失**\n",
    "- 深层网络中，梯度在反向传播时容易消失。\n",
    "- 残差连接提供了一条“捷径”，使梯度可以直接传播到浅层，缓解梯度消失问题。\n",
    "\n",
    "**（3）增强模型表达能力**\n",
    "- 前馈神经网络引入了非线性变换，增强了模型的表达能力。\n",
    "- 残差连接将这种非线性变换与原始特征结合，进一步提升模型性能。\n",
    "\n",
    "**（4）加速训练**\n",
    "- 残差连接使模型更容易优化，加速训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "norm1 = nn.LayerNorm(d_model)\n",
    "norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "x = norm1(x + projected_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在此之后，这个`x`再经过我们之前提到过的`Feed-Forward`得到的`ffn_output = ffn(projected_output)`进行残差链接，最后将`x`进行层归一化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "x = norm2(x + ffn_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 为什么要层归一化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，主要用于加速训练过程并提高模型的稳定性。以下是详细解释：\n",
    "\n",
    "**. 层归一化的作用**\n",
    "- **归一化特征**：\n",
    "  - 对每个样本的特征进行归一化，使其均值为0，方差为1。\n",
    "  - 减少内部协变量偏移（Internal Covariate Shift），使训练更稳定。\n",
    "- **加速收敛**：\n",
    "  - 归一化后的特征分布更稳定，有助于加速模型收敛。\n",
    "- **适用于不同任务**：\n",
    "  - 特别适合处理变长序列（如NLP任务）和小批量数据。\n",
    "\n",
    "**. 层归一化的公式**\n",
    "层归一化的计算公式如下：\n",
    "```python\n",
    "y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "```\n",
    "- **`x`**：输入特征。\n",
    "- **`mean`**：输入特征的均值。\n",
    "- **`var`**：输入特征的方差。\n",
    "- **`eps`**：一个小常数，用于数值稳定性（默认`1e-5`）。\n",
    "- **`gamma`**：可学习的缩放参数（权重）。\n",
    "- **`beta`**：可学习的偏移参数（偏置）。\n",
    "\n",
    "---\n",
    "\n",
    "**. 层归一化的特点**\n",
    "- **独立于批量大小**：\n",
    "  - 与批量归一化（Batch Normalization）不同，层归一化不依赖于批量大小，适合处理小批量或变长序列。\n",
    "- **逐样本归一化**：\n",
    "  - 对每个样本的特征进行归一化，而不是跨样本归一化。\n",
    "- **可学习的参数**：\n",
    "  - `gamma` 和 `beta` 是可学习的参数，允许模型调整归一化后的特征分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是`Encoder`中一个`EncoderLayer`的全部内容，为了获取`Encoder`，我们需要将`EncoderLayer`堆叠起来。你可以在`transformer_encoder.py`中找到完整的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Decoder 🐱 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. **掩码多头自注意力**：\n",
    "   - 捕捉已生成序列的内部关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 为什么就需要掩码了？之前的Encoder中的注意力不需要掩码呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **任务性质不同**\n",
    "- **Encoder**：\n",
    "  - 处理的是完整的输入序列（如源语言句子）\n",
    "  - 需要同时看到整个序列的所有信息\n",
    "  - 目标是捕捉序列中所有元素之间的关系\n",
    "\n",
    "- **Decoder**：\n",
    "  - 处理的是目标序列（如目标语言句子）\n",
    "  - 需要逐步生成序列，不能\"偷看\"未来信息\n",
    "  - 目标是基于已生成的部分序列和Encoder的输出来预测下一个元素\n",
    "\n",
    "##### 2. **掩码的作用**\n",
    "- **防止信息泄露**：\n",
    "  - 在训练时，Decoder会接收完整的目标序列\n",
    "  - 如果没有掩码，模型可能会直接\"看到\"未来的信息，导致训练作弊\n",
    "  - 掩码确保模型只能使用当前位置及之前的信息\n",
    "\n",
    "- **保持自回归性质**：\n",
    "  - 在推理时，Decoder需要逐个生成序列元素\n",
    "  - 掩码确保模型只能基于已生成的部分序列进行预测\n",
    "  - 这是序列生成任务（如机器翻译、文本生成）的基本要求\n",
    "\n",
    "##### 3. **具体实现**\n",
    "- **Encoder中的注意力**：\n",
    "  - 计算注意力分数时，所有位置之间都可以相互关注\n",
    "  - 不需要任何限制，因为整个输入序列是已知的\n",
    "\n",
    "- **Decoder中的掩码注意力**：\n",
    "  - 使用下三角掩码（`torch.tril`）\n",
    "  - 确保每个位置只能关注到它自身及之前的位置\n",
    "  - 未来位置的注意力分数被设置为`-inf`，在softmax后权重为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "Encoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "Decoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "mask 的形状： torch.Size([50, 50])\n",
      "mask 的值： tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "# Encoder注意力分数（无掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "# print(\"Encoder注意力分数 scores：\", scores)\n",
    "print(\"Encoder注意力分数 scores 的形状：\", scores.shape)\n",
    "# Decoder注意力分数（带掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # 下三角掩码\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))  # 应用掩码\n",
    "# print(\"Decoder注意力分数 scores：\", scores)\n",
    "print(\"Decoder注意力分数 scores 的形状：\", scores.shape)\n",
    "print(\"mask 的形状：\", mask.shape)\n",
    "print(\"mask 的值：\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 为什么需要下三角掩码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **自回归任务的性质**\n",
    "在自回归任务中，模型需要**逐步生成序列**，即每次只能基于已经生成的部分序列来预测下一个元素。例如：\n",
    "- 在文本生成中，模型只能基于已经生成的单词来预测下一个单词。\n",
    "- 在机器翻译中，模型只能基于已经生成的目标语言单词来预测下一个单词。\n",
    "\n",
    "如果模型能够“看到”未来的信息，它就会作弊，直接使用未来的信息来预测当前的位置，这会导致训练和推理不一致。\n",
    "\n",
    "\n",
    "##### 2. **下三角掩码的作用**\n",
    "下三角掩码的作用是**限制模型只能访问当前位置及之前的信息**，而不能访问未来的信息。具体来说：\n",
    "- **下三角部分**：值为1，表示允许模型访问这些位置的信息。\n",
    "- **上三角部分**：值为0，表示禁止模型访问这些位置的信息。\n",
    "\n",
    "通过将上三角部分的注意力分数设置为`-inf`，在softmax操作后，这些位置的权重会变为0，从而确保模型无法利用未来的信息。\n",
    "\n",
    "##### 3. **掩码的实现**\n",
    "在代码中，下三角掩码通常通过`torch.tril`函数生成：\n",
    "````python\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "````\n",
    "例如，对于一个长度为5的序列，掩码矩阵如下：\n",
    "````\n",
    "1 0 0 0 0\n",
    "1 1 0 0 0\n",
    "1 1 1 0 0\n",
    "1 1 1 1 0\n",
    "1 1 1 1 1\n",
    "````\n",
    "- 第1行：只能看到第1个位置。\n",
    "- 第2行：可以看到第1和第2个位置。\n",
    "- 第3行：可以看到第1、第2和第3个位置。\n",
    "- 以此类推。\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **为什么需要下三角掩码？**\n",
    "###### （1）**训练时防止信息泄露**\n",
    "- 在训练时，Decoder会接收完整的目标序列（如目标语言句子）。\n",
    "- 如果没有掩码，模型可能会直接“看到”未来的信息，导致训练作弊。\n",
    "- 掩码确保模型只能使用当前位置及之前的信息。\n",
    "\n",
    "###### （2）**推理时保持自回归性质**\n",
    "- 在推理时，Decoder需要逐个生成序列元素。\n",
    "- 掩码确保模型只能基于已生成的部分序列进行预测。\n",
    "- 这是序列生成任务（如机器翻译、文本生成）的基本要求。\n",
    "\n",
    "###### （3）**确保训练和推理一致性**\n",
    "- 训练时使用掩码，推理时也使用掩码，确保模型的行为一致。\n",
    "- 如果训练时不使用掩码，模型可能会学习到依赖未来信息的错误模式，导致推理时性能下降。\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **示例**\n",
    "假设我们有一个长度为3的序列，计算注意力分数时：\n",
    "- **无掩码**：模型可以看到所有位置的信息。\n",
    "  ````\n",
    "  scores = [[s11, s12, s13],\n",
    "            [s21, s22, s23],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "- **有掩码**：模型只能看到当前位置及之前的信息。\n",
    "  ````\n",
    "  scores = [[s11, -inf, -inf],\n",
    "            [s21, s22, -inf],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "在softmax后，`-inf`的位置权重为0，模型无法利用这些信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 6. **总结**\n",
    "| 特性                | 无掩码                  | 下三角掩码              |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| 信息访问            | 可以访问整个序列        | 只能访问当前位置及之前  |\n",
    "| 训练时              | 可能作弊，利用未来信息  | 防止信息泄露            |\n",
    "| 推理时              | 无法逐步生成序列        | 保持自回归性质          |\n",
    "| 适用场景            | Encoder                | Decoder                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 输入是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2. **编码器-解码器注意力**：\n",
    "   - 捕捉源序列与目标序列之间的关系。\n",
    "\n",
    "### 3.3. **前馈神经网络**：\n",
    "   - 对注意力机制的输出进行非线性变换。\n",
    "\n",
    "### 3.4. **输出**：\n",
    "   - 解码器的最终输出，用于预测下一个单词。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
