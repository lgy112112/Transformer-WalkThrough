{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[-0.2914,  0.1942, -0.1194],\n",
      "        [ 0.6089,  2.2277,  0.4700],\n",
      "        [ 1.3072,  0.6316, -1.0134]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "现在让我们把刚刚提到的`Embedding`和`PositionalEncoding`组合成一个完整的预处理模块`TransformerPreprocessor`。你可以在`transformer_preprocess.py`中找到一样的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征**映射到新的特征空间**。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。我认为在这里，以实用的角度来讲，不必深究`Q`、`K`、`V`的意义，只需要知道它们是通过线性变换得到的即可。`x`的形状是什么，那么`Q`、`K`、`V`的形状就是什么，只不过这些矩阵将他们分别映射到了不同的特征空间。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 原有的输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇在这里，我们使用提到的`TransformermerPreprocessor`来对输入进行预处理，获取一个`x`以便给我们接下来的*线性变换*做示范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformer_preprocess import TransformerPreprocessor\n",
    "\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[-0.3949, -0.1637, -0.6622,  ..., -0.1304, -1.4259,  0.3307],\n",
      "         [-0.3339, -0.1035, -0.0557,  ...,  0.1101, -0.7132,  1.1825],\n",
      "         [-0.1742,  0.2948,  1.1003,  ..., -0.0083, -0.2818, -0.5201],\n",
      "         ...,\n",
      "         [-0.3162, -0.5694, -1.1288,  ...,  0.5722,  0.6777, -0.4214],\n",
      "         [ 0.0990, -0.5885, -0.2295,  ...,  0.2177,  0.8256,  0.0022],\n",
      "         [-0.2764, -0.3338, -0.4434,  ...,  0.2422, -0.8636,  1.1030]],\n",
      "\n",
      "        [[ 0.5777,  0.0667,  0.2965,  ...,  0.1701, -0.0807,  0.4047],\n",
      "         [-1.2470, -0.5215,  0.8945,  ...,  0.4602, -0.4264, -0.7330],\n",
      "         [-0.1857, -0.8672,  0.3404,  ..., -0.0679,  0.1493,  1.1590],\n",
      "         ...,\n",
      "         [-0.2324,  0.8631, -0.5468,  ..., -0.6442, -0.2026,  0.9937],\n",
      "         [ 0.0610,  0.0838, -0.2239,  ...,  0.1714, -0.6186,  0.0073],\n",
      "         [-0.6553,  0.0996, -0.3595,  ...,  0.1600, -0.1639,  0.1289]],\n",
      "\n",
      "        [[ 0.6834,  0.7929,  0.2303,  ..., -0.2573, -0.8958, -0.1455],\n",
      "         [-0.0245, -0.3340, -0.3804,  ..., -1.1758, -0.2522,  0.7153],\n",
      "         [-0.8751, -0.5675,  0.0358,  ...,  0.1737, -1.3072,  0.4198],\n",
      "         ...,\n",
      "         [-1.0979,  0.0048,  1.1823,  ...,  0.7239, -0.6324,  0.7411],\n",
      "         [-0.1396, -0.5263,  0.2374,  ...,  0.5941, -0.5686, -1.0898],\n",
      "         [-0.5265, -1.9743,  0.2067,  ..., -0.2590, -0.2244, -0.0331]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0802,  0.6965,  0.2971,  ..., -0.2910, -0.3603, -0.1040],\n",
      "         [-1.2454, -1.1257, -0.0776,  ..., -0.4823,  0.6073, -0.3058],\n",
      "         [-0.7745,  0.1406,  0.5262,  ..., -0.6377,  0.2997, -0.7274],\n",
      "         ...,\n",
      "         [ 0.0334,  0.1192,  0.1113,  ..., -0.7187, -1.2236,  0.3039],\n",
      "         [-0.7238, -0.3673, -0.2212,  ..., -0.7395, -0.0553,  0.6132],\n",
      "         [ 0.0261,  0.1922,  0.6123,  ...,  0.2405, -0.2834,  0.2616]],\n",
      "\n",
      "        [[-0.5971,  0.5127,  0.0282,  ..., -0.9642, -0.6348,  0.5382],\n",
      "         [ 0.4098, -0.8182,  0.3237,  ..., -0.6947,  0.3857,  0.1307],\n",
      "         [-0.4035, -0.4693,  1.1791,  ...,  0.7957, -0.6339, -0.6357],\n",
      "         ...,\n",
      "         [ 0.3082,  0.1045, -0.0655,  ..., -0.7238, -0.9886,  0.2658],\n",
      "         [ 0.1801, -0.5228,  0.5310,  ...,  1.6943, -0.3491,  0.1382],\n",
      "         [-0.5829,  0.0084,  0.0322,  ...,  0.4662, -0.9330,  0.2719]],\n",
      "\n",
      "        [[-0.5949, -0.7584, -0.1196,  ...,  0.3520,  0.2301,  0.6924],\n",
      "         [ 0.1592, -0.7597,  0.0698,  ..., -0.1583, -1.2589,  0.5358],\n",
      "         [ 0.9920, -0.0242, -0.5090,  ...,  0.2713, -0.1523, -0.3473],\n",
      "         ...,\n",
      "         [ 0.7854,  0.4968, -0.7288,  ..., -0.0485,  0.6472,  0.6459],\n",
      "         [-1.1218, -1.2282,  0.1703,  ...,  1.4068, -0.6342,  0.4222],\n",
      "         [-0.0569, -0.5657, -0.5341,  ...,  0.8050, -0.5851, -0.8885]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[ 9.8396e-01,  2.0360e-01,  2.8317e-01,  ..., -2.0204e+00,\n",
      "           1.1984e+00, -9.9773e-01],\n",
      "         [-9.2302e-02, -6.4416e-01, -1.0763e+00,  ..., -1.1544e+00,\n",
      "           1.6073e+00, -4.4733e-01],\n",
      "         [ 4.6439e-01,  4.0005e-01,  1.1396e+00,  ...,  9.5802e-04,\n",
      "           8.7933e-01,  6.4018e-01],\n",
      "         ...,\n",
      "         [ 2.9969e-01, -3.8445e-01, -6.9560e-01,  ...,  6.2911e-01,\n",
      "           1.3390e+00, -8.2403e-02],\n",
      "         [ 1.0337e-01,  4.1088e-01,  1.3197e+00,  ...,  6.1284e-01,\n",
      "           1.3061e-02,  1.3415e-01],\n",
      "         [-4.6377e-01,  1.4196e-01,  7.6538e-01,  ...,  1.2227e-01,\n",
      "           2.3622e-01,  1.2510e-01]],\n",
      "\n",
      "        [[-5.3712e-03, -6.0530e-01, -3.1258e-01,  ...,  2.0283e-01,\n",
      "           1.2717e+00, -7.9513e-01],\n",
      "         [ 7.9256e-01, -6.8601e-01,  7.8285e-01,  ..., -5.0915e-01,\n",
      "           1.3969e+00,  4.5772e-01],\n",
      "         [-1.4422e-01, -6.1470e-01, -1.7105e-01,  ..., -3.8722e-01,\n",
      "           6.2497e-01,  3.5455e-01],\n",
      "         ...,\n",
      "         [ 4.0068e-01, -8.9400e-01,  6.4656e-01,  ...,  4.5213e-01,\n",
      "          -5.3902e-01, -4.4744e-01],\n",
      "         [-7.9870e-01,  2.7827e-01,  2.5634e-01,  ...,  1.0204e+00,\n",
      "           9.0452e-01,  3.9696e-01],\n",
      "         [-4.7553e-01,  6.1591e-02,  1.2277e+00,  ..., -1.2812e-01,\n",
      "          -4.4634e-01, -6.1637e-02]],\n",
      "\n",
      "        [[-1.0530e-02, -6.5422e-01, -2.4362e-01,  ...,  3.1027e-01,\n",
      "           1.6147e+00,  3.2929e-01],\n",
      "         [-2.7630e-01,  1.5617e-01,  1.8321e-01,  ...,  5.6727e-01,\n",
      "           1.1833e+00,  1.4566e-01],\n",
      "         [ 2.1039e-01,  5.1647e-01,  3.6820e-01,  ...,  1.6242e-01,\n",
      "           1.0812e+00,  6.7642e-01],\n",
      "         ...,\n",
      "         [ 3.0075e-01,  1.6194e-01, -4.4898e-01,  ...,  2.9961e-01,\n",
      "           1.8145e-01, -3.8144e-01],\n",
      "         [-7.6563e-01,  1.3835e+00,  2.3464e-01,  ...,  5.8473e-01,\n",
      "           5.8482e-01, -2.3669e-02],\n",
      "         [-1.1446e+00, -5.0534e-02,  3.4102e-01,  ...,  4.2500e-01,\n",
      "           8.6034e-01, -1.0131e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.3820e-01, -1.1458e+00, -5.8537e-01,  ...,  3.8771e-01,\n",
      "           1.3126e+00, -1.9644e+00],\n",
      "         [-8.1105e-01, -5.9374e-01, -5.7358e-01,  ...,  2.1814e-01,\n",
      "           5.0430e-01, -5.0675e-01],\n",
      "         [ 7.4723e-02, -1.4819e-01,  4.0912e-01,  ...,  6.2498e-01,\n",
      "           5.0348e-01,  3.1855e-01],\n",
      "         ...,\n",
      "         [-2.5303e-01, -1.0735e-01,  3.9824e-01,  ...,  2.8231e-01,\n",
      "           5.3152e-02, -9.4251e-01],\n",
      "         [ 8.6395e-01,  6.6723e-01,  3.3625e-01,  ...,  6.3585e-01,\n",
      "          -1.9386e-01,  1.2284e+00],\n",
      "         [ 1.2481e+00,  8.7680e-01, -7.8133e-01,  ...,  5.9250e-01,\n",
      "          -5.0447e-01,  4.0101e-01]],\n",
      "\n",
      "        [[-5.4717e-01, -3.1971e-01, -4.5338e-01,  ..., -1.6679e-01,\n",
      "           2.6805e+00, -1.1567e-01],\n",
      "         [-4.2572e-01, -2.1474e-01,  4.7767e-01,  ...,  1.4137e-01,\n",
      "           1.2841e+00, -4.9770e-01],\n",
      "         [-4.0836e-01,  4.9269e-01,  1.3155e-01,  ...,  4.2293e-02,\n",
      "           1.7104e-01,  5.4225e-01],\n",
      "         ...,\n",
      "         [ 3.7598e-01,  1.5661e-01,  1.3313e+00,  ...,  3.8355e-01,\n",
      "          -3.5181e-01,  9.2758e-01],\n",
      "         [-9.9036e-01,  4.5861e-01,  6.7058e-01,  ...,  5.0322e-01,\n",
      "           1.0354e-01, -1.6839e-01],\n",
      "         [ 3.6215e-01, -6.8399e-01, -5.2771e-01,  ...,  9.1398e-01,\n",
      "           1.2879e+00,  2.8085e-01]],\n",
      "\n",
      "        [[-8.0971e-01, -1.6250e-01, -4.8458e-02,  ...,  1.3190e+00,\n",
      "           5.5414e-01,  3.5081e-02],\n",
      "         [-1.7690e+00,  5.1129e-01,  5.7789e-01,  ...,  1.9787e-01,\n",
      "           8.7328e-01,  5.5146e-01],\n",
      "         [ 5.6740e-03, -1.0832e+00,  2.0688e-01,  ...,  3.7438e-01,\n",
      "           6.9742e-01, -6.1518e-01],\n",
      "         ...,\n",
      "         [ 4.1694e-01, -6.8241e-01,  3.4156e-01,  ...,  5.2357e-01,\n",
      "          -6.4014e-01,  1.7077e-01],\n",
      "         [-1.4458e-01, -1.1785e+00,  5.9522e-01,  ..., -5.5449e-01,\n",
      "           1.0051e-01,  5.2331e-01],\n",
      "         [ 6.7164e-02,  1.4098e-02,  1.1951e+00,  ...,  2.1323e-01,\n",
      "           1.1742e+00,  1.3317e-01]]], grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[ 1.1744,  1.3345, -0.2161,  ...,  0.1740,  0.3945, -1.9234],\n",
      "         [ 0.9795,  0.9407,  0.2674,  ...,  0.7726,  0.2037, -0.0807],\n",
      "         [ 0.2427,  1.3120,  0.2005,  ..., -1.0325, -0.0646, -0.1071],\n",
      "         ...,\n",
      "         [-0.5535,  0.6599,  0.3004,  ...,  0.0135, -0.9000, -0.5510],\n",
      "         [-0.1246, -0.0742,  0.5143,  ...,  0.3812,  1.0472, -1.0746],\n",
      "         [-0.3535, -0.2946,  0.9488,  ...,  0.3595,  0.6988, -0.9933]],\n",
      "\n",
      "        [[-0.7461,  1.0804,  0.4713,  ..., -0.8550,  0.5594, -0.1176],\n",
      "         [ 0.8250,  0.3450,  1.3395,  ..., -0.6661, -0.1012, -0.1973],\n",
      "         [ 0.3127, -0.1733,  0.7889,  ..., -0.2419,  0.3309,  1.3865],\n",
      "         ...,\n",
      "         [-0.0951,  0.5270,  0.2789,  ..., -1.1022,  0.2655, -0.6527],\n",
      "         [ 0.8968, -1.0160,  0.2379,  ..., -0.3834,  0.4114, -1.0391],\n",
      "         [-0.8783,  0.1510,  1.1003,  ..., -0.8201,  1.2781, -1.5645]],\n",
      "\n",
      "        [[-0.0373,  0.4121,  0.1020,  ..., -0.6427, -0.6227, -0.4664],\n",
      "         [-0.0557,  0.8295,  0.9911,  ..., -0.4470,  0.8116,  0.9174],\n",
      "         [ 0.9306, -0.4201,  0.5358,  ..., -0.6429, -0.0369, -0.2779],\n",
      "         ...,\n",
      "         [-0.7807,  0.3198,  0.6698,  ..., -0.8033,  0.3994, -0.1803],\n",
      "         [-0.3060,  0.1375,  0.3801,  ...,  1.0082,  1.2841, -0.1187],\n",
      "         [-1.0261, -0.0303,  0.7031,  ..., -1.1769,  1.2149, -1.5147]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1563,  1.1158, -0.2676,  ...,  0.0324,  0.4867,  0.0315],\n",
      "         [ 0.0474,  0.5460,  1.1706,  ..., -0.9445, -0.8454, -0.7405],\n",
      "         [ 0.5602,  0.1883, -0.1157,  ...,  1.5285,  0.1083,  0.8005],\n",
      "         ...,\n",
      "         [-0.7923,  1.1867,  0.6387,  ...,  0.7889,  0.3611, -0.6852],\n",
      "         [ 0.1860,  0.1509,  0.1742,  ..., -0.7117,  0.2907,  0.5570],\n",
      "         [-0.6900,  0.2944,  0.3094,  ...,  0.5390,  0.5257,  0.1213]],\n",
      "\n",
      "        [[ 0.6134,  0.3642,  0.7228,  ..., -0.1577, -0.1398, -0.6768],\n",
      "         [ 0.1606,  1.5600,  0.8099,  ..., -0.0317,  0.2693,  0.3662],\n",
      "         [-0.4631,  0.3213,  0.8381,  ...,  0.5561, -0.4661, -0.3897],\n",
      "         ...,\n",
      "         [ 0.2442, -0.0927,  0.5529,  ..., -0.3089, -0.1673, -0.0806],\n",
      "         [-0.3602, -0.0510,  0.1819,  ..., -0.3398, -0.0866, -0.3927],\n",
      "         [-0.8552,  0.7170, -0.2656,  ..., -0.9120,  1.3581, -0.5116]],\n",
      "\n",
      "        [[-0.8112,  0.4369,  0.2941,  ..., -0.0246,  0.4549, -0.5562],\n",
      "         [-0.2233, -0.5500,  0.3137,  ..., -0.7038,  0.4345, -0.0155],\n",
      "         [ 0.0189,  0.0866,  1.0940,  ..., -2.0165, -0.7546, -0.4408],\n",
      "         ...,\n",
      "         [-0.0550,  0.5957, -0.2732,  ..., -0.2440,  1.1208, -0.2460],\n",
      "         [ 0.0472, -0.4674,  0.2858,  ..., -0.6165,  0.1636, -0.5431],\n",
      "         [ 0.0309,  0.6326,  0.2584,  ...,  0.1970,  0.8516, -1.5220]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **分割多头的目的**\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n",
    "\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim\n",
    "#　将`num_heads`维度提到前面，方便后续并行计算。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0129, 0.0149, 0.0108,  ..., 0.0256, 0.0092, 0.0232],\n",
      "          [0.0100, 0.0153, 0.0122,  ..., 0.0197, 0.0176, 0.0362],\n",
      "          [0.0151, 0.0071, 0.0084,  ..., 0.0199, 0.0151, 0.0566],\n",
      "          ...,\n",
      "          [0.0065, 0.0277, 0.0097,  ..., 0.0331, 0.0093, 0.0103],\n",
      "          [0.0097, 0.0318, 0.0036,  ..., 0.0174, 0.0114, 0.0320],\n",
      "          [0.0144, 0.0215, 0.0153,  ..., 0.0172, 0.0079, 0.0235]],\n",
      "\n",
      "         [[0.0303, 0.0283, 0.0185,  ..., 0.0088, 0.0076, 0.0205],\n",
      "          [0.0441, 0.0197, 0.0164,  ..., 0.0105, 0.0134, 0.0171],\n",
      "          [0.0154, 0.0187, 0.0122,  ..., 0.0122, 0.0289, 0.0210],\n",
      "          ...,\n",
      "          [0.0277, 0.0132, 0.0034,  ..., 0.0041, 0.0031, 0.0107],\n",
      "          [0.0290, 0.0163, 0.0076,  ..., 0.0091, 0.0082, 0.0218],\n",
      "          [0.0176, 0.0259, 0.0140,  ..., 0.0086, 0.0107, 0.0236]],\n",
      "\n",
      "         [[0.0240, 0.0348, 0.0167,  ..., 0.0132, 0.0108, 0.0187],\n",
      "          [0.0140, 0.0175, 0.0100,  ..., 0.0148, 0.0124, 0.0170],\n",
      "          [0.0230, 0.0232, 0.0101,  ..., 0.0104, 0.0160, 0.0309],\n",
      "          ...,\n",
      "          [0.0216, 0.0138, 0.0096,  ..., 0.0154, 0.0178, 0.0090],\n",
      "          [0.0162, 0.0180, 0.0152,  ..., 0.0090, 0.0213, 0.0090],\n",
      "          [0.0474, 0.0529, 0.0270,  ..., 0.0099, 0.0153, 0.0100]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0173, 0.0185, 0.0095,  ..., 0.0173, 0.0384, 0.0229],\n",
      "          [0.0307, 0.0201, 0.0114,  ..., 0.0310, 0.0243, 0.0112],\n",
      "          [0.0376, 0.0219, 0.0067,  ..., 0.0326, 0.0349, 0.0143],\n",
      "          ...,\n",
      "          [0.0226, 0.0200, 0.0225,  ..., 0.0307, 0.0319, 0.0155],\n",
      "          [0.0224, 0.0266, 0.0127,  ..., 0.0189, 0.0180, 0.0227],\n",
      "          [0.0205, 0.0186, 0.0076,  ..., 0.0165, 0.0206, 0.0142]],\n",
      "\n",
      "         [[0.0114, 0.0121, 0.0247,  ..., 0.0242, 0.0200, 0.0161],\n",
      "          [0.0132, 0.0190, 0.0278,  ..., 0.0266, 0.0174, 0.0163],\n",
      "          [0.0148, 0.0232, 0.0235,  ..., 0.0365, 0.0131, 0.0140],\n",
      "          ...,\n",
      "          [0.0120, 0.0157, 0.0155,  ..., 0.0386, 0.0214, 0.0254],\n",
      "          [0.0288, 0.0141, 0.0249,  ..., 0.0137, 0.0168, 0.0067],\n",
      "          [0.0080, 0.0170, 0.0230,  ..., 0.0300, 0.0396, 0.0275]],\n",
      "\n",
      "         [[0.0111, 0.0145, 0.0151,  ..., 0.0114, 0.0235, 0.0127],\n",
      "          [0.0111, 0.0171, 0.0194,  ..., 0.0206, 0.0212, 0.0194],\n",
      "          [0.0170, 0.0258, 0.0148,  ..., 0.0182, 0.0346, 0.0197],\n",
      "          ...,\n",
      "          [0.0152, 0.0334, 0.0285,  ..., 0.0156, 0.0226, 0.0159],\n",
      "          [0.0276, 0.0225, 0.0176,  ..., 0.0281, 0.0120, 0.0253],\n",
      "          [0.0122, 0.0159, 0.0158,  ..., 0.0245, 0.0176, 0.0313]]],\n",
      "\n",
      "\n",
      "        [[[0.0154, 0.0072, 0.0161,  ..., 0.0112, 0.0095, 0.0128],\n",
      "          [0.0331, 0.0154, 0.0212,  ..., 0.0237, 0.0204, 0.0323],\n",
      "          [0.0283, 0.0106, 0.0218,  ..., 0.0092, 0.0058, 0.0157],\n",
      "          ...,\n",
      "          [0.0183, 0.0109, 0.0314,  ..., 0.0143, 0.0206, 0.0176],\n",
      "          [0.0131, 0.0075, 0.0258,  ..., 0.0052, 0.0060, 0.0090],\n",
      "          [0.0162, 0.0066, 0.0163,  ..., 0.0069, 0.0120, 0.0103]],\n",
      "\n",
      "         [[0.0290, 0.0151, 0.0302,  ..., 0.0195, 0.0246, 0.0150],\n",
      "          [0.0214, 0.0161, 0.0138,  ..., 0.0157, 0.0114, 0.0093],\n",
      "          [0.0242, 0.0093, 0.0128,  ..., 0.0169, 0.0251, 0.0059],\n",
      "          ...,\n",
      "          [0.0303, 0.0130, 0.0359,  ..., 0.0173, 0.0350, 0.0173],\n",
      "          [0.0305, 0.0097, 0.0273,  ..., 0.0189, 0.0274, 0.0302],\n",
      "          [0.0323, 0.0135, 0.0335,  ..., 0.0148, 0.0216, 0.0094]],\n",
      "\n",
      "         [[0.0156, 0.0269, 0.0195,  ..., 0.0188, 0.0284, 0.0175],\n",
      "          [0.0235, 0.0055, 0.0421,  ..., 0.0235, 0.0265, 0.0289],\n",
      "          [0.0236, 0.0139, 0.0186,  ..., 0.0337, 0.0159, 0.0383],\n",
      "          ...,\n",
      "          [0.0202, 0.0106, 0.0238,  ..., 0.0296, 0.0143, 0.0232],\n",
      "          [0.0260, 0.0106, 0.0392,  ..., 0.0134, 0.0230, 0.0183],\n",
      "          [0.0141, 0.0094, 0.0451,  ..., 0.0131, 0.0315, 0.0142]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0202, 0.0570, 0.0259,  ..., 0.0158, 0.0277, 0.0231],\n",
      "          [0.0161, 0.0184, 0.0188,  ..., 0.0137, 0.0290, 0.0254],\n",
      "          [0.0168, 0.0149, 0.0250,  ..., 0.0132, 0.0146, 0.0205],\n",
      "          ...,\n",
      "          [0.0098, 0.0177, 0.0256,  ..., 0.0210, 0.0264, 0.0199],\n",
      "          [0.0200, 0.0122, 0.0205,  ..., 0.0164, 0.0140, 0.0240],\n",
      "          [0.0104, 0.0195, 0.0141,  ..., 0.0129, 0.0230, 0.0222]],\n",
      "\n",
      "         [[0.0263, 0.0112, 0.0184,  ..., 0.0219, 0.0124, 0.0284],\n",
      "          [0.0221, 0.0200, 0.0335,  ..., 0.0162, 0.0216, 0.0202],\n",
      "          [0.0215, 0.0203, 0.0368,  ..., 0.0170, 0.0212, 0.0183],\n",
      "          ...,\n",
      "          [0.0184, 0.0134, 0.0293,  ..., 0.0331, 0.0084, 0.0146],\n",
      "          [0.0297, 0.0110, 0.0166,  ..., 0.0501, 0.0077, 0.0100],\n",
      "          [0.0269, 0.0236, 0.0202,  ..., 0.0279, 0.0111, 0.0093]],\n",
      "\n",
      "         [[0.0131, 0.0139, 0.0178,  ..., 0.0257, 0.0290, 0.0259],\n",
      "          [0.0280, 0.0180, 0.0248,  ..., 0.0162, 0.0164, 0.0215],\n",
      "          [0.0091, 0.0282, 0.0200,  ..., 0.0169, 0.0137, 0.0116],\n",
      "          ...,\n",
      "          [0.0161, 0.0460, 0.0259,  ..., 0.0131, 0.0144, 0.0124],\n",
      "          [0.0146, 0.0211, 0.0174,  ..., 0.0129, 0.0128, 0.0231],\n",
      "          [0.0189, 0.0326, 0.0177,  ..., 0.0101, 0.0091, 0.0150]]],\n",
      "\n",
      "\n",
      "        [[[0.0121, 0.0125, 0.0192,  ..., 0.0127, 0.0310, 0.0249],\n",
      "          [0.0099, 0.0114, 0.0163,  ..., 0.0155, 0.0126, 0.0181],\n",
      "          [0.0110, 0.0045, 0.0081,  ..., 0.0190, 0.0215, 0.0158],\n",
      "          ...,\n",
      "          [0.0164, 0.0132, 0.0176,  ..., 0.0090, 0.0284, 0.0214],\n",
      "          [0.0108, 0.0106, 0.0139,  ..., 0.0061, 0.0126, 0.0129],\n",
      "          [0.0220, 0.0109, 0.0076,  ..., 0.0103, 0.0084, 0.0206]],\n",
      "\n",
      "         [[0.0170, 0.0115, 0.0088,  ..., 0.0323, 0.0225, 0.0195],\n",
      "          [0.0232, 0.0120, 0.0111,  ..., 0.0267, 0.0176, 0.0215],\n",
      "          [0.0623, 0.0159, 0.0225,  ..., 0.0203, 0.0130, 0.0204],\n",
      "          ...,\n",
      "          [0.0135, 0.0185, 0.0581,  ..., 0.0196, 0.0248, 0.0116],\n",
      "          [0.0202, 0.0063, 0.0152,  ..., 0.0142, 0.0282, 0.0213],\n",
      "          [0.0258, 0.0103, 0.0130,  ..., 0.0246, 0.0411, 0.0312]],\n",
      "\n",
      "         [[0.0258, 0.0251, 0.0130,  ..., 0.0248, 0.0220, 0.0177],\n",
      "          [0.0226, 0.0289, 0.0261,  ..., 0.0206, 0.0095, 0.0188],\n",
      "          [0.0370, 0.0462, 0.0219,  ..., 0.0200, 0.0093, 0.0096],\n",
      "          ...,\n",
      "          [0.0188, 0.0207, 0.0323,  ..., 0.0118, 0.0182, 0.0262],\n",
      "          [0.0217, 0.0192, 0.0418,  ..., 0.0178, 0.0291, 0.0115],\n",
      "          [0.0193, 0.0200, 0.0348,  ..., 0.0255, 0.0224, 0.0127]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0122, 0.0127, 0.0267,  ..., 0.0139, 0.0185, 0.0250],\n",
      "          [0.0226, 0.0133, 0.0160,  ..., 0.0150, 0.0177, 0.0220],\n",
      "          [0.0164, 0.0108, 0.0247,  ..., 0.0116, 0.0176, 0.0157],\n",
      "          ...,\n",
      "          [0.0110, 0.0076, 0.0226,  ..., 0.0086, 0.0161, 0.0294],\n",
      "          [0.0179, 0.0132, 0.0195,  ..., 0.0319, 0.0408, 0.0352],\n",
      "          [0.0168, 0.0191, 0.0191,  ..., 0.0162, 0.0183, 0.0111]],\n",
      "\n",
      "         [[0.0206, 0.0307, 0.0317,  ..., 0.0142, 0.0144, 0.0367],\n",
      "          [0.0282, 0.0115, 0.0114,  ..., 0.0176, 0.0177, 0.0085],\n",
      "          [0.0236, 0.0228, 0.0225,  ..., 0.0494, 0.0165, 0.0245],\n",
      "          ...,\n",
      "          [0.0243, 0.0274, 0.0088,  ..., 0.0139, 0.0117, 0.0055],\n",
      "          [0.0207, 0.0311, 0.0140,  ..., 0.0129, 0.0171, 0.0092],\n",
      "          [0.0137, 0.0271, 0.0119,  ..., 0.0143, 0.0111, 0.0135]],\n",
      "\n",
      "         [[0.0193, 0.0276, 0.0382,  ..., 0.0294, 0.0166, 0.0277],\n",
      "          [0.0093, 0.0142, 0.0150,  ..., 0.0199, 0.0164, 0.0169],\n",
      "          [0.0117, 0.0166, 0.0104,  ..., 0.0139, 0.0145, 0.0192],\n",
      "          ...,\n",
      "          [0.0106, 0.0229, 0.0072,  ..., 0.0153, 0.0151, 0.0116],\n",
      "          [0.0194, 0.0195, 0.0161,  ..., 0.0184, 0.0170, 0.0165],\n",
      "          [0.0225, 0.0163, 0.0113,  ..., 0.0323, 0.0165, 0.0111]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0270, 0.0302, 0.0225,  ..., 0.0372, 0.0130, 0.0237],\n",
      "          [0.0120, 0.0233, 0.0127,  ..., 0.0403, 0.0086, 0.0144],\n",
      "          [0.0125, 0.0180, 0.0118,  ..., 0.0297, 0.0093, 0.0203],\n",
      "          ...,\n",
      "          [0.0146, 0.0235, 0.0211,  ..., 0.0344, 0.0073, 0.0301],\n",
      "          [0.0091, 0.0216, 0.0133,  ..., 0.0296, 0.0157, 0.0148],\n",
      "          [0.0365, 0.0167, 0.0186,  ..., 0.0121, 0.0097, 0.0299]],\n",
      "\n",
      "         [[0.0067, 0.0210, 0.0202,  ..., 0.0478, 0.0087, 0.0128],\n",
      "          [0.0127, 0.0191, 0.0182,  ..., 0.0329, 0.0180, 0.0217],\n",
      "          [0.0159, 0.0097, 0.0148,  ..., 0.0209, 0.0089, 0.0175],\n",
      "          ...,\n",
      "          [0.0067, 0.0205, 0.0138,  ..., 0.0103, 0.0142, 0.0305],\n",
      "          [0.0065, 0.0107, 0.0170,  ..., 0.0157, 0.0064, 0.0204],\n",
      "          [0.0143, 0.0080, 0.0154,  ..., 0.0097, 0.0036, 0.0256]],\n",
      "\n",
      "         [[0.0237, 0.0080, 0.0103,  ..., 0.0141, 0.0218, 0.0175],\n",
      "          [0.0343, 0.0150, 0.0123,  ..., 0.0148, 0.0239, 0.0188],\n",
      "          [0.0144, 0.0151, 0.0214,  ..., 0.0198, 0.0198, 0.0310],\n",
      "          ...,\n",
      "          [0.0199, 0.0188, 0.0107,  ..., 0.0277, 0.0242, 0.0117],\n",
      "          [0.0291, 0.0182, 0.0258,  ..., 0.0152, 0.0120, 0.0183],\n",
      "          [0.0273, 0.0186, 0.0247,  ..., 0.0211, 0.0241, 0.0241]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0192, 0.0191, 0.0183,  ..., 0.0144, 0.0231, 0.0155],\n",
      "          [0.0173, 0.0332, 0.0160,  ..., 0.0110, 0.0192, 0.0109],\n",
      "          [0.0131, 0.0179, 0.0238,  ..., 0.0175, 0.0215, 0.0182],\n",
      "          ...,\n",
      "          [0.0173, 0.0278, 0.0182,  ..., 0.0131, 0.0138, 0.0109],\n",
      "          [0.0185, 0.0164, 0.0216,  ..., 0.0162, 0.0219, 0.0154],\n",
      "          [0.0190, 0.0242, 0.0223,  ..., 0.0162, 0.0393, 0.0174]],\n",
      "\n",
      "         [[0.0230, 0.0093, 0.0247,  ..., 0.0273, 0.0367, 0.0137],\n",
      "          [0.0134, 0.0088, 0.0271,  ..., 0.0314, 0.0175, 0.0164],\n",
      "          [0.0266, 0.0316, 0.0237,  ..., 0.0112, 0.0287, 0.0180],\n",
      "          ...,\n",
      "          [0.0227, 0.0169, 0.0248,  ..., 0.0135, 0.0141, 0.0179],\n",
      "          [0.0226, 0.0100, 0.0148,  ..., 0.0596, 0.0237, 0.0193],\n",
      "          [0.0136, 0.0063, 0.0244,  ..., 0.0319, 0.0199, 0.0164]],\n",
      "\n",
      "         [[0.0258, 0.0216, 0.0320,  ..., 0.0294, 0.0122, 0.0151],\n",
      "          [0.0177, 0.0191, 0.0162,  ..., 0.0365, 0.0229, 0.0183],\n",
      "          [0.0231, 0.0297, 0.0379,  ..., 0.0269, 0.0123, 0.0168],\n",
      "          ...,\n",
      "          [0.0151, 0.0175, 0.0239,  ..., 0.0393, 0.0384, 0.0337],\n",
      "          [0.0214, 0.0216, 0.0235,  ..., 0.0257, 0.0452, 0.0321],\n",
      "          [0.0220, 0.0294, 0.0180,  ..., 0.0375, 0.0117, 0.0219]]],\n",
      "\n",
      "\n",
      "        [[[0.0187, 0.0188, 0.0111,  ..., 0.0214, 0.0215, 0.0078],\n",
      "          [0.0313, 0.0339, 0.0219,  ..., 0.0344, 0.0206, 0.0114],\n",
      "          [0.0269, 0.0218, 0.0201,  ..., 0.0395, 0.0375, 0.0146],\n",
      "          ...,\n",
      "          [0.0209, 0.0177, 0.0119,  ..., 0.0413, 0.0212, 0.0187],\n",
      "          [0.0173, 0.0288, 0.0161,  ..., 0.0138, 0.0182, 0.0136],\n",
      "          [0.0206, 0.0458, 0.0094,  ..., 0.0103, 0.0134, 0.0091]],\n",
      "\n",
      "         [[0.0170, 0.0104, 0.0041,  ..., 0.0160, 0.0159, 0.0150],\n",
      "          [0.0149, 0.0118, 0.0079,  ..., 0.0129, 0.0143, 0.0175],\n",
      "          [0.0075, 0.0330, 0.0064,  ..., 0.0188, 0.0313, 0.0175],\n",
      "          ...,\n",
      "          [0.0255, 0.0160, 0.0046,  ..., 0.0138, 0.0276, 0.0142],\n",
      "          [0.0141, 0.0197, 0.0151,  ..., 0.0190, 0.0264, 0.0177],\n",
      "          [0.0218, 0.0121, 0.0052,  ..., 0.0137, 0.0503, 0.0152]],\n",
      "\n",
      "         [[0.0092, 0.0096, 0.0263,  ..., 0.0312, 0.0235, 0.0290],\n",
      "          [0.0160, 0.0319, 0.0118,  ..., 0.0116, 0.0250, 0.0054],\n",
      "          [0.0280, 0.0399, 0.0342,  ..., 0.0319, 0.0126, 0.0175],\n",
      "          ...,\n",
      "          [0.0336, 0.0478, 0.0119,  ..., 0.0382, 0.0142, 0.0291],\n",
      "          [0.0277, 0.0193, 0.0298,  ..., 0.0300, 0.0161, 0.0318],\n",
      "          [0.0230, 0.0244, 0.0241,  ..., 0.0176, 0.0268, 0.0189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0283, 0.0219, 0.0325,  ..., 0.0210, 0.0226, 0.0178],\n",
      "          [0.0057, 0.0178, 0.0144,  ..., 0.0350, 0.0322, 0.0319],\n",
      "          [0.0104, 0.0116, 0.0146,  ..., 0.0177, 0.0267, 0.0093],\n",
      "          ...,\n",
      "          [0.0317, 0.0200, 0.0133,  ..., 0.0211, 0.0245, 0.0256],\n",
      "          [0.0282, 0.0135, 0.0124,  ..., 0.0299, 0.0207, 0.0248],\n",
      "          [0.0245, 0.0256, 0.0281,  ..., 0.0166, 0.0246, 0.0347]],\n",
      "\n",
      "         [[0.0357, 0.0215, 0.0303,  ..., 0.0156, 0.0275, 0.0179],\n",
      "          [0.0194, 0.0211, 0.0129,  ..., 0.0127, 0.0320, 0.0219],\n",
      "          [0.0268, 0.0206, 0.0105,  ..., 0.0175, 0.0335, 0.0158],\n",
      "          ...,\n",
      "          [0.0324, 0.0170, 0.0241,  ..., 0.0200, 0.0232, 0.0223],\n",
      "          [0.0548, 0.0262, 0.0281,  ..., 0.0220, 0.0174, 0.0415],\n",
      "          [0.0243, 0.0285, 0.0275,  ..., 0.0149, 0.0570, 0.0157]],\n",
      "\n",
      "         [[0.0160, 0.0147, 0.0157,  ..., 0.0237, 0.0179, 0.0211],\n",
      "          [0.0212, 0.0173, 0.0164,  ..., 0.0131, 0.0186, 0.0143],\n",
      "          [0.0075, 0.0216, 0.0320,  ..., 0.0303, 0.0183, 0.0147],\n",
      "          ...,\n",
      "          [0.0141, 0.0228, 0.0101,  ..., 0.0179, 0.0178, 0.0183],\n",
      "          [0.0185, 0.0174, 0.0151,  ..., 0.0358, 0.0081, 0.0190],\n",
      "          [0.0105, 0.0202, 0.0275,  ..., 0.0496, 0.0114, 0.0127]]],\n",
      "\n",
      "\n",
      "        [[[0.0182, 0.0132, 0.0217,  ..., 0.0138, 0.0111, 0.0128],\n",
      "          [0.0063, 0.0102, 0.0224,  ..., 0.0038, 0.0360, 0.0116],\n",
      "          [0.0302, 0.0064, 0.0149,  ..., 0.0098, 0.0228, 0.0061],\n",
      "          ...,\n",
      "          [0.0249, 0.0265, 0.0309,  ..., 0.0142, 0.0204, 0.0187],\n",
      "          [0.0283, 0.0129, 0.0146,  ..., 0.0067, 0.0234, 0.0081],\n",
      "          [0.0299, 0.0129, 0.0376,  ..., 0.0108, 0.0256, 0.0217]],\n",
      "\n",
      "         [[0.0112, 0.0080, 0.0209,  ..., 0.0151, 0.0278, 0.0356],\n",
      "          [0.0185, 0.0126, 0.0217,  ..., 0.0087, 0.0112, 0.0180],\n",
      "          [0.0127, 0.0109, 0.0262,  ..., 0.0113, 0.0115, 0.0163],\n",
      "          ...,\n",
      "          [0.0171, 0.0218, 0.0309,  ..., 0.0179, 0.0145, 0.0170],\n",
      "          [0.0177, 0.0208, 0.0134,  ..., 0.0213, 0.0091, 0.0198],\n",
      "          [0.0202, 0.0192, 0.0190,  ..., 0.0163, 0.0182, 0.0147]],\n",
      "\n",
      "         [[0.0349, 0.0210, 0.0138,  ..., 0.0109, 0.0237, 0.0212],\n",
      "          [0.0523, 0.0310, 0.0224,  ..., 0.0256, 0.0363, 0.0228],\n",
      "          [0.0295, 0.0204, 0.0168,  ..., 0.0107, 0.0176, 0.0102],\n",
      "          ...,\n",
      "          [0.0529, 0.0196, 0.0318,  ..., 0.0101, 0.0074, 0.0221],\n",
      "          [0.0303, 0.0142, 0.0117,  ..., 0.0150, 0.0278, 0.0164],\n",
      "          [0.0277, 0.0102, 0.0077,  ..., 0.0146, 0.0155, 0.0198]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0182, 0.0200, 0.0170,  ..., 0.0168, 0.0279, 0.0510],\n",
      "          [0.0224, 0.0267, 0.0213,  ..., 0.0159, 0.0336, 0.0204],\n",
      "          [0.0246, 0.0194, 0.0145,  ..., 0.0156, 0.0203, 0.0124],\n",
      "          ...,\n",
      "          [0.0100, 0.0223, 0.0231,  ..., 0.0280, 0.0278, 0.0320],\n",
      "          [0.0174, 0.0186, 0.0280,  ..., 0.0180, 0.0176, 0.0234],\n",
      "          [0.0208, 0.0258, 0.0211,  ..., 0.0163, 0.0165, 0.0153]],\n",
      "\n",
      "         [[0.0105, 0.0219, 0.0147,  ..., 0.0190, 0.0100, 0.0293],\n",
      "          [0.0247, 0.0149, 0.0319,  ..., 0.0187, 0.0212, 0.0245],\n",
      "          [0.0261, 0.0161, 0.0295,  ..., 0.0121, 0.0241, 0.0283],\n",
      "          ...,\n",
      "          [0.0190, 0.0157, 0.0291,  ..., 0.0342, 0.0182, 0.0546],\n",
      "          [0.0114, 0.0141, 0.0203,  ..., 0.0244, 0.0229, 0.0269],\n",
      "          [0.0275, 0.0143, 0.0104,  ..., 0.0202, 0.0183, 0.0266]],\n",
      "\n",
      "         [[0.0187, 0.0099, 0.0159,  ..., 0.0384, 0.0256, 0.0290],\n",
      "          [0.0148, 0.0159, 0.0261,  ..., 0.0258, 0.0316, 0.0168],\n",
      "          [0.0227, 0.0166, 0.0184,  ..., 0.0273, 0.0147, 0.0190],\n",
      "          ...,\n",
      "          [0.0190, 0.0116, 0.0189,  ..., 0.0269, 0.0188, 0.0133],\n",
      "          [0.0155, 0.0139, 0.0287,  ..., 0.0299, 0.0109, 0.0129],\n",
      "          [0.0101, 0.0188, 0.0371,  ..., 0.0253, 0.0138, 0.0178]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-4.8605e-02,  3.1187e-01,  2.2985e-01,  ...,  2.1551e-01,\n",
      "           -7.6329e-01,  3.3646e-02],\n",
      "          [-2.6232e-02,  3.2899e-01,  2.6102e-01,  ...,  2.8442e-01,\n",
      "           -6.7853e-01, -3.2132e-02],\n",
      "          [-4.7293e-02,  2.9358e-01,  2.9654e-01,  ...,  3.0999e-01,\n",
      "           -6.5841e-01, -9.9041e-02],\n",
      "          ...,\n",
      "          [ 1.8335e-02,  3.6748e-01,  2.5036e-01,  ...,  1.7284e-01,\n",
      "           -6.4598e-01, -1.0585e-02],\n",
      "          [-8.3520e-02,  3.1206e-01,  2.1516e-01,  ...,  2.4639e-01,\n",
      "           -6.3629e-01, -8.8852e-02],\n",
      "          [-1.7880e-02,  3.0138e-01,  2.1838e-01,  ...,  2.1607e-01,\n",
      "           -6.1491e-01, -4.9492e-02]],\n",
      "\n",
      "         [[-1.3999e-01,  1.1580e-02,  3.2872e-02,  ..., -1.7144e-01,\n",
      "           -5.4335e-01, -5.2589e-01],\n",
      "          [-1.4831e-01,  5.9674e-02,  2.0459e-03,  ..., -2.3580e-01,\n",
      "           -5.4267e-01, -5.8924e-01],\n",
      "          [-1.7218e-01, -9.3496e-03,  3.4196e-02,  ..., -1.9492e-01,\n",
      "           -5.1791e-01, -5.5653e-01],\n",
      "          ...,\n",
      "          [-9.6890e-02,  5.9846e-02,  1.2125e-02,  ..., -1.6775e-01,\n",
      "           -5.2117e-01, -6.4461e-01],\n",
      "          [-1.0967e-01,  5.4747e-04,  2.4664e-02,  ..., -1.3044e-01,\n",
      "           -5.6441e-01, -6.0018e-01],\n",
      "          [-1.2756e-01, -9.9208e-04,  8.4773e-02,  ..., -1.8274e-01,\n",
      "           -5.5592e-01, -5.4967e-01]],\n",
      "\n",
      "         [[-3.9462e-01, -2.5859e-01,  1.1883e-01,  ..., -4.0068e-02,\n",
      "            2.4147e-02,  4.2768e-01],\n",
      "          [-3.8901e-01, -2.3641e-01,  1.4376e-01,  ..., -9.9566e-02,\n",
      "            3.9006e-02,  4.6755e-01],\n",
      "          [-4.5959e-01, -2.2950e-01,  9.9557e-02,  ..., -8.8918e-02,\n",
      "            5.6063e-02,  5.0685e-01],\n",
      "          ...,\n",
      "          [-4.0064e-01, -1.6012e-01,  1.9414e-01,  ..., -1.1393e-01,\n",
      "            8.1448e-02,  4.8109e-01],\n",
      "          [-3.7607e-01, -1.8282e-01,  1.4787e-01,  ..., -9.6461e-02,\n",
      "            4.1963e-02,  4.7564e-01],\n",
      "          [-3.5960e-01, -1.6704e-01,  1.0368e-01,  ..., -8.2425e-02,\n",
      "           -4.6630e-02,  4.0673e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0163e-02, -3.5878e-01,  2.6381e-01,  ...,  1.2500e-01,\n",
      "            3.5230e-02, -6.3972e-01],\n",
      "          [-6.7709e-02, -3.6272e-01,  3.2696e-01,  ...,  1.3796e-01,\n",
      "            1.2449e-03, -6.5353e-01],\n",
      "          [-9.0791e-02, -4.1801e-01,  3.1640e-01,  ...,  1.3810e-01,\n",
      "            6.5949e-02, -7.0855e-01],\n",
      "          ...,\n",
      "          [-5.7441e-02, -3.7490e-01,  2.9581e-01,  ...,  1.7040e-01,\n",
      "            8.5715e-02, -6.6889e-01],\n",
      "          [-6.6966e-02, -3.8184e-01,  3.3629e-01,  ...,  1.4714e-01,\n",
      "            7.6199e-02, -6.6693e-01],\n",
      "          [-2.9694e-03, -4.4060e-01,  3.3098e-01,  ...,  1.5692e-01,\n",
      "            6.4102e-02, -6.3945e-01]],\n",
      "\n",
      "         [[ 4.9803e-02,  8.0477e-01,  5.0776e-01,  ...,  2.3042e-02,\n",
      "            1.9716e-01,  1.4429e-01],\n",
      "          [ 9.5609e-02,  7.4766e-01,  4.4955e-01,  ..., -3.6096e-02,\n",
      "            1.5464e-01,  1.8894e-01],\n",
      "          [ 1.3448e-01,  8.0482e-01,  5.0358e-01,  ...,  4.3297e-03,\n",
      "            2.6073e-01,  1.0359e-01],\n",
      "          ...,\n",
      "          [ 3.3679e-03,  7.8597e-01,  4.8603e-01,  ...,  1.6569e-02,\n",
      "            2.3220e-01,  1.3484e-01],\n",
      "          [-2.7083e-03,  7.4811e-01,  4.9703e-01,  ...,  6.1779e-02,\n",
      "            1.9764e-01,  2.6596e-01],\n",
      "          [-5.9330e-03,  7.6781e-01,  5.2248e-01,  ..., -3.1457e-02,\n",
      "            1.7516e-01,  1.9045e-01]],\n",
      "\n",
      "         [[ 2.7493e-01,  8.3738e-01,  4.7433e-01,  ..., -1.9476e-01,\n",
      "            2.7989e-01, -3.2026e-01],\n",
      "          [ 2.7626e-01,  9.3030e-01,  4.6746e-01,  ..., -1.8053e-01,\n",
      "            2.4373e-01, -2.4075e-01],\n",
      "          [ 2.4342e-01,  9.2362e-01,  4.5217e-01,  ..., -1.4678e-01,\n",
      "            2.1677e-01, -3.6789e-01],\n",
      "          ...,\n",
      "          [ 2.7641e-01,  9.0646e-01,  4.4637e-01,  ..., -2.1296e-01,\n",
      "            1.9438e-01, -3.5142e-01],\n",
      "          [ 3.1003e-01,  9.0564e-01,  4.6508e-01,  ..., -2.0694e-01,\n",
      "            2.1918e-01, -3.4910e-01],\n",
      "          [ 2.6249e-01,  8.8376e-01,  5.0959e-01,  ..., -2.9702e-01,\n",
      "            2.4491e-01, -2.7968e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5805e-02,  4.0404e-01,  3.1663e-01,  ...,  3.1421e-01,\n",
      "           -4.9299e-01,  7.6251e-02],\n",
      "          [-4.0839e-02,  3.2795e-01,  3.7531e-01,  ...,  2.5834e-01,\n",
      "           -5.2693e-01,  1.0523e-01],\n",
      "          [-2.2940e-02,  3.8368e-01,  3.1646e-01,  ...,  3.1350e-01,\n",
      "           -5.5290e-01,  7.8426e-02],\n",
      "          ...,\n",
      "          [-5.3387e-02,  3.5769e-01,  3.7922e-01,  ...,  3.2083e-01,\n",
      "           -5.1566e-01,  4.0698e-02],\n",
      "          [-2.2334e-02,  3.9738e-01,  3.4659e-01,  ...,  2.9973e-01,\n",
      "           -5.2617e-01,  1.2000e-01],\n",
      "          [-2.0106e-02,  3.6218e-01,  3.8925e-01,  ...,  2.3098e-01,\n",
      "           -4.9049e-01,  8.6882e-02]],\n",
      "\n",
      "         [[ 1.7013e-01,  3.1351e-01,  5.9618e-02,  ..., -3.5050e-01,\n",
      "           -6.3399e-01, -5.4956e-01],\n",
      "          [ 1.0802e-01,  1.9925e-01, -6.8469e-03,  ..., -4.1768e-01,\n",
      "           -6.2232e-01, -5.4214e-01],\n",
      "          [ 1.3659e-01,  2.5243e-01,  1.1863e-02,  ..., -3.5522e-01,\n",
      "           -5.4587e-01, -5.0570e-01],\n",
      "          ...,\n",
      "          [ 1.4728e-01,  2.1562e-01,  3.1288e-02,  ..., -3.5225e-01,\n",
      "           -5.3123e-01, -5.0498e-01],\n",
      "          [ 1.3511e-01,  1.7462e-01, -4.7783e-02,  ..., -3.6092e-01,\n",
      "           -5.7779e-01, -4.3903e-01],\n",
      "          [ 1.8139e-01,  2.4513e-01,  9.3712e-03,  ..., -3.5644e-01,\n",
      "           -5.9433e-01, -4.9726e-01]],\n",
      "\n",
      "         [[-2.2818e-01, -3.9791e-01,  1.7580e-01,  ..., -2.2883e-01,\n",
      "            7.8555e-02,  4.6548e-01],\n",
      "          [-2.7018e-01, -3.7366e-01,  2.2567e-01,  ..., -2.0174e-01,\n",
      "            3.2861e-02,  4.5003e-01],\n",
      "          [-2.5136e-01, -3.8817e-01,  2.1982e-01,  ..., -2.3199e-01,\n",
      "            2.4600e-02,  5.0010e-01],\n",
      "          ...,\n",
      "          [-2.9031e-01, -3.9516e-01,  2.2466e-01,  ..., -2.7510e-01,\n",
      "            8.3682e-02,  4.4326e-01],\n",
      "          [-2.8924e-01, -3.7106e-01,  2.1143e-01,  ..., -2.4526e-01,\n",
      "            6.0705e-02,  4.5608e-01],\n",
      "          [-2.9889e-01, -3.5721e-01,  2.1712e-01,  ..., -2.0630e-01,\n",
      "            1.2560e-01,  3.3768e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1410e-01, -1.6800e-01,  3.5977e-01,  ...,  1.8031e-01,\n",
      "           -9.1322e-02, -6.4830e-01],\n",
      "          [-6.8848e-03, -1.4430e-01,  2.7765e-01,  ...,  2.9532e-01,\n",
      "            1.3737e-02, -7.8314e-01],\n",
      "          [ 1.0976e-01, -6.9248e-02,  3.0349e-01,  ...,  2.7766e-01,\n",
      "            8.4170e-02, -6.5737e-01],\n",
      "          ...,\n",
      "          [ 1.8144e-02, -8.3026e-02,  3.1817e-01,  ...,  2.9193e-01,\n",
      "           -3.1784e-02, -7.0546e-01],\n",
      "          [-3.5095e-02, -1.5142e-01,  1.9064e-01,  ...,  3.4118e-01,\n",
      "           -7.0502e-02, -6.9579e-01],\n",
      "          [ 1.1651e-02, -1.4678e-01,  2.6509e-01,  ...,  2.8640e-01,\n",
      "           -1.7321e-03, -7.4080e-01]],\n",
      "\n",
      "         [[ 1.0710e-01,  5.7839e-01,  5.6314e-01,  ..., -2.0107e-01,\n",
      "            7.5077e-02,  1.0404e-01],\n",
      "          [ 1.2744e-01,  6.1269e-01,  5.8905e-01,  ..., -2.5272e-01,\n",
      "            3.1787e-02,  1.0153e-01],\n",
      "          [ 1.2878e-01,  5.7600e-01,  5.9289e-01,  ..., -1.8952e-01,\n",
      "            7.3553e-02,  1.7502e-01],\n",
      "          ...,\n",
      "          [ 1.9551e-01,  6.1273e-01,  6.1534e-01,  ..., -1.9277e-01,\n",
      "            5.7514e-02,  1.8740e-01],\n",
      "          [ 1.9508e-01,  5.6834e-01,  5.8118e-01,  ..., -1.6161e-01,\n",
      "            7.5737e-02,  2.2664e-01],\n",
      "          [ 1.3328e-01,  5.4954e-01,  5.6353e-01,  ..., -1.5518e-01,\n",
      "            5.2680e-02,  2.1982e-01]],\n",
      "\n",
      "         [[ 1.7336e-01,  8.3172e-01,  4.8122e-01,  ..., -2.1599e-01,\n",
      "            1.7135e-01, -3.8662e-01],\n",
      "          [ 2.0597e-01,  7.6441e-01,  5.0661e-01,  ..., -3.0938e-01,\n",
      "            1.7760e-01, -2.6030e-01],\n",
      "          [ 2.2434e-01,  8.2279e-01,  5.3818e-01,  ..., -2.7553e-01,\n",
      "            1.0725e-01, -3.4781e-01],\n",
      "          ...,\n",
      "          [ 2.3214e-01,  8.0182e-01,  5.3308e-01,  ..., -2.5197e-01,\n",
      "            1.2714e-01, -3.0760e-01],\n",
      "          [ 1.7577e-01,  8.6660e-01,  4.9661e-01,  ..., -2.1289e-01,\n",
      "            1.8312e-01, -3.6784e-01],\n",
      "          [ 2.3386e-01,  7.4510e-01,  5.5480e-01,  ..., -2.8856e-01,\n",
      "            1.7215e-01, -2.7728e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0264e-02,  2.7140e-01,  3.5849e-01,  ...,  2.3313e-01,\n",
      "           -6.1077e-01, -2.2913e-03],\n",
      "          [-3.2175e-02,  2.7505e-01,  3.3540e-01,  ...,  2.1638e-01,\n",
      "           -6.1928e-01,  2.8732e-03],\n",
      "          [-2.2883e-02,  3.0288e-01,  4.0098e-01,  ...,  2.0204e-01,\n",
      "           -6.0425e-01, -1.5365e-02],\n",
      "          ...,\n",
      "          [ 3.7596e-02,  1.9298e-01,  3.3378e-01,  ...,  1.4663e-01,\n",
      "           -5.7997e-01,  4.8994e-02],\n",
      "          [ 2.9563e-02,  2.5327e-01,  4.0995e-01,  ...,  1.3247e-01,\n",
      "           -6.2422e-01,  1.0195e-02],\n",
      "          [-1.7950e-02,  2.6898e-01,  4.7714e-01,  ...,  1.6563e-01,\n",
      "           -5.9931e-01,  2.2391e-02]],\n",
      "\n",
      "         [[ 4.8354e-02,  2.1872e-01, -1.2749e-01,  ..., -4.1137e-01,\n",
      "           -5.1949e-01, -4.9056e-01],\n",
      "          [ 4.8543e-02,  1.5270e-01, -1.2550e-01,  ..., -4.7599e-01,\n",
      "           -4.8784e-01, -5.1613e-01],\n",
      "          [ 5.3317e-02,  1.4558e-01, -6.9515e-02,  ..., -4.3624e-01,\n",
      "           -6.0595e-01, -5.6945e-01],\n",
      "          ...,\n",
      "          [ 8.7953e-02,  1.3547e-01, -6.9772e-02,  ..., -4.2942e-01,\n",
      "           -5.5836e-01, -5.9787e-01],\n",
      "          [ 4.4102e-02,  1.7833e-01, -9.6515e-02,  ..., -4.5942e-01,\n",
      "           -4.9519e-01, -5.4452e-01],\n",
      "          [ 1.8777e-02,  1.8263e-01, -8.6118e-02,  ..., -4.9212e-01,\n",
      "           -5.9010e-01, -5.9484e-01]],\n",
      "\n",
      "         [[-2.8532e-01, -2.8262e-01,  2.8496e-01,  ..., -5.7717e-02,\n",
      "            4.8476e-02,  5.1505e-01],\n",
      "          [-2.8623e-01, -2.6965e-01,  3.0751e-01,  ..., -1.0228e-01,\n",
      "            4.8680e-03,  5.0872e-01],\n",
      "          [-2.8451e-01, -2.0322e-01,  2.5218e-01,  ..., -3.1386e-02,\n",
      "            2.3090e-02,  5.4615e-01],\n",
      "          ...,\n",
      "          [-3.1221e-01, -2.3292e-01,  2.9769e-01,  ..., -1.9961e-01,\n",
      "            1.9246e-02,  5.2122e-01],\n",
      "          [-3.1132e-01, -1.9741e-01,  2.5597e-01,  ..., -6.8143e-02,\n",
      "            4.1371e-02,  4.7322e-01],\n",
      "          [-3.2609e-01, -2.7549e-01,  3.0362e-01,  ..., -1.2771e-01,\n",
      "            7.1585e-02,  4.9707e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5907e-02, -3.8679e-01,  3.6077e-01,  ...,  3.1655e-02,\n",
      "            1.1725e-01, -6.1429e-01],\n",
      "          [ 5.1420e-02, -3.9131e-01,  3.5006e-01,  ...,  3.0775e-02,\n",
      "            1.3280e-01, -5.7731e-01],\n",
      "          [ 4.0203e-02, -4.0726e-01,  3.5751e-01,  ...,  4.4995e-02,\n",
      "            1.1536e-01, -6.5676e-01],\n",
      "          ...,\n",
      "          [ 2.3692e-02, -4.2528e-01,  3.2644e-01,  ...,  3.1624e-02,\n",
      "            1.3415e-01, -5.9513e-01],\n",
      "          [-2.1236e-02, -4.0222e-01,  3.8488e-01,  ...,  1.0301e-01,\n",
      "            1.9000e-01, -5.8517e-01],\n",
      "          [ 8.3854e-03, -4.2183e-01,  3.1749e-01,  ...,  3.4844e-02,\n",
      "            1.7581e-01, -5.9814e-01]],\n",
      "\n",
      "         [[ 4.6882e-02,  9.6182e-01,  6.8679e-01,  ..., -1.1407e-01,\n",
      "            8.1161e-02,  4.2213e-01],\n",
      "          [-2.6565e-02,  9.8682e-01,  6.6062e-01,  ..., -1.0257e-01,\n",
      "            2.6567e-02,  4.6146e-01],\n",
      "          [-2.6790e-02,  9.9182e-01,  6.7476e-01,  ..., -1.4364e-01,\n",
      "           -3.3449e-02,  3.7744e-01],\n",
      "          ...,\n",
      "          [-1.1327e-02,  8.3827e-01,  7.0794e-01,  ..., -6.5996e-02,\n",
      "            6.2345e-02,  4.9554e-01],\n",
      "          [-6.7224e-02,  8.9719e-01,  6.6076e-01,  ..., -8.7718e-02,\n",
      "            6.8332e-02,  4.9586e-01],\n",
      "          [-2.8577e-03,  1.0324e+00,  6.3605e-01,  ..., -7.4250e-02,\n",
      "            1.6549e-02,  4.6223e-01]],\n",
      "\n",
      "         [[ 3.6762e-01,  7.6169e-01,  5.0773e-01,  ..., -2.9593e-01,\n",
      "            3.5207e-01, -1.5499e-01],\n",
      "          [ 4.0379e-01,  7.7835e-01,  3.9691e-01,  ..., -3.2191e-01,\n",
      "            3.0929e-01, -1.0628e-01],\n",
      "          [ 3.7546e-01,  7.5540e-01,  3.9205e-01,  ..., -2.9483e-01,\n",
      "            2.8796e-01, -6.4220e-02],\n",
      "          ...,\n",
      "          [ 4.3787e-01,  7.5734e-01,  3.2836e-01,  ..., -3.5396e-01,\n",
      "            2.9660e-01, -1.0007e-01],\n",
      "          [ 3.9128e-01,  7.6935e-01,  4.6650e-01,  ..., -3.8582e-01,\n",
      "            3.6320e-01, -6.4718e-02],\n",
      "          [ 4.0000e-01,  8.1383e-01,  4.3109e-01,  ..., -3.3396e-01,\n",
      "            2.9368e-01, -9.4244e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.6815e-01,  2.9128e-01,  2.2584e-01,  ...,  1.1261e-01,\n",
      "           -7.6545e-01, -2.3106e-01],\n",
      "          [-2.4295e-01,  3.4818e-01,  2.6169e-01,  ...,  2.1194e-01,\n",
      "           -7.3058e-01, -2.3043e-01],\n",
      "          [-2.6277e-01,  2.1145e-01,  2.2486e-01,  ...,  2.0629e-01,\n",
      "           -7.4926e-01, -2.4157e-01],\n",
      "          ...,\n",
      "          [-2.6321e-01,  2.5463e-01,  2.3460e-01,  ...,  1.2902e-01,\n",
      "           -8.3036e-01, -2.5268e-01],\n",
      "          [-2.5311e-01,  3.2660e-01,  2.0151e-01,  ...,  2.0020e-01,\n",
      "           -7.0373e-01, -1.7241e-01],\n",
      "          [-2.3860e-01,  2.8241e-01,  1.7553e-01,  ...,  1.2999e-01,\n",
      "           -7.5832e-01, -1.7854e-01]],\n",
      "\n",
      "         [[ 1.1602e-01,  1.5148e-01, -1.2458e-01,  ..., -3.9051e-01,\n",
      "           -5.6493e-01, -6.3175e-01],\n",
      "          [ 1.1511e-01,  1.9311e-01, -1.0331e-01,  ..., -3.6870e-01,\n",
      "           -5.5193e-01, -5.9978e-01],\n",
      "          [ 7.1378e-02,  9.5298e-02, -1.0706e-01,  ..., -3.6228e-01,\n",
      "           -6.0952e-01, -5.8349e-01],\n",
      "          ...,\n",
      "          [ 1.7293e-01,  2.3537e-01, -9.8095e-02,  ..., -3.6805e-01,\n",
      "           -5.3877e-01, -5.9011e-01],\n",
      "          [ 2.1574e-01,  1.8731e-01, -1.2036e-01,  ..., -3.8746e-01,\n",
      "           -5.5241e-01, -6.0456e-01],\n",
      "          [ 2.1691e-01,  7.0314e-02, -1.6152e-01,  ..., -3.7405e-01,\n",
      "           -5.0745e-01, -5.7734e-01]],\n",
      "\n",
      "         [[-2.0354e-01, -3.2638e-01,  2.9422e-01,  ..., -1.1624e-01,\n",
      "            3.7819e-02,  5.2914e-01],\n",
      "          [-2.0430e-01, -2.6244e-01,  2.3730e-01,  ..., -1.1975e-01,\n",
      "            1.5717e-02,  5.6432e-01],\n",
      "          [-2.0232e-01, -2.4118e-01,  1.8916e-01,  ..., -1.3796e-01,\n",
      "            8.8196e-03,  5.5184e-01],\n",
      "          ...,\n",
      "          [-1.5621e-01, -2.8608e-01,  2.0605e-01,  ..., -7.5443e-02,\n",
      "            7.2704e-02,  4.7084e-01],\n",
      "          [-1.8393e-01, -2.5951e-01,  2.5022e-01,  ..., -9.6670e-02,\n",
      "            1.3070e-02,  5.6673e-01],\n",
      "          [-1.7335e-01, -2.7768e-01,  2.1595e-01,  ..., -4.4465e-02,\n",
      "            4.6810e-02,  5.2740e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2916e-02, -2.5483e-01,  2.5565e-01,  ...,  2.3690e-01,\n",
      "           -7.6872e-02, -6.2146e-01],\n",
      "          [ 6.2597e-04, -2.9043e-01,  3.1426e-01,  ...,  2.5248e-01,\n",
      "           -7.2039e-02, -6.2345e-01],\n",
      "          [ 3.6984e-02, -2.4793e-01,  2.6852e-01,  ...,  2.5762e-01,\n",
      "           -7.9025e-02, -6.4442e-01],\n",
      "          ...,\n",
      "          [ 5.3710e-02, -2.3516e-01,  3.7017e-01,  ...,  2.1265e-01,\n",
      "           -2.5901e-02, -6.3081e-01],\n",
      "          [ 7.7355e-03, -2.6082e-01,  3.8152e-01,  ...,  1.6003e-01,\n",
      "           -2.3427e-02, -6.2516e-01],\n",
      "          [-3.2410e-03, -2.4781e-01,  3.3844e-01,  ...,  2.6420e-01,\n",
      "           -3.9698e-02, -6.3284e-01]],\n",
      "\n",
      "         [[-2.8075e-02,  7.2054e-01,  7.1732e-01,  ...,  8.3758e-02,\n",
      "            1.2654e-01,  2.5626e-01],\n",
      "          [-2.5329e-02,  7.4281e-01,  6.9317e-01,  ...,  1.0061e-01,\n",
      "            1.5429e-01,  2.7680e-01],\n",
      "          [ 2.1073e-02,  8.0570e-01,  6.8505e-01,  ...,  3.9199e-02,\n",
      "            1.0890e-01,  1.9259e-01],\n",
      "          ...,\n",
      "          [-2.1585e-02,  7.3400e-01,  7.0252e-01,  ...,  8.1285e-02,\n",
      "            1.3488e-01,  2.5524e-01],\n",
      "          [-6.9921e-02,  7.6257e-01,  7.4037e-01,  ...,  9.3452e-02,\n",
      "            1.2147e-01,  3.1501e-01],\n",
      "          [-5.2955e-02,  7.3963e-01,  7.4398e-01,  ...,  5.8393e-02,\n",
      "            1.6106e-01,  2.4001e-01]],\n",
      "\n",
      "         [[ 2.8039e-01,  7.3298e-01,  4.0556e-01,  ..., -3.4915e-01,\n",
      "            1.5492e-01, -2.3293e-01],\n",
      "          [ 3.2338e-01,  7.3693e-01,  4.1793e-01,  ..., -3.9450e-01,\n",
      "            2.4911e-01, -2.4579e-01],\n",
      "          [ 2.7423e-01,  6.8214e-01,  4.1819e-01,  ..., -4.0188e-01,\n",
      "            2.3550e-01, -1.7236e-01],\n",
      "          ...,\n",
      "          [ 3.3951e-01,  7.1447e-01,  4.8479e-01,  ..., -4.2534e-01,\n",
      "            2.2639e-01, -2.6208e-01],\n",
      "          [ 3.1053e-01,  7.5983e-01,  4.5107e-01,  ..., -3.9494e-01,\n",
      "            1.6818e-01, -2.1573e-01],\n",
      "          [ 3.0757e-01,  7.4105e-01,  3.6021e-01,  ..., -3.9798e-01,\n",
      "            1.1947e-01, -2.3483e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.2109e-02,  1.6545e-01,  2.3987e-01,  ...,  2.9147e-01,\n",
      "           -5.9621e-01,  1.5815e-03],\n",
      "          [-4.6876e-02,  1.5550e-01,  2.3945e-01,  ...,  2.5965e-01,\n",
      "           -6.7404e-01, -1.1544e-01],\n",
      "          [-3.7531e-02,  1.6824e-01,  2.5267e-01,  ...,  3.0289e-01,\n",
      "           -6.2993e-01, -6.1589e-02],\n",
      "          ...,\n",
      "          [-6.4585e-02,  1.6557e-01,  2.4198e-01,  ...,  2.8946e-01,\n",
      "           -5.6346e-01, -9.4395e-02],\n",
      "          [-7.1733e-02,  2.2062e-01,  2.3336e-01,  ...,  3.2031e-01,\n",
      "           -6.6121e-01, -5.5819e-02],\n",
      "          [-4.4045e-02,  2.1799e-01,  2.2797e-01,  ...,  3.0514e-01,\n",
      "           -6.8612e-01, -5.8605e-02]],\n",
      "\n",
      "         [[ 7.7606e-02,  1.8803e-02, -8.1519e-02,  ..., -2.9896e-01,\n",
      "           -5.1936e-01, -5.4733e-01],\n",
      "          [ 4.7062e-02, -2.9878e-03, -1.0687e-01,  ..., -3.2331e-01,\n",
      "           -5.5285e-01, -4.9517e-01],\n",
      "          [ 1.1266e-01, -1.2364e-01, -1.0409e-01,  ..., -3.0500e-01,\n",
      "           -6.3844e-01, -5.6049e-01],\n",
      "          ...,\n",
      "          [ 6.9224e-02,  8.4195e-03, -1.3795e-01,  ..., -3.0881e-01,\n",
      "           -5.6607e-01, -4.6076e-01],\n",
      "          [ 9.1616e-02, -2.9729e-02, -1.0538e-01,  ..., -3.3610e-01,\n",
      "           -6.2212e-01, -5.2895e-01],\n",
      "          [ 1.6296e-02, -3.0456e-02, -1.1681e-01,  ..., -3.1913e-01,\n",
      "           -6.0176e-01, -4.2171e-01]],\n",
      "\n",
      "         [[-3.8000e-01, -3.8777e-01,  1.3504e-01,  ...,  3.6389e-02,\n",
      "            1.6817e-01,  5.6981e-01],\n",
      "          [-2.7505e-01, -4.0468e-01,  1.1153e-01,  ..., -9.4783e-02,\n",
      "            2.1885e-01,  4.8585e-01],\n",
      "          [-2.8356e-01, -3.7250e-01,  1.4799e-01,  ..., -5.5970e-02,\n",
      "            6.9527e-02,  4.7592e-01],\n",
      "          ...,\n",
      "          [-3.0276e-01, -3.2818e-01,  1.8373e-01,  ...,  6.7409e-02,\n",
      "            7.1568e-02,  4.7560e-01],\n",
      "          [-3.0024e-01, -2.9720e-01,  2.1328e-01,  ...,  7.4519e-02,\n",
      "            1.2642e-01,  5.0199e-01],\n",
      "          [-2.4170e-01, -3.1844e-01,  1.9728e-01,  ...,  9.8658e-02,\n",
      "            1.7306e-01,  4.9495e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3654e-02, -2.1121e-01,  2.9094e-01,  ...,  2.3098e-01,\n",
      "            6.4472e-02, -6.1701e-01],\n",
      "          [ 6.2820e-02, -2.3245e-01,  2.9973e-01,  ...,  1.3880e-01,\n",
      "            9.9961e-02, -6.0248e-01],\n",
      "          [ 6.7735e-02, -2.1296e-01,  2.3042e-01,  ...,  1.2626e-01,\n",
      "            1.1710e-01, -6.1798e-01],\n",
      "          ...,\n",
      "          [ 3.0805e-02, -2.1368e-01,  2.6829e-01,  ...,  1.1889e-01,\n",
      "            9.8285e-02, -5.6787e-01],\n",
      "          [ 6.4354e-02, -2.2716e-01,  2.4623e-01,  ...,  1.0193e-01,\n",
      "            1.0460e-01, -6.0769e-01],\n",
      "          [-8.2040e-03, -2.1760e-01,  3.3838e-01,  ...,  1.3694e-01,\n",
      "            5.9595e-02, -5.7724e-01]],\n",
      "\n",
      "         [[-3.5246e-02,  6.9804e-01,  6.9074e-01,  ..., -5.9054e-02,\n",
      "           -8.9110e-02,  1.9155e-01],\n",
      "          [-1.1014e-02,  7.4769e-01,  7.6467e-01,  ..., -3.9529e-02,\n",
      "           -8.3105e-02,  1.8587e-01],\n",
      "          [-5.5495e-02,  7.9704e-01,  7.8348e-01,  ..., -5.4998e-02,\n",
      "           -4.3896e-02,  2.2882e-01],\n",
      "          ...,\n",
      "          [-7.5004e-02,  8.0885e-01,  7.5041e-01,  ..., -3.0306e-02,\n",
      "           -4.3071e-02,  2.4906e-01],\n",
      "          [-2.3274e-02,  7.7426e-01,  6.6052e-01,  ..., -4.6578e-02,\n",
      "           -9.3826e-02,  1.3967e-01],\n",
      "          [-4.8358e-02,  7.2494e-01,  8.4029e-01,  ..., -4.0873e-02,\n",
      "           -3.2022e-02,  2.2843e-01]],\n",
      "\n",
      "         [[ 3.2124e-01,  7.4708e-01,  5.1718e-01,  ..., -2.6536e-01,\n",
      "            3.0190e-01, -1.4042e-01],\n",
      "          [ 2.8596e-01,  7.6474e-01,  4.6509e-01,  ..., -2.8522e-01,\n",
      "            2.9067e-01, -9.0543e-02],\n",
      "          [ 2.9762e-01,  6.5368e-01,  4.9197e-01,  ..., -2.2656e-01,\n",
      "            3.3710e-01, -1.3804e-01],\n",
      "          ...,\n",
      "          [ 2.9175e-01,  7.3960e-01,  5.1743e-01,  ..., -2.9043e-01,\n",
      "            3.4352e-01, -1.2533e-01],\n",
      "          [ 3.0199e-01,  7.3119e-01,  5.5410e-01,  ..., -3.6651e-01,\n",
      "            3.1737e-01, -1.3162e-01],\n",
      "          [ 3.5841e-01,  6.7929e-01,  5.4205e-01,  ..., -2.9148e-01,\n",
      "            4.0450e-01, -1.1785e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.5815e-01,  3.6848e-01,  2.0598e-01,  ...,  2.5280e-01,\n",
      "           -6.7625e-01, -1.0603e-01],\n",
      "          [-1.8379e-01,  3.3469e-01,  2.6074e-01,  ...,  3.5287e-01,\n",
      "           -7.0778e-01, -9.1929e-02],\n",
      "          [-2.2644e-01,  3.6683e-01,  2.7574e-01,  ...,  2.5060e-01,\n",
      "           -7.0468e-01, -3.3839e-02],\n",
      "          ...,\n",
      "          [-2.0881e-01,  3.2751e-01,  2.2919e-01,  ...,  2.8737e-01,\n",
      "           -6.8925e-01, -1.2120e-01],\n",
      "          [-1.6828e-01,  4.6695e-01,  1.8674e-01,  ...,  2.1914e-01,\n",
      "           -6.6018e-01, -6.6658e-02],\n",
      "          [-1.8116e-01,  4.2368e-01,  1.9398e-01,  ...,  2.3565e-01,\n",
      "           -6.6599e-01,  2.0721e-02]],\n",
      "\n",
      "         [[-8.0346e-02,  4.5411e-02, -1.7521e-01,  ..., -5.2706e-01,\n",
      "           -4.1076e-01, -6.2626e-01],\n",
      "          [-5.7303e-03,  4.2915e-02, -5.3367e-02,  ..., -5.1503e-01,\n",
      "           -3.8974e-01, -5.8887e-01],\n",
      "          [ 3.5183e-02,  5.3816e-02, -9.7589e-02,  ..., -5.2411e-01,\n",
      "           -3.4050e-01, -5.4293e-01],\n",
      "          ...,\n",
      "          [ 1.9332e-02,  8.2970e-02, -1.4722e-01,  ..., -4.8670e-01,\n",
      "           -3.8759e-01, -5.4300e-01],\n",
      "          [-2.6286e-02,  9.7738e-02, -1.4099e-01,  ..., -4.4731e-01,\n",
      "           -3.5783e-01, -4.5953e-01],\n",
      "          [-4.1542e-02,  5.2150e-02, -1.1490e-01,  ..., -5.2051e-01,\n",
      "           -4.4100e-01, -4.8183e-01]],\n",
      "\n",
      "         [[-3.8680e-01, -2.3265e-01,  4.9169e-02,  ..., -3.1279e-01,\n",
      "            3.6732e-02,  5.0030e-01],\n",
      "          [-4.0904e-01, -2.0183e-01,  1.3599e-01,  ..., -2.9917e-01,\n",
      "            7.9939e-02,  4.8247e-01],\n",
      "          [-3.8667e-01, -2.2173e-01,  4.0169e-02,  ..., -3.1724e-01,\n",
      "            4.0207e-02,  5.6435e-01],\n",
      "          ...,\n",
      "          [-4.2277e-01, -1.7447e-01,  1.0620e-01,  ..., -3.2529e-01,\n",
      "           -1.4298e-02,  5.5382e-01],\n",
      "          [-4.0195e-01, -2.2679e-01, -1.7251e-03,  ..., -3.3523e-01,\n",
      "            6.2755e-02,  5.2837e-01],\n",
      "          [-4.5481e-01, -2.5551e-01, -2.6750e-02,  ..., -3.3053e-01,\n",
      "            5.9303e-02,  5.6512e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0150e-01, -3.1243e-01,  3.3504e-01,  ...,  2.7826e-01,\n",
      "            5.2828e-03, -5.7331e-01],\n",
      "          [ 1.5832e-01, -2.4557e-01,  3.0785e-01,  ...,  2.1702e-01,\n",
      "            8.0694e-02, -6.0230e-01],\n",
      "          [ 1.0279e-01, -2.5526e-01,  3.3698e-01,  ...,  3.0072e-01,\n",
      "            5.3717e-02, -5.5832e-01],\n",
      "          ...,\n",
      "          [ 5.5614e-02, -3.7042e-01,  3.8936e-01,  ...,  2.8274e-01,\n",
      "           -4.4294e-03, -5.6347e-01],\n",
      "          [ 7.4886e-02, -3.1771e-01,  3.4700e-01,  ...,  3.2307e-01,\n",
      "            6.5478e-02, -5.3066e-01],\n",
      "          [ 9.8976e-02, -3.0637e-01,  3.2059e-01,  ...,  2.9477e-01,\n",
      "            8.6046e-02, -5.3170e-01]],\n",
      "\n",
      "         [[ 1.3436e-01,  7.5904e-01,  6.7560e-01,  ..., -2.5209e-01,\n",
      "            2.5596e-01,  1.0190e-01],\n",
      "          [ 6.7484e-02,  6.9203e-01,  6.8309e-01,  ..., -2.3400e-01,\n",
      "            2.0844e-01,  2.2063e-01],\n",
      "          [ 6.9789e-02,  7.4197e-01,  7.2614e-01,  ..., -2.3250e-01,\n",
      "            2.1292e-01,  1.3462e-01],\n",
      "          ...,\n",
      "          [ 6.5333e-02,  6.5976e-01,  6.2606e-01,  ..., -2.6880e-01,\n",
      "            2.4614e-01,  1.6245e-01],\n",
      "          [ 1.0060e-01,  7.4759e-01,  7.1642e-01,  ..., -2.3563e-01,\n",
      "            1.7341e-01,  1.4057e-01],\n",
      "          [ 7.2340e-02,  6.7828e-01,  7.0642e-01,  ..., -2.5715e-01,\n",
      "            2.6918e-01,  2.2140e-01]],\n",
      "\n",
      "         [[ 2.3639e-01,  6.3412e-01,  5.4899e-01,  ..., -2.3475e-01,\n",
      "            2.7836e-01, -2.2381e-01],\n",
      "          [ 2.6498e-01,  6.2480e-01,  5.4978e-01,  ..., -2.7206e-01,\n",
      "            2.3825e-01, -2.5672e-01],\n",
      "          [ 3.0892e-01,  6.4064e-01,  5.4645e-01,  ..., -2.5519e-01,\n",
      "            2.9871e-01, -1.7211e-01],\n",
      "          ...,\n",
      "          [ 2.0968e-01,  6.8528e-01,  5.0697e-01,  ..., -2.3726e-01,\n",
      "            2.7919e-01, -2.4574e-01],\n",
      "          [ 2.4931e-01,  5.9546e-01,  5.2354e-01,  ..., -3.0911e-01,\n",
      "            2.5550e-01, -1.5473e-01],\n",
      "          [ 2.5032e-01,  5.6735e-01,  6.3709e-01,  ..., -3.4237e-01,\n",
      "            2.0233e-01, -1.6799e-01]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要进行残差连接？\n",
    "也许你已经注意到，前馈神经网络的输出`ffn_output 的形状： torch.Size([32, 50, 512])`与预处理之后的数据`x = preprocessor(input_ids)`、多头注意力的输出`拼接后的 output 的形状： torch.Size([32, 50, 512])`的形状一致。这让我们想到也许能够将其进行相加之类的操作。\n",
    "\n",
    "**（1）保留原始信息**\n",
    "- 多头注意力机制已经捕捉了序列中元素之间的关系。\n",
    "- 残差连接确保这些信息不会被前馈神经网络完全覆盖，保留原始特征。\n",
    "\n",
    "**（2）缓解梯度消失**\n",
    "- 深层网络中，梯度在反向传播时容易消失。\n",
    "- 残差连接提供了一条“捷径”，使梯度可以直接传播到浅层，缓解梯度消失问题。\n",
    "\n",
    "**（3）增强模型表达能力**\n",
    "- 前馈神经网络引入了非线性变换，增强了模型的表达能力。\n",
    "- 残差连接将这种非线性变换与原始特征结合，进一步提升模型性能。\n",
    "\n",
    "**（4）加速训练**\n",
    "- 残差连接使模型更容易优化，加速训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "norm1 = nn.LayerNorm(d_model)\n",
    "norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "x = norm1(x + projected_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在此之后，这个`x`再经过我们之前提到过的`Feed-Forward`得到的`ffn_output = ffn(projected_output)`进行残差链接，最后将`x`进行层归一化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "x = norm2(x + ffn_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 为什么要层归一化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，主要用于加速训练过程并提高模型的稳定性。以下是详细解释：\n",
    "\n",
    "**. 层归一化的作用**\n",
    "- **归一化特征**：\n",
    "  - 对每个样本的特征进行归一化，使其均值为0，方差为1。\n",
    "  - 减少内部协变量偏移（Internal Covariate Shift），使训练更稳定。\n",
    "- **加速收敛**：\n",
    "  - 归一化后的特征分布更稳定，有助于加速模型收敛。\n",
    "- **适用于不同任务**：\n",
    "  - 特别适合处理变长序列（如NLP任务）和小批量数据。\n",
    "\n",
    "**. 层归一化的公式**\n",
    "层归一化的计算公式如下：\n",
    "```python\n",
    "y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "```\n",
    "- **`x`**：输入特征。\n",
    "- **`mean`**：输入特征的均值。\n",
    "- **`var`**：输入特征的方差。\n",
    "- **`eps`**：一个小常数，用于数值稳定性（默认`1e-5`）。\n",
    "- **`gamma`**：可学习的缩放参数（权重）。\n",
    "- **`beta`**：可学习的偏移参数（偏置）。\n",
    "\n",
    "---\n",
    "\n",
    "**. 层归一化的特点**\n",
    "- **独立于批量大小**：\n",
    "  - 与批量归一化（Batch Normalization）不同，层归一化不依赖于批量大小，适合处理小批量或变长序列。\n",
    "- **逐样本归一化**：\n",
    "  - 对每个样本的特征进行归一化，而不是跨样本归一化。\n",
    "- **可学习的参数**：\n",
    "  - `gamma` 和 `beta` 是可学习的参数，允许模型调整归一化后的特征分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是`Encoder`中一个`EncoderLayer`的全部内容，为了获取`Encoder`，我们需要将`EncoderLayer`堆叠起来。你可以在`transformer_encoder.py`中找到完整的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Decoder 🐱 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. **掩码多头自注意力**：\n",
    "   - 捕捉已生成序列的内部关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 为什么就需要掩码了？之前的Encoder中的注意力不需要掩码呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **任务性质不同**\n",
    "- **Encoder**：\n",
    "  - 处理的是完整的输入序列（如源语言句子）\n",
    "  - 需要同时看到整个序列的所有信息\n",
    "  - 目标是捕捉序列中所有元素之间的关系\n",
    "\n",
    "- **Decoder**：\n",
    "  - 处理的是目标序列（如目标语言句子）\n",
    "  - 需要逐步生成序列，不能\"偷看\"未来信息\n",
    "  - 目标是基于已生成的部分序列和Encoder的输出来预测下一个元素\n",
    "\n",
    "##### 2. **掩码的作用**\n",
    "- **防止信息泄露**：\n",
    "  - 在训练时，Decoder会接收完整的目标序列\n",
    "  - 如果没有掩码，模型可能会直接\"看到\"未来的信息，导致训练作弊\n",
    "  - 掩码确保模型只能使用当前位置及之前的信息\n",
    "\n",
    "- **保持自回归性质**：\n",
    "  - 在推理时，Decoder需要逐个生成序列元素\n",
    "  - 掩码确保模型只能基于已生成的部分序列进行预测\n",
    "  - 这是序列生成任务（如机器翻译、文本生成）的基本要求\n",
    "\n",
    "##### 3. **具体实现**\n",
    "- **Encoder中的注意力**：\n",
    "  - 计算注意力分数时，所有位置之间都可以相互关注\n",
    "  - 不需要任何限制，因为整个输入序列是已知的\n",
    "\n",
    "- **Decoder中的掩码注意力**：\n",
    "  - 使用下三角掩码（`torch.tril`）\n",
    "  - 确保每个位置只能关注到它自身及之前的位置\n",
    "  - 未来位置的注意力分数被设置为`-inf`，在softmax后权重为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "Encoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "Decoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "mask 的形状： torch.Size([50, 50])\n",
      "mask 的值： tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "# Encoder注意力分数（无掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "# print(\"Encoder注意力分数 scores：\", scores)\n",
    "print(\"Encoder注意力分数 scores 的形状：\", scores.shape)\n",
    "# Decoder注意力分数（带掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # 下三角掩码\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))  # 应用掩码\n",
    "# print(\"Decoder注意力分数 scores：\", scores)\n",
    "print(\"Decoder注意力分数 scores 的形状：\", scores.shape)\n",
    "print(\"mask 的形状：\", mask.shape)\n",
    "print(\"mask 的值：\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 为什么需要下三角掩码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **自回归任务的性质**\n",
    "在自回归任务中，模型需要**逐步生成序列**，即每次只能基于已经生成的部分序列来预测下一个元素。例如：\n",
    "- 在文本生成中，模型只能基于已经生成的单词来预测下一个单词。\n",
    "- 在机器翻译中，模型只能基于已经生成的目标语言单词来预测下一个单词。\n",
    "\n",
    "如果模型能够“看到”未来的信息，它就会作弊，直接使用未来的信息来预测当前的位置，这会导致训练和推理不一致。\n",
    "\n",
    "\n",
    "##### 2. **下三角掩码的作用**\n",
    "下三角掩码的作用是**限制模型只能访问当前位置及之前的信息**，而不能访问未来的信息。具体来说：\n",
    "- **下三角部分**：值为1，表示允许模型访问这些位置的信息。\n",
    "- **上三角部分**：值为0，表示禁止模型访问这些位置的信息。\n",
    "\n",
    "通过将上三角部分的注意力分数设置为`-inf`，在softmax操作后，这些位置的权重会变为0，从而确保模型无法利用未来的信息。\n",
    "\n",
    "##### 3. **掩码的实现**\n",
    "在代码中，下三角掩码通常通过`torch.tril`函数生成：\n",
    "````python\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "````\n",
    "例如，对于一个长度为5的序列，掩码矩阵如下：\n",
    "````\n",
    "1 0 0 0 0\n",
    "1 1 0 0 0\n",
    "1 1 1 0 0\n",
    "1 1 1 1 0\n",
    "1 1 1 1 1\n",
    "````\n",
    "- 第1行：只能看到第1个位置。\n",
    "- 第2行：可以看到第1和第2个位置。\n",
    "- 第3行：可以看到第1、第2和第3个位置。\n",
    "- 以此类推。\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **为什么需要下三角掩码？**\n",
    "###### （1）**训练时防止信息泄露**\n",
    "- 在训练时，Decoder会接收完整的目标序列（如目标语言句子）。\n",
    "- 如果没有掩码，模型可能会直接“看到”未来的信息，导致训练作弊。\n",
    "- 掩码确保模型只能使用当前位置及之前的信息。\n",
    "\n",
    "###### （2）**推理时保持自回归性质**\n",
    "- 在推理时，Decoder需要逐个生成序列元素。\n",
    "- 掩码确保模型只能基于已生成的部分序列进行预测。\n",
    "- 这是序列生成任务（如机器翻译、文本生成）的基本要求。\n",
    "\n",
    "###### （3）**确保训练和推理一致性**\n",
    "- 训练时使用掩码，推理时也使用掩码，确保模型的行为一致。\n",
    "- 如果训练时不使用掩码，模型可能会学习到依赖未来信息的错误模式，导致推理时性能下降。\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **示例**\n",
    "假设我们有一个长度为3的序列，计算注意力分数时：\n",
    "- **无掩码**：模型可以看到所有位置的信息。\n",
    "  ````\n",
    "  scores = [[s11, s12, s13],\n",
    "            [s21, s22, s23],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "- **有掩码**：模型只能看到当前位置及之前的信息。\n",
    "  ````\n",
    "  scores = [[s11, -inf, -inf],\n",
    "            [s21, s22, -inf],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "在softmax后，`-inf`的位置权重为0，模型无法利用这些信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 6. **总结**\n",
    "| 特性                | 无掩码                  | 下三角掩码              |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| 信息访问            | 可以访问整个序列        | 只能访问当前位置及之前  |\n",
    "| 训练时              | 可能作弊，利用未来信息  | 防止信息泄露            |\n",
    "| 推理时              | 无法逐步生成序列        | 保持自回归性质          |\n",
    "| 适用场景            | Encoder                | Decoder                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 输入是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tgt: 目标序列的嵌入表示。也就是我们之前通过TransformerPreprocessor得到的被预处理的输入。\n",
    "2. tgt_mask: 下三角掩码，用于限制模型只能访问当前位置及之前的信息。\n",
    "3. memory: Encoder的输出，用于计算Encoder-Decoder Attention。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Output Layer 🐱 输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在《Attention is All You Need》论文中，输出层的主要意义是将解码器的输出转换为最终的预测结果。具体来说，输出层的作用包括：\n",
    "\n",
    "1. **线性变换**：将解码器的输出（通常是高维向量，在我们的例子中`输出: torch.Size([32, 50, 512])`）映射到词汇表大小的维度。这一步通过一个线性层（`nn.Linear`）实现，生成每个位置每个词的概率分数（logits）。\n",
    "\n",
    "2. **生成概率分布**：通过 Softmax 函数将 logits 转换为概率分布。这个概率分布表示每个位置每个词的概率，模型可以根据这个分布选择最可能的词作为输出。\n",
    "\n",
    "3. **序列生成**：在序列生成任务（如机器翻译、文本生成等）中，输出层的概率分布用于生成最终的序列。通常，模型会选择概率最高的词作为下一个词，逐步生成整个序列。\n",
    "\n",
    "总结来说，输出层的作用是将解码器的抽象表示转换为具体的词汇概率分布，从而生成最终的预测结果。这是 Transformer 模型完成序列生成任务的关键步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits 形状: torch.Size([32, 50, 10000])\n",
      "概率分布形状: torch.Size([32, 50, 10000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        # 线性层，将 d_model 维映射到词汇表大小\n",
    "        self.linear = nn.Linear(d_model, vocab_size) # 别忘了我们设置d_model为512，vocab_size为10000\n",
    "        # Softmax 函数，用于生成概率分布\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性变换，输出形状: [batch_size, seq_len, vocab_size]\n",
    "        logits = self.linear(x)\n",
    "        # Softmax 生成概率分布\n",
    "        probs = self.softmax(logits)\n",
    "        return logits, probs\n",
    "    \n",
    "\n",
    "# 初始化输出层\n",
    "output_layer = OutputLayer(d_model=512, vocab_size=vocab_size)\n",
    "\n",
    "# 解码器的输出，形状为 [32, 50, 512]\n",
    "decoder_output = torch.randn(32, 50, 512)\n",
    "\n",
    "# 前向传播\n",
    "logits, probs = output_layer(decoder_output)\n",
    "\n",
    "# 检查输出形状\n",
    "print(\"Logits 形状:\", logits.shape)  # 输出: torch.Size([32, 50, 10000])\n",
    "print(\"概率分布形状:\", probs.shape)  # 输出: torch.Size([32, 50, 10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`logits` 形状 `[32, 50, 10000]`**：\n",
    "  - `32` 是批次大小（batch size），表示同时处理 32 个样本。\n",
    "  - `50` 是序列长度（sequence length），表示每个样本有 50 个时间步或位置。\n",
    "  - `10000` 是词汇表大小（vocab size），表示每个位置有 10000 个可能的词。\n",
    "\n",
    "- **`probs` 形状 `[32, 50, 10000]`**：\n",
    "  - 这是 `logits` 经过 Softmax 后的概率分布，形状与 `logits` 相同。\n",
    "  - 每个位置的概率分布总和为 1，表示模型对每个位置预测的词的概率。\n",
    "\n",
    "### 为什么是这种形状？\n",
    "- 这种形状符合 Transformer 的输出设计，适用于序列生成任务（如机器翻译、文本生成等）。\n",
    "- 每个位置的概率分布可以用于选择最可能的词，逐步生成输出序列。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
