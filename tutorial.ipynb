{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[-0.8799,  0.4084,  0.2450],\n",
      "        [ 1.1380, -0.1481, -0.4387],\n",
      "        [ 0.9900,  0.2413, -0.2034]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征映射到新的特征空间。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[-0.1226,  0.2862,  1.5646,  ..., -0.1917, -0.4041,  0.0341],\n",
      "         [-0.6776,  1.0134,  0.0759,  ...,  0.0781, -0.3726,  0.6562],\n",
      "         [-0.4465,  0.3382,  0.7220,  ...,  0.6275, -0.2743,  0.1470],\n",
      "         ...,\n",
      "         [ 0.2620,  1.3220, -0.0358,  ..., -0.0144,  0.4962, -0.5854],\n",
      "         [ 0.8198,  0.8948,  1.6828,  ...,  0.8175,  0.5629,  0.1582],\n",
      "         [ 0.1410,  0.0686,  1.3047,  ...,  0.1412, -0.5859, -0.4544]],\n",
      "\n",
      "        [[-0.5151,  0.6531, -0.0987,  ...,  0.1626,  0.1482,  0.3918],\n",
      "         [-0.6260, -0.0616, -0.0175,  ...,  0.4259, -0.1129,  0.0267],\n",
      "         [-1.1387,  0.3664,  0.3647,  ...,  0.0475, -0.3409,  0.5844],\n",
      "         ...,\n",
      "         [ 1.2401,  0.8850,  0.5799,  ...,  0.6223,  0.4027, -0.3126],\n",
      "         [ 0.7176,  0.2893,  1.1336,  ...,  0.5507,  0.1798,  0.1274],\n",
      "         [ 0.1768,  0.4510,  0.1322,  ...,  1.1007, -0.0029, -0.8731]],\n",
      "\n",
      "        [[-0.7582,  0.9219,  0.3688,  ..., -0.1324, -0.3082,  0.0451],\n",
      "         [ 0.3861,  0.9874,  0.2233,  ...,  0.8794, -1.0756,  1.0110],\n",
      "         [-0.7768,  0.1555, -1.4599,  ...,  1.2956,  0.6782, -0.1259],\n",
      "         ...,\n",
      "         [ 0.2344,  0.7341,  0.7630,  ...,  0.4006, -0.0032,  0.3170],\n",
      "         [ 0.8848,  1.3560,  0.3002,  ...,  1.2849,  0.2671, -0.1364],\n",
      "         [ 0.0614, -0.2119,  1.1807,  ...,  0.6025,  0.4922,  1.2457]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7381,  0.3716,  0.9601,  ...,  0.9638, -1.0365, -0.6811],\n",
      "         [-1.0934,  0.7407,  0.5336,  ...,  1.0180, -0.9181,  0.5562],\n",
      "         [-0.4753,  0.4446,  1.1816,  ..., -0.3033, -1.7357,  0.7297],\n",
      "         ...,\n",
      "         [ 0.8217,  0.3981,  0.2345,  ...,  1.0075,  0.3865, -0.5128],\n",
      "         [ 1.0301,  0.8126,  0.2805,  ...,  0.7347, -1.0556,  0.5918],\n",
      "         [ 0.6670, -0.7536,  0.4176,  ..., -0.0096, -0.1290,  0.2087]],\n",
      "\n",
      "        [[-0.3120,  0.8192,  1.9575,  ...,  0.7554,  0.9986,  0.6783],\n",
      "         [ 0.2559,  0.7327,  0.6480,  ..., -0.2461, -0.9906,  0.4849],\n",
      "         [-0.7306,  0.8479,  0.9548,  ...,  0.9673, -1.5706,  0.0636],\n",
      "         ...,\n",
      "         [-0.1274,  0.9485,  1.0521,  ...,  0.0449,  0.8311, -1.2947],\n",
      "         [ 0.7786,  0.8899,  0.7042,  ...,  0.3138,  0.6390,  1.0112],\n",
      "         [ 0.3738,  0.1027,  1.6025,  ...,  0.2178,  0.0825, -0.5435]],\n",
      "\n",
      "        [[ 0.1596,  0.3861, -0.3555,  ...,  0.2751,  0.6663,  0.3950],\n",
      "         [ 0.7115, -0.0551, -0.3642,  ...,  1.4115, -1.0406, -0.0650],\n",
      "         [ 0.3498, -0.0784,  0.1442,  ...,  0.8559,  0.1885, -0.1802],\n",
      "         ...,\n",
      "         [ 1.0516,  0.0280,  0.6393,  ..., -0.8188, -0.2773,  0.4627],\n",
      "         [ 0.8645, -0.7262, -0.0660,  ..., -0.0929,  0.4192, -0.8666],\n",
      "         [ 0.5336,  1.2314,  0.2506,  ..., -0.2053,  0.0074, -0.2432]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[ 0.1862, -0.9980,  1.1941,  ...,  0.4128,  0.0309, -0.2603],\n",
      "         [ 0.2883, -1.3367,  0.5619,  ..., -0.2786,  0.4650, -1.5338],\n",
      "         [ 1.5733, -0.2811,  1.8463,  ...,  0.2132, -0.0113, -0.3896],\n",
      "         ...,\n",
      "         [-0.1653, -1.0391, -0.2038,  ...,  0.3715,  1.6271,  0.0661],\n",
      "         [-0.5538, -0.5909,  0.2244,  ...,  0.3379,  1.9594,  1.1377],\n",
      "         [-0.5053,  0.9001,  0.2776,  ...,  0.3639,  0.8736, -0.4729]],\n",
      "\n",
      "        [[ 1.3498, -0.8324,  0.0187,  ...,  0.3003,  0.4722, -0.0606],\n",
      "         [ 0.0341,  0.5431,  0.7953,  ...,  0.1260,  0.0977, -1.4632],\n",
      "         [ 1.2146, -0.0846, -0.2377,  ..., -0.5830,  0.0758,  0.2158],\n",
      "         ...,\n",
      "         [ 0.0569, -0.4491,  0.6229,  ...,  0.9351,  0.4361, -0.4048],\n",
      "         [-0.0827, -0.4223,  0.0377,  ...,  0.0769,  0.4162,  0.9552],\n",
      "         [-0.5377,  0.0579,  0.7260,  ...,  0.6118,  0.6669, -0.5818]],\n",
      "\n",
      "        [[ 2.3839, -0.3381,  0.0900,  ...,  0.6671,  0.4231, -0.5244],\n",
      "         [-0.0711,  0.1469,  0.2031,  ...,  0.6255, -0.1853, -1.3061],\n",
      "         [ 0.6418,  0.2791,  1.1359,  ..., -0.2108,  0.3411,  0.0861],\n",
      "         ...,\n",
      "         [-0.0677, -0.8873,  1.4009,  ..., -0.3925,  0.0868, -0.6904],\n",
      "         [-0.5419,  0.1655, -0.3653,  ...,  0.7661,  0.9553, -1.3103],\n",
      "         [-0.6522,  0.3584, -1.6241,  ...,  1.5697,  1.0975, -0.4778]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0130, -0.6221,  0.8764,  ...,  0.6699,  0.3847, -0.1878],\n",
      "         [ 0.7054, -0.0647,  0.7722,  ...,  0.2364,  0.6935,  1.4756],\n",
      "         [ 1.3377,  0.9868, -0.2304,  ..., -0.4612,  0.4276, -0.2342],\n",
      "         ...,\n",
      "         [ 1.0305,  0.4753,  0.1727,  ...,  1.0024,  0.0042, -0.6718],\n",
      "         [-0.9317, -0.5480, -0.3677,  ..., -0.2483,  0.2123, -0.7029],\n",
      "         [ 0.0723, -0.5581,  0.7148,  ..., -1.0055, -0.2772,  0.2460]],\n",
      "\n",
      "        [[ 0.1069, -0.8872, -0.0112,  ...,  0.4665, -0.1548,  0.2803],\n",
      "         [-0.7635, -0.1704,  0.2488,  ..., -0.0229,  1.4299,  0.4887],\n",
      "         [ 0.9835, -0.6534,  0.1256,  ..., -0.2895,  0.9778, -0.7282],\n",
      "         ...,\n",
      "         [-0.1441, -0.3607, -0.2320,  ..., -0.5816, -0.6278, -0.4580],\n",
      "         [ 0.5856, -0.5126,  0.3256,  ...,  0.6688, -0.2777, -0.0999],\n",
      "         [-0.6233, -0.6606,  0.0794,  ...,  0.2357, -0.0136, -0.9872]],\n",
      "\n",
      "        [[ 1.0428, -0.9314,  0.5852,  ..., -0.2046,  1.4931, -0.5051],\n",
      "         [ 0.7967, -0.5663,  0.9938,  ...,  0.6886, -0.2883, -0.1556],\n",
      "         [-0.0485,  0.0584,  1.5339,  ..., -0.0462, -0.0997, -0.2206],\n",
      "         ...,\n",
      "         [ 0.6194, -0.9872,  0.7847,  ..., -0.1174, -0.6118, -0.4235],\n",
      "         [-0.8317, -1.1139,  1.1163,  ...,  0.1795,  0.8640, -0.3863],\n",
      "         [-0.2801, -0.3589,  0.0738,  ...,  0.7383,  0.6711,  0.0073]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[ 7.4460e-02, -2.4420e-02, -2.4543e-01,  ...,  4.6486e-01,\n",
      "           2.0357e-02,  3.3402e-01],\n",
      "         [ 2.6088e-01, -3.2276e-01, -7.1354e-01,  ..., -6.7818e-01,\n",
      "          -1.3527e+00, -1.4361e+00],\n",
      "         [-8.8744e-01, -7.5276e-01,  1.2953e-02,  ...,  2.6267e-01,\n",
      "          -8.1043e-01, -2.2658e-01],\n",
      "         ...,\n",
      "         [-8.7365e-02, -1.2480e-01, -1.1142e+00,  ..., -7.6763e-02,\n",
      "          -2.9498e-02,  5.3579e-01],\n",
      "         [ 4.2268e-01, -7.6757e-01, -5.1605e-01,  ..., -3.6273e-02,\n",
      "           4.3858e-02, -5.1905e-01],\n",
      "         [ 2.8738e-01, -2.3666e-01, -5.9761e-01,  ..., -3.3073e-01,\n",
      "           1.9729e-01,  4.5245e-01]],\n",
      "\n",
      "        [[ 2.9637e-01, -4.9153e-01, -9.8560e-01,  ...,  6.5303e-01,\n",
      "          -1.5145e+00, -8.0871e-01],\n",
      "         [-1.1063e+00, -5.1515e-01, -1.2364e+00,  ...,  9.2645e-01,\n",
      "           1.3640e-01, -1.0547e-01],\n",
      "         [-4.0620e-01, -7.9838e-01,  5.3947e-01,  ...,  1.8142e-02,\n",
      "           4.2734e-01,  1.0466e+00],\n",
      "         ...,\n",
      "         [ 4.3120e-01, -1.1056e-01,  4.8159e-01,  ...,  5.5539e-01,\n",
      "          -4.3566e-01,  2.4849e-01],\n",
      "         [ 8.3933e-02, -6.7067e-01, -1.8932e+00,  ..., -1.0737e-01,\n",
      "           1.4042e-01, -6.1634e-01],\n",
      "         [-9.2539e-01,  8.1986e-02, -2.6094e-01,  ..., -2.4293e-01,\n",
      "          -1.4097e-01,  7.9465e-01]],\n",
      "\n",
      "        [[-4.8399e-01,  1.5279e-01, -1.0969e+00,  ..., -3.1189e-02,\n",
      "          -4.2233e-01, -2.6535e-01],\n",
      "         [-1.1480e+00, -3.7605e-01, -5.9316e-02,  ..., -2.1535e-01,\n",
      "          -1.4707e-01, -5.7939e-04],\n",
      "         [-1.2423e+00, -9.7475e-01,  7.0459e-02,  ..., -2.0036e-01,\n",
      "          -3.2488e-01, -1.2483e-01],\n",
      "         ...,\n",
      "         [-2.7811e-02, -6.4858e-01, -4.9960e-01,  ...,  4.1972e-01,\n",
      "          -2.9178e-01,  1.0860e-01],\n",
      "         [-6.8841e-01, -4.1981e-01, -9.4274e-01,  ..., -6.8797e-01,\n",
      "           1.1561e-02,  2.0726e-01],\n",
      "         [-1.9994e+00, -1.9168e-01, -1.4201e-01,  ...,  1.1067e+00,\n",
      "           5.3729e-01,  5.8543e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.1306e-02, -9.0211e-01, -1.2032e+00,  ...,  2.5033e-01,\n",
      "          -2.9637e-01, -7.4022e-01],\n",
      "         [-2.8599e-01, -8.7616e-02, -4.3611e-01,  ...,  1.4959e-01,\n",
      "          -1.9196e-01,  5.0210e-01],\n",
      "         [-1.4074e+00, -5.1678e-01, -8.3527e-01,  ..., -8.0578e-01,\n",
      "           2.5425e-01, -4.5824e-01],\n",
      "         ...,\n",
      "         [-1.3521e+00,  6.4469e-04, -9.7456e-01,  ..., -9.2113e-01,\n",
      "          -8.6289e-03, -1.4415e-01],\n",
      "         [ 4.0253e-01, -1.5117e-01, -8.7069e-01,  ...,  1.2977e-01,\n",
      "           3.6843e-01, -2.0962e-01],\n",
      "         [-3.5187e-01, -1.1596e+00,  1.9227e-02,  ...,  7.5990e-01,\n",
      "           2.2932e-01,  1.1482e+00]],\n",
      "\n",
      "        [[-4.7918e-01, -5.3762e-01, -2.1460e-01,  ...,  5.9280e-01,\n",
      "           9.5116e-01, -2.7832e-01],\n",
      "         [ 6.8320e-01, -1.7875e-01, -1.6997e-01,  ..., -8.5413e-02,\n",
      "          -5.9794e-01,  5.1899e-01],\n",
      "         [ 4.4343e-01, -6.6213e-01, -2.3622e+00,  ..., -3.9782e-01,\n",
      "          -1.0876e+00, -1.4285e+00],\n",
      "         ...,\n",
      "         [ 2.2043e-01, -7.6795e-01, -7.2841e-01,  ...,  4.7425e-01,\n",
      "          -7.2770e-01, -5.6592e-01],\n",
      "         [-5.1704e-01,  5.1047e-01,  1.7047e-01,  ..., -8.8128e-01,\n",
      "          -2.9603e-01, -5.3421e-01],\n",
      "         [-9.8023e-01, -5.2046e-01, -1.3900e+00,  ...,  3.9537e-01,\n",
      "           6.0802e-01, -3.3956e-01]],\n",
      "\n",
      "        [[-5.2180e-01,  8.5957e-01, -1.2115e+00,  ...,  4.6473e-01,\n",
      "          -3.7103e-01, -7.6623e-01],\n",
      "         [-6.2950e-02, -4.0397e-01, -1.1724e+00,  ...,  4.7766e-01,\n",
      "           4.4551e-01, -6.6651e-01],\n",
      "         [-9.4035e-01, -1.6304e-01, -1.1082e+00,  ..., -1.6581e-01,\n",
      "          -4.8936e-01, -8.3091e-01],\n",
      "         ...,\n",
      "         [ 3.7573e-01, -1.2244e-01,  1.4045e-01,  ..., -3.3369e-01,\n",
      "          -7.4223e-01, -6.8987e-01],\n",
      "         [-1.8564e+00, -7.5100e-01,  3.1446e-01,  ..., -1.3553e-01,\n",
      "          -3.2297e-01,  2.7132e-02],\n",
      "         [ 4.7859e-01, -1.1688e-02, -1.4449e+00,  ..., -4.0436e-02,\n",
      "           1.6902e+00,  9.6392e-01]]], grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **分割多头的目的**\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n",
    "\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim\n",
    "#　将`num_heads`维度提到前面，方便后续并行计算。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0207, 0.0130, 0.0080,  ..., 0.0092, 0.0066, 0.0086],\n",
      "          [0.0213, 0.0136, 0.0079,  ..., 0.0239, 0.0084, 0.0101],\n",
      "          [0.0152, 0.0163, 0.0103,  ..., 0.0158, 0.0179, 0.0150],\n",
      "          ...,\n",
      "          [0.0116, 0.0104, 0.0272,  ..., 0.0093, 0.0225, 0.0113],\n",
      "          [0.0132, 0.0154, 0.0462,  ..., 0.0072, 0.0119, 0.0136],\n",
      "          [0.0249, 0.0194, 0.0296,  ..., 0.0206, 0.0259, 0.0189]],\n",
      "\n",
      "         [[0.0324, 0.0123, 0.0160,  ..., 0.0307, 0.0190, 0.0336],\n",
      "          [0.0170, 0.0214, 0.0222,  ..., 0.0212, 0.0287, 0.0214],\n",
      "          [0.0223, 0.0148, 0.0268,  ..., 0.0582, 0.0168, 0.0493],\n",
      "          ...,\n",
      "          [0.0206, 0.0195, 0.0137,  ..., 0.0290, 0.0109, 0.0199],\n",
      "          [0.0225, 0.0074, 0.0186,  ..., 0.0185, 0.0289, 0.0242],\n",
      "          [0.0128, 0.0104, 0.0125,  ..., 0.0210, 0.0335, 0.0201]],\n",
      "\n",
      "         [[0.0386, 0.0223, 0.0082,  ..., 0.0065, 0.0319, 0.0141],\n",
      "          [0.0449, 0.0201, 0.0182,  ..., 0.0196, 0.0327, 0.0227],\n",
      "          [0.0436, 0.0203, 0.0154,  ..., 0.0161, 0.0370, 0.0401],\n",
      "          ...,\n",
      "          [0.0174, 0.0115, 0.0111,  ..., 0.0231, 0.0119, 0.0162],\n",
      "          [0.0182, 0.0088, 0.0069,  ..., 0.0187, 0.0315, 0.0339],\n",
      "          [0.0328, 0.0098, 0.0090,  ..., 0.0227, 0.0251, 0.0171]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0212, 0.0177, 0.0131,  ..., 0.0126, 0.0125, 0.0145],\n",
      "          [0.0181, 0.0114, 0.0124,  ..., 0.0216, 0.0174, 0.0191],\n",
      "          [0.0199, 0.0093, 0.0108,  ..., 0.0123, 0.0170, 0.0282],\n",
      "          ...,\n",
      "          [0.0295, 0.0142, 0.0238,  ..., 0.0193, 0.0154, 0.0192],\n",
      "          [0.0170, 0.0114, 0.0204,  ..., 0.0093, 0.0175, 0.0261],\n",
      "          [0.0236, 0.0099, 0.0179,  ..., 0.0357, 0.0153, 0.0304]],\n",
      "\n",
      "         [[0.0160, 0.0080, 0.0080,  ..., 0.0157, 0.0144, 0.0244],\n",
      "          [0.0210, 0.0207, 0.0059,  ..., 0.0166, 0.0084, 0.0253],\n",
      "          [0.0248, 0.0211, 0.0102,  ..., 0.0255, 0.0417, 0.0254],\n",
      "          ...,\n",
      "          [0.0196, 0.0282, 0.0270,  ..., 0.0163, 0.0191, 0.0137],\n",
      "          [0.0104, 0.0089, 0.0342,  ..., 0.0246, 0.0212, 0.0100],\n",
      "          [0.0274, 0.0185, 0.0282,  ..., 0.0257, 0.0264, 0.0131]],\n",
      "\n",
      "         [[0.0252, 0.0096, 0.0213,  ..., 0.0119, 0.0091, 0.0181],\n",
      "          [0.0216, 0.0042, 0.0170,  ..., 0.0232, 0.0232, 0.0158],\n",
      "          [0.0229, 0.0110, 0.0212,  ..., 0.0151, 0.0326, 0.0348],\n",
      "          ...,\n",
      "          [0.0265, 0.0208, 0.0229,  ..., 0.0276, 0.0210, 0.0245],\n",
      "          [0.0183, 0.0048, 0.0203,  ..., 0.0167, 0.0216, 0.0290],\n",
      "          [0.0194, 0.0276, 0.0374,  ..., 0.0149, 0.0240, 0.0247]]],\n",
      "\n",
      "\n",
      "        [[[0.0165, 0.0174, 0.0185,  ..., 0.0396, 0.0190, 0.0167],\n",
      "          [0.0174, 0.0208, 0.0103,  ..., 0.0262, 0.0255, 0.0177],\n",
      "          [0.0131, 0.0215, 0.0108,  ..., 0.0150, 0.0172, 0.0219],\n",
      "          ...,\n",
      "          [0.0202, 0.0194, 0.0295,  ..., 0.0099, 0.0136, 0.0205],\n",
      "          [0.0300, 0.0167, 0.0153,  ..., 0.0204, 0.0146, 0.0321],\n",
      "          [0.0210, 0.0147, 0.0265,  ..., 0.0128, 0.0139, 0.0237]],\n",
      "\n",
      "         [[0.0176, 0.0446, 0.0195,  ..., 0.0257, 0.0175, 0.0179],\n",
      "          [0.0125, 0.0117, 0.0217,  ..., 0.0296, 0.0160, 0.0295],\n",
      "          [0.0144, 0.0430, 0.0172,  ..., 0.0289, 0.0079, 0.0090],\n",
      "          ...,\n",
      "          [0.0343, 0.0260, 0.0246,  ..., 0.0377, 0.0082, 0.0146],\n",
      "          [0.0140, 0.0110, 0.0212,  ..., 0.0172, 0.0225, 0.0196],\n",
      "          [0.0102, 0.0265, 0.0364,  ..., 0.0207, 0.0306, 0.0210]],\n",
      "\n",
      "         [[0.0227, 0.0293, 0.0259,  ..., 0.0218, 0.0269, 0.0254],\n",
      "          [0.0236, 0.0136, 0.0232,  ..., 0.0325, 0.0174, 0.0324],\n",
      "          [0.0187, 0.0268, 0.0195,  ..., 0.0148, 0.0303, 0.0259],\n",
      "          ...,\n",
      "          [0.0318, 0.0136, 0.0178,  ..., 0.0193, 0.0340, 0.0268],\n",
      "          [0.0146, 0.0075, 0.0189,  ..., 0.0219, 0.0127, 0.0405],\n",
      "          [0.0199, 0.0125, 0.0215,  ..., 0.0183, 0.0530, 0.0114]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0111, 0.0165, 0.0185,  ..., 0.0190, 0.0124, 0.0159],\n",
      "          [0.0081, 0.0084, 0.0133,  ..., 0.0265, 0.0212, 0.0154],\n",
      "          [0.0149, 0.0097, 0.0186,  ..., 0.0194, 0.0190, 0.0168],\n",
      "          ...,\n",
      "          [0.0106, 0.0103, 0.0164,  ..., 0.0317, 0.0180, 0.0118],\n",
      "          [0.0099, 0.0158, 0.0153,  ..., 0.0149, 0.0092, 0.0133],\n",
      "          [0.0134, 0.0103, 0.0270,  ..., 0.0395, 0.0254, 0.0089]],\n",
      "\n",
      "         [[0.0137, 0.0138, 0.0083,  ..., 0.0095, 0.0151, 0.0294],\n",
      "          [0.0178, 0.0160, 0.0126,  ..., 0.0117, 0.0217, 0.0189],\n",
      "          [0.0194, 0.0283, 0.0112,  ..., 0.0064, 0.0203, 0.0160],\n",
      "          ...,\n",
      "          [0.0121, 0.0165, 0.0323,  ..., 0.0151, 0.0213, 0.0204],\n",
      "          [0.0067, 0.0155, 0.0109,  ..., 0.0114, 0.0257, 0.0230],\n",
      "          [0.0128, 0.0135, 0.0202,  ..., 0.0150, 0.0253, 0.0236]],\n",
      "\n",
      "         [[0.0211, 0.0206, 0.0125,  ..., 0.0236, 0.0186, 0.0203],\n",
      "          [0.0210, 0.0211, 0.0195,  ..., 0.0252, 0.0183, 0.0238],\n",
      "          [0.0144, 0.0078, 0.0295,  ..., 0.0116, 0.0197, 0.0180],\n",
      "          ...,\n",
      "          [0.0567, 0.0230, 0.0249,  ..., 0.0235, 0.0340, 0.0240],\n",
      "          [0.0200, 0.0225, 0.0217,  ..., 0.0158, 0.0125, 0.0168],\n",
      "          [0.0195, 0.0162, 0.0141,  ..., 0.0544, 0.0122, 0.0315]]],\n",
      "\n",
      "\n",
      "        [[[0.0075, 0.0068, 0.0059,  ..., 0.0231, 0.0168, 0.0153],\n",
      "          [0.0139, 0.0126, 0.0061,  ..., 0.0092, 0.0093, 0.0269],\n",
      "          [0.0090, 0.0092, 0.0068,  ..., 0.0114, 0.0113, 0.0282],\n",
      "          ...,\n",
      "          [0.0298, 0.0164, 0.0166,  ..., 0.0134, 0.0144, 0.0167],\n",
      "          [0.0276, 0.0156, 0.0127,  ..., 0.0333, 0.0185, 0.0202],\n",
      "          [0.0279, 0.0151, 0.0362,  ..., 0.0140, 0.0207, 0.0099]],\n",
      "\n",
      "         [[0.0107, 0.0215, 0.0076,  ..., 0.0381, 0.0287, 0.0348],\n",
      "          [0.0114, 0.0142, 0.0174,  ..., 0.0523, 0.0317, 0.0168],\n",
      "          [0.0071, 0.0124, 0.0051,  ..., 0.0334, 0.0211, 0.0233],\n",
      "          ...,\n",
      "          [0.0207, 0.0175, 0.0158,  ..., 0.0173, 0.0115, 0.0149],\n",
      "          [0.0203, 0.0119, 0.0179,  ..., 0.0399, 0.0343, 0.0236],\n",
      "          [0.0197, 0.0118, 0.0092,  ..., 0.0318, 0.0388, 0.0202]],\n",
      "\n",
      "         [[0.0098, 0.0344, 0.0211,  ..., 0.0187, 0.0269, 0.0473],\n",
      "          [0.0254, 0.0327, 0.0225,  ..., 0.0139, 0.0169, 0.0246],\n",
      "          [0.0070, 0.0422, 0.0175,  ..., 0.0080, 0.0228, 0.0751],\n",
      "          ...,\n",
      "          [0.0151, 0.0252, 0.0108,  ..., 0.0172, 0.0161, 0.0242],\n",
      "          [0.0133, 0.0589, 0.0238,  ..., 0.0189, 0.0212, 0.0395],\n",
      "          [0.0106, 0.0221, 0.0175,  ..., 0.0177, 0.0311, 0.0503]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0116, 0.0112, 0.0167,  ..., 0.0176, 0.0170, 0.0223],\n",
      "          [0.0132, 0.0316, 0.0243,  ..., 0.0130, 0.0134, 0.0134],\n",
      "          [0.0249, 0.0268, 0.0207,  ..., 0.0142, 0.0112, 0.0121],\n",
      "          ...,\n",
      "          [0.0185, 0.0234, 0.0273,  ..., 0.0138, 0.0166, 0.0090],\n",
      "          [0.0116, 0.0280, 0.0428,  ..., 0.0096, 0.0217, 0.0157],\n",
      "          [0.0089, 0.0139, 0.0176,  ..., 0.0091, 0.0141, 0.0164]],\n",
      "\n",
      "         [[0.0156, 0.0080, 0.0286,  ..., 0.0340, 0.0135, 0.0274],\n",
      "          [0.0202, 0.0192, 0.0186,  ..., 0.0347, 0.0209, 0.0125],\n",
      "          [0.0106, 0.0084, 0.0093,  ..., 0.0216, 0.0190, 0.0361],\n",
      "          ...,\n",
      "          [0.0127, 0.0120, 0.0165,  ..., 0.0344, 0.0283, 0.0279],\n",
      "          [0.0088, 0.0172, 0.0128,  ..., 0.0183, 0.0132, 0.0380],\n",
      "          [0.0193, 0.0194, 0.0101,  ..., 0.0244, 0.0264, 0.0175]],\n",
      "\n",
      "         [[0.0248, 0.0094, 0.0156,  ..., 0.0189, 0.0332, 0.0107],\n",
      "          [0.0159, 0.0122, 0.0099,  ..., 0.0310, 0.0211, 0.0205],\n",
      "          [0.0186, 0.0114, 0.0122,  ..., 0.0192, 0.0235, 0.0354],\n",
      "          ...,\n",
      "          [0.0108, 0.0111, 0.0277,  ..., 0.0209, 0.0188, 0.0252],\n",
      "          [0.0139, 0.0201, 0.0183,  ..., 0.0257, 0.0349, 0.0402],\n",
      "          [0.0180, 0.0122, 0.0411,  ..., 0.0133, 0.0265, 0.0138]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0087, 0.0130, 0.0169,  ..., 0.0128, 0.0203, 0.0137],\n",
      "          [0.0271, 0.0123, 0.0200,  ..., 0.0135, 0.0141, 0.0112],\n",
      "          [0.0178, 0.0097, 0.0124,  ..., 0.0103, 0.0244, 0.0286],\n",
      "          ...,\n",
      "          [0.0236, 0.0329, 0.0138,  ..., 0.0140, 0.0074, 0.0262],\n",
      "          [0.0125, 0.0359, 0.0340,  ..., 0.0200, 0.0043, 0.0184],\n",
      "          [0.0259, 0.0400, 0.0175,  ..., 0.0144, 0.0055, 0.0249]],\n",
      "\n",
      "         [[0.0094, 0.0085, 0.0129,  ..., 0.0147, 0.0204, 0.0210],\n",
      "          [0.0093, 0.0438, 0.0443,  ..., 0.0216, 0.0189, 0.0203],\n",
      "          [0.0224, 0.0245, 0.0449,  ..., 0.0150, 0.0429, 0.0225],\n",
      "          ...,\n",
      "          [0.0161, 0.0218, 0.0146,  ..., 0.0220, 0.0247, 0.0343],\n",
      "          [0.0279, 0.0181, 0.0143,  ..., 0.0197, 0.0264, 0.0288],\n",
      "          [0.0212, 0.0391, 0.0276,  ..., 0.0230, 0.0156, 0.0207]],\n",
      "\n",
      "         [[0.0348, 0.0164, 0.0154,  ..., 0.0210, 0.0213, 0.0268],\n",
      "          [0.0287, 0.0242, 0.0040,  ..., 0.0521, 0.0253, 0.0159],\n",
      "          [0.0261, 0.0100, 0.0097,  ..., 0.0333, 0.0253, 0.0502],\n",
      "          ...,\n",
      "          [0.0135, 0.0183, 0.0125,  ..., 0.0138, 0.0143, 0.0208],\n",
      "          [0.0168, 0.0103, 0.0077,  ..., 0.0128, 0.0063, 0.0126],\n",
      "          [0.0188, 0.0203, 0.0382,  ..., 0.0058, 0.0080, 0.0170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0114, 0.0171, 0.0219,  ..., 0.0040, 0.0164, 0.0097],\n",
      "          [0.0112, 0.0195, 0.0047,  ..., 0.0065, 0.0053, 0.0102],\n",
      "          [0.0091, 0.0239, 0.0139,  ..., 0.0108, 0.0055, 0.0136],\n",
      "          ...,\n",
      "          [0.0154, 0.0234, 0.0338,  ..., 0.0084, 0.0195, 0.0119],\n",
      "          [0.0234, 0.0309, 0.0266,  ..., 0.0110, 0.0102, 0.0129],\n",
      "          [0.0099, 0.0217, 0.0182,  ..., 0.0118, 0.0091, 0.0145]],\n",
      "\n",
      "         [[0.0161, 0.0076, 0.0116,  ..., 0.0162, 0.0200, 0.0148],\n",
      "          [0.0275, 0.0030, 0.0225,  ..., 0.0209, 0.0213, 0.0129],\n",
      "          [0.0126, 0.0116, 0.0165,  ..., 0.0138, 0.0399, 0.0156],\n",
      "          ...,\n",
      "          [0.0185, 0.0159, 0.0281,  ..., 0.0132, 0.0327, 0.0165],\n",
      "          [0.0321, 0.0070, 0.0281,  ..., 0.0146, 0.0217, 0.0132],\n",
      "          [0.0341, 0.0067, 0.0167,  ..., 0.0439, 0.0516, 0.0202]],\n",
      "\n",
      "         [[0.0110, 0.0119, 0.0151,  ..., 0.0328, 0.0214, 0.0189],\n",
      "          [0.0593, 0.0249, 0.0173,  ..., 0.0189, 0.0122, 0.0125],\n",
      "          [0.0231, 0.0179, 0.0214,  ..., 0.0288, 0.0354, 0.0311],\n",
      "          ...,\n",
      "          [0.0310, 0.0179, 0.0105,  ..., 0.0422, 0.0260, 0.0106],\n",
      "          [0.0324, 0.0118, 0.0121,  ..., 0.0275, 0.0133, 0.0141],\n",
      "          [0.0545, 0.0176, 0.0136,  ..., 0.0200, 0.0143, 0.0124]]],\n",
      "\n",
      "\n",
      "        [[[0.0242, 0.0194, 0.0152,  ..., 0.0117, 0.0193, 0.0121],\n",
      "          [0.0139, 0.0142, 0.0100,  ..., 0.0138, 0.0182, 0.0306],\n",
      "          [0.0249, 0.0172, 0.0119,  ..., 0.0057, 0.0197, 0.0179],\n",
      "          ...,\n",
      "          [0.0205, 0.0270, 0.0202,  ..., 0.0131, 0.0275, 0.0283],\n",
      "          [0.0277, 0.0185, 0.0171,  ..., 0.0099, 0.0226, 0.0153],\n",
      "          [0.0203, 0.0163, 0.0340,  ..., 0.0085, 0.0136, 0.0133]],\n",
      "\n",
      "         [[0.0200, 0.0304, 0.0284,  ..., 0.0209, 0.0100, 0.0104],\n",
      "          [0.0207, 0.0280, 0.0390,  ..., 0.0162, 0.0185, 0.0199],\n",
      "          [0.0340, 0.0118, 0.0241,  ..., 0.0488, 0.0148, 0.0255],\n",
      "          ...,\n",
      "          [0.0298, 0.0147, 0.0159,  ..., 0.0203, 0.0178, 0.0184],\n",
      "          [0.0312, 0.0368, 0.0386,  ..., 0.0611, 0.0210, 0.0195],\n",
      "          [0.0116, 0.0177, 0.0203,  ..., 0.0141, 0.0133, 0.0206]],\n",
      "\n",
      "         [[0.0223, 0.0257, 0.0099,  ..., 0.0185, 0.0257, 0.0130],\n",
      "          [0.0235, 0.0158, 0.0240,  ..., 0.0083, 0.0412, 0.0191],\n",
      "          [0.0215, 0.0232, 0.0131,  ..., 0.0165, 0.0263, 0.0211],\n",
      "          ...,\n",
      "          [0.0235, 0.0192, 0.0076,  ..., 0.0160, 0.0254, 0.0206],\n",
      "          [0.0224, 0.0332, 0.0071,  ..., 0.0071, 0.0167, 0.0171],\n",
      "          [0.0145, 0.0267, 0.0182,  ..., 0.0103, 0.0323, 0.0152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0089, 0.0137, 0.0130,  ..., 0.0216, 0.0185, 0.0156],\n",
      "          [0.0121, 0.0170, 0.0323,  ..., 0.0173, 0.0317, 0.0226],\n",
      "          [0.0074, 0.0174, 0.0164,  ..., 0.0268, 0.0293, 0.0135],\n",
      "          ...,\n",
      "          [0.0151, 0.0676, 0.0171,  ..., 0.0119, 0.0129, 0.0123],\n",
      "          [0.0179, 0.0346, 0.0295,  ..., 0.0137, 0.0200, 0.0220],\n",
      "          [0.0087, 0.0179, 0.0199,  ..., 0.0156, 0.0176, 0.0282]],\n",
      "\n",
      "         [[0.0119, 0.0085, 0.0181,  ..., 0.0118, 0.0233, 0.0196],\n",
      "          [0.0135, 0.0112, 0.0339,  ..., 0.0178, 0.0224, 0.0127],\n",
      "          [0.0150, 0.0118, 0.0132,  ..., 0.0221, 0.0344, 0.0538],\n",
      "          ...,\n",
      "          [0.0148, 0.0115, 0.0102,  ..., 0.0194, 0.0223, 0.0268],\n",
      "          [0.0204, 0.0067, 0.0163,  ..., 0.0244, 0.0270, 0.0246],\n",
      "          [0.0198, 0.0103, 0.0183,  ..., 0.0403, 0.0284, 0.0136]],\n",
      "\n",
      "         [[0.0108, 0.0402, 0.0216,  ..., 0.0048, 0.0131, 0.0093],\n",
      "          [0.0119, 0.0221, 0.0146,  ..., 0.0237, 0.0281, 0.0320],\n",
      "          [0.0221, 0.0203, 0.0345,  ..., 0.0444, 0.0307, 0.0202],\n",
      "          ...,\n",
      "          [0.0219, 0.0229, 0.0388,  ..., 0.0374, 0.0110, 0.0198],\n",
      "          [0.0234, 0.0373, 0.0155,  ..., 0.0289, 0.0215, 0.0124],\n",
      "          [0.0099, 0.0205, 0.0306,  ..., 0.0160, 0.0189, 0.0161]]],\n",
      "\n",
      "\n",
      "        [[[0.0135, 0.0112, 0.0142,  ..., 0.0143, 0.0189, 0.0336],\n",
      "          [0.0133, 0.0120, 0.0151,  ..., 0.0265, 0.0143, 0.0205],\n",
      "          [0.0162, 0.0185, 0.0216,  ..., 0.0354, 0.0240, 0.0167],\n",
      "          ...,\n",
      "          [0.0100, 0.0091, 0.0096,  ..., 0.0081, 0.0109, 0.0190],\n",
      "          [0.0252, 0.0195, 0.0192,  ..., 0.0276, 0.0169, 0.0200],\n",
      "          [0.0221, 0.0176, 0.0217,  ..., 0.0102, 0.0168, 0.0355]],\n",
      "\n",
      "         [[0.0229, 0.0259, 0.0348,  ..., 0.0220, 0.0208, 0.0125],\n",
      "          [0.0265, 0.0311, 0.0262,  ..., 0.0285, 0.0252, 0.0216],\n",
      "          [0.0308, 0.0237, 0.0477,  ..., 0.0290, 0.0324, 0.0270],\n",
      "          ...,\n",
      "          [0.0133, 0.0134, 0.0172,  ..., 0.0218, 0.0110, 0.0211],\n",
      "          [0.0174, 0.0200, 0.0170,  ..., 0.0283, 0.0154, 0.0172],\n",
      "          [0.0142, 0.0239, 0.0183,  ..., 0.0176, 0.0148, 0.0228]],\n",
      "\n",
      "         [[0.0169, 0.0240, 0.0273,  ..., 0.0181, 0.0186, 0.0423],\n",
      "          [0.0165, 0.0270, 0.0242,  ..., 0.0075, 0.0204, 0.0258],\n",
      "          [0.0116, 0.0239, 0.0145,  ..., 0.0195, 0.0281, 0.0323],\n",
      "          ...,\n",
      "          [0.0132, 0.0295, 0.0219,  ..., 0.0236, 0.0144, 0.0143],\n",
      "          [0.0064, 0.0134, 0.0191,  ..., 0.0206, 0.0187, 0.0219],\n",
      "          [0.0200, 0.0406, 0.0385,  ..., 0.0251, 0.0151, 0.0105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0118, 0.0176, 0.0114,  ..., 0.0128, 0.0161, 0.0364],\n",
      "          [0.0142, 0.0153, 0.0251,  ..., 0.0095, 0.0224, 0.0293],\n",
      "          [0.0332, 0.0258, 0.0295,  ..., 0.0109, 0.0231, 0.0207],\n",
      "          ...,\n",
      "          [0.0113, 0.0130, 0.0072,  ..., 0.0121, 0.0107, 0.0384],\n",
      "          [0.0087, 0.0112, 0.0245,  ..., 0.0135, 0.0224, 0.0477],\n",
      "          [0.0179, 0.0201, 0.0148,  ..., 0.0174, 0.0343, 0.0374]],\n",
      "\n",
      "         [[0.0125, 0.0128, 0.0116,  ..., 0.0088, 0.0158, 0.0151],\n",
      "          [0.0127, 0.0204, 0.0157,  ..., 0.0115, 0.0111, 0.0120],\n",
      "          [0.0112, 0.0110, 0.0235,  ..., 0.0037, 0.0132, 0.0094],\n",
      "          ...,\n",
      "          [0.0108, 0.0103, 0.0145,  ..., 0.0251, 0.0212, 0.0272],\n",
      "          [0.0076, 0.0099, 0.0085,  ..., 0.0173, 0.0400, 0.0348],\n",
      "          [0.0081, 0.0110, 0.0255,  ..., 0.0261, 0.0135, 0.0345]],\n",
      "\n",
      "         [[0.0313, 0.0157, 0.0098,  ..., 0.0201, 0.0335, 0.0134],\n",
      "          [0.0140, 0.0214, 0.0156,  ..., 0.0234, 0.0168, 0.0061],\n",
      "          [0.0228, 0.0116, 0.0084,  ..., 0.0140, 0.0238, 0.0126],\n",
      "          ...,\n",
      "          [0.0116, 0.0155, 0.0118,  ..., 0.0213, 0.0132, 0.0314],\n",
      "          [0.0228, 0.0161, 0.0104,  ..., 0.0151, 0.0257, 0.0077],\n",
      "          [0.0200, 0.0149, 0.0100,  ..., 0.0218, 0.0232, 0.0110]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-2.4090e-01, -1.5315e-01, -5.7065e-01,  ...,  1.4542e-02,\n",
      "           -2.0302e-02, -3.5509e-01],\n",
      "          [-3.4927e-01, -1.4877e-01, -6.5870e-01,  ..., -1.0107e-01,\n",
      "           -1.8227e-02, -2.9727e-01],\n",
      "          [-3.3238e-01, -1.4190e-01, -6.8181e-01,  ..., -1.8562e-01,\n",
      "           -8.4850e-02, -2.2513e-01],\n",
      "          ...,\n",
      "          [-3.5993e-01, -3.1820e-01, -7.4843e-01,  ..., -1.3314e-01,\n",
      "            7.2016e-02, -2.9883e-01],\n",
      "          [-3.2962e-01, -1.6897e-01, -6.7369e-01,  ..., -1.1551e-01,\n",
      "            6.4341e-02, -3.0525e-01],\n",
      "          [-3.0362e-01, -2.1198e-01, -6.7845e-01,  ..., -1.4262e-01,\n",
      "            7.0629e-02, -2.7738e-01]],\n",
      "\n",
      "         [[-4.2497e-01,  3.2842e-01,  6.5800e-02,  ...,  4.9763e-01,\n",
      "            1.3507e-01,  4.0124e-01],\n",
      "          [-4.8631e-01,  3.6572e-01,  4.8753e-02,  ...,  5.1702e-01,\n",
      "            1.3218e-01,  4.5098e-01],\n",
      "          [-4.7962e-01,  3.5612e-01,  2.6960e-02,  ...,  4.7405e-01,\n",
      "            1.8971e-01,  3.9735e-01],\n",
      "          ...,\n",
      "          [-4.5185e-01,  3.2962e-01,  4.6964e-02,  ...,  5.2459e-01,\n",
      "            1.8521e-01,  4.5634e-01],\n",
      "          [-4.6204e-01,  3.4558e-01,  9.7061e-02,  ...,  5.3085e-01,\n",
      "            8.7778e-02,  4.0330e-01],\n",
      "          [-4.4421e-01,  3.7631e-01,  7.3935e-02,  ...,  5.6549e-01,\n",
      "            9.3077e-02,  3.9791e-01]],\n",
      "\n",
      "         [[-2.6570e-01, -1.3980e-01,  2.7617e-01,  ...,  3.6211e-02,\n",
      "           -1.2374e-01, -4.8045e-01],\n",
      "          [-2.5065e-01, -1.6121e-01,  2.3985e-01,  ...,  6.0658e-03,\n",
      "           -1.3733e-01, -4.6047e-01],\n",
      "          [-2.4368e-01, -9.8304e-02,  2.1312e-01,  ..., -2.4484e-02,\n",
      "           -1.3258e-01, -3.5081e-01],\n",
      "          ...,\n",
      "          [-2.7473e-01, -1.7750e-01,  3.1003e-01,  ...,  3.7134e-03,\n",
      "           -1.4038e-01, -3.9749e-01],\n",
      "          [-2.0001e-01, -1.4905e-01,  2.9203e-01,  ..., -6.1602e-03,\n",
      "           -9.9161e-02, -3.9893e-01],\n",
      "          [-2.0943e-01, -1.6530e-01,  2.8890e-01,  ..., -5.1096e-02,\n",
      "           -7.0316e-02, -3.9035e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6885e-01,  6.5648e-01, -9.7058e-01,  ..., -4.8698e-02,\n",
      "            2.2409e-01, -1.8582e-01],\n",
      "          [ 4.7850e-01,  7.3184e-01, -9.9094e-01,  ..., -4.1575e-02,\n",
      "            2.4621e-01, -1.5945e-01],\n",
      "          [ 4.2798e-01,  6.7221e-01, -1.0482e+00,  ..., -3.2413e-02,\n",
      "            2.1337e-01, -1.2705e-01],\n",
      "          ...,\n",
      "          [ 5.3307e-01,  5.8713e-01, -9.8963e-01,  ...,  8.3710e-03,\n",
      "            3.2094e-01, -1.5582e-01],\n",
      "          [ 4.7079e-01,  5.6571e-01, -1.0314e+00,  ..., -9.0176e-03,\n",
      "            2.8620e-01, -9.4260e-02],\n",
      "          [ 4.5400e-01,  6.0255e-01, -9.8610e-01,  ..., -7.0240e-02,\n",
      "            2.8861e-01, -6.9521e-02]],\n",
      "\n",
      "         [[-6.1860e-02, -6.0105e-01, -4.1061e-01,  ...,  2.3867e-02,\n",
      "           -4.4326e-01,  2.1517e-01],\n",
      "          [ 3.1960e-02, -5.9353e-01, -4.3108e-01,  ...,  4.4366e-02,\n",
      "           -3.7757e-01,  1.6235e-01],\n",
      "          [-1.2228e-01, -5.2547e-01, -4.4627e-01,  ...,  3.8165e-02,\n",
      "           -3.5461e-01,  1.8950e-01],\n",
      "          ...,\n",
      "          [-1.0303e-01, -5.3449e-01, -3.8932e-01,  ...,  9.6201e-02,\n",
      "           -3.7570e-01,  2.5035e-01],\n",
      "          [-9.6297e-02, -5.1644e-01, -3.8733e-01,  ...,  1.1074e-01,\n",
      "           -4.0793e-01,  3.3244e-01],\n",
      "          [-1.0381e-01, -5.3343e-01, -4.5038e-01,  ...,  3.4868e-02,\n",
      "           -4.3473e-01,  2.5630e-01]],\n",
      "\n",
      "         [[ 1.5380e-01,  4.4496e-01,  1.8588e-01,  ...,  3.9202e-01,\n",
      "           -2.0005e-01,  1.2860e-02],\n",
      "          [ 1.5588e-01,  3.7456e-01,  1.8902e-01,  ...,  4.4563e-01,\n",
      "           -1.7250e-01, -6.4513e-02],\n",
      "          [ 1.7075e-01,  4.0709e-01,  2.6370e-01,  ...,  2.8726e-01,\n",
      "           -2.1310e-01, -4.6202e-02],\n",
      "          ...,\n",
      "          [ 2.0524e-01,  3.9155e-01,  1.6299e-01,  ...,  3.4340e-01,\n",
      "           -1.7936e-01, -5.6481e-02],\n",
      "          [ 1.2994e-01,  3.3545e-01,  2.2475e-01,  ...,  3.6519e-01,\n",
      "           -1.5075e-01, -6.0087e-02],\n",
      "          [ 1.7525e-01,  4.8752e-01,  2.0844e-01,  ...,  2.8721e-01,\n",
      "           -2.7872e-01, -1.4966e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.7696e-01, -1.6746e-01, -6.7485e-01,  ..., -1.2568e-01,\n",
      "            1.1981e-01, -3.8928e-02],\n",
      "          [-4.4601e-01, -1.5578e-01, -6.9551e-01,  ..., -8.1298e-02,\n",
      "            8.3200e-02, -7.7774e-02],\n",
      "          [-4.7628e-01, -1.7527e-01, -6.9992e-01,  ..., -8.3375e-02,\n",
      "            1.2705e-01, -1.1888e-01],\n",
      "          ...,\n",
      "          [-4.0052e-01, -1.7178e-01, -7.1712e-01,  ..., -1.5332e-01,\n",
      "            1.7403e-01, -1.3695e-01],\n",
      "          [-3.7689e-01, -1.8253e-01, -6.4429e-01,  ..., -1.2244e-01,\n",
      "            1.3516e-01, -1.7494e-01],\n",
      "          [-3.9546e-01, -2.0787e-01, -6.5349e-01,  ..., -1.2955e-01,\n",
      "            1.3270e-01, -1.3180e-01]],\n",
      "\n",
      "         [[-2.9272e-01,  3.4315e-01,  2.9182e-02,  ...,  6.9410e-01,\n",
      "            1.6741e-01,  4.2581e-01],\n",
      "          [-2.5990e-01,  3.5262e-01,  9.0583e-03,  ...,  7.7531e-01,\n",
      "            1.8051e-01,  3.7526e-01],\n",
      "          [-2.6985e-01,  2.5593e-01, -1.4976e-02,  ...,  7.0821e-01,\n",
      "            1.6490e-01,  4.6015e-01],\n",
      "          ...,\n",
      "          [-3.0424e-01,  2.7379e-01,  1.3150e-02,  ...,  6.8978e-01,\n",
      "            1.6976e-01,  4.9732e-01],\n",
      "          [-2.8977e-01,  2.7471e-01,  2.4185e-02,  ...,  6.8771e-01,\n",
      "            1.4639e-01,  3.5912e-01],\n",
      "          [-2.7429e-01,  3.0083e-01,  5.9920e-03,  ...,  7.4983e-01,\n",
      "            1.4795e-01,  4.1700e-01]],\n",
      "\n",
      "         [[-1.7159e-01, -7.3632e-02,  3.2950e-02,  ...,  1.1357e-01,\n",
      "           -2.8652e-01, -4.9090e-01],\n",
      "          [-1.1634e-01, -1.0473e-01,  1.0016e-02,  ...,  1.3423e-01,\n",
      "           -3.4936e-01, -4.9512e-01],\n",
      "          [-2.2769e-01, -4.5911e-02,  6.5052e-02,  ...,  1.3217e-01,\n",
      "           -3.1550e-01, -5.2934e-01],\n",
      "          ...,\n",
      "          [-1.2286e-01, -5.0162e-02,  7.5241e-02,  ...,  1.3119e-01,\n",
      "           -2.7158e-01, -5.0718e-01],\n",
      "          [-1.6336e-01, -2.9249e-02,  1.1331e-01,  ...,  1.4466e-01,\n",
      "           -3.3314e-01, -4.9714e-01],\n",
      "          [-1.4266e-01,  9.8489e-03,  1.1625e-01,  ...,  1.6203e-01,\n",
      "           -2.7447e-01, -5.3574e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1782e-01,  6.6592e-01, -8.7439e-01,  ..., -7.7522e-02,\n",
      "            3.1769e-01, -1.5115e-01],\n",
      "          [ 4.8197e-01,  6.7285e-01, -8.9195e-01,  ..., -1.0219e-01,\n",
      "            2.7544e-01, -1.6182e-01],\n",
      "          [ 5.2557e-01,  6.7722e-01, -8.8367e-01,  ..., -1.2162e-01,\n",
      "            3.3852e-01, -1.2740e-01],\n",
      "          ...,\n",
      "          [ 4.8905e-01,  6.6677e-01, -8.5347e-01,  ..., -9.3852e-02,\n",
      "            3.0756e-01, -1.5239e-01],\n",
      "          [ 5.2098e-01,  6.4046e-01, -8.2059e-01,  ..., -8.1094e-03,\n",
      "            2.3208e-01, -1.6878e-01],\n",
      "          [ 5.6448e-01,  6.1965e-01, -8.4908e-01,  ..., -1.5268e-01,\n",
      "            2.6083e-01, -2.5474e-01]],\n",
      "\n",
      "         [[-9.6152e-02, -6.3090e-01, -4.1113e-01,  ..., -4.9765e-02,\n",
      "           -4.0994e-01,  3.7421e-01],\n",
      "          [-8.3098e-02, -6.0004e-01, -4.0580e-01,  ...,  3.5059e-02,\n",
      "           -2.6629e-01,  3.3932e-01],\n",
      "          [-8.3345e-02, -5.8205e-01, -3.4271e-01,  ..., -3.0370e-02,\n",
      "           -3.8244e-01,  3.6427e-01],\n",
      "          ...,\n",
      "          [-4.4438e-02, -6.2178e-01, -3.7578e-01,  ..., -2.0641e-02,\n",
      "           -3.1555e-01,  2.7442e-01],\n",
      "          [-1.1374e-01, -6.0999e-01, -3.7406e-01,  ..., -5.7061e-02,\n",
      "           -3.5186e-01,  2.9978e-01],\n",
      "          [-7.4325e-02, -5.8592e-01, -3.8769e-01,  ..., -1.8915e-02,\n",
      "           -2.9733e-01,  3.1651e-01]],\n",
      "\n",
      "         [[ 3.1059e-01,  3.1913e-01,  1.0064e-01,  ...,  4.7588e-01,\n",
      "           -2.8036e-01, -3.6351e-02],\n",
      "          [ 3.2746e-01,  3.4656e-01,  8.3204e-02,  ...,  4.7656e-01,\n",
      "           -2.5343e-01, -2.7384e-02],\n",
      "          [ 2.8343e-01,  3.3251e-01,  8.4249e-02,  ...,  4.3368e-01,\n",
      "           -1.7465e-01, -1.3996e-01],\n",
      "          ...,\n",
      "          [ 3.2169e-01,  3.1024e-01,  3.7648e-02,  ...,  4.9714e-01,\n",
      "           -2.8664e-01, -6.6704e-02],\n",
      "          [ 2.8025e-01,  3.2376e-01,  1.2850e-01,  ...,  4.1104e-01,\n",
      "           -2.6906e-01, -1.1607e-03],\n",
      "          [ 2.3585e-01,  3.3385e-01,  7.1827e-02,  ...,  4.8217e-01,\n",
      "           -2.1726e-01,  2.5538e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.4290e-01, -2.3649e-01, -7.2390e-01,  ..., -2.0562e-01,\n",
      "           -1.4994e-02, -6.3963e-02],\n",
      "          [-3.6078e-01, -2.1716e-01, -7.1029e-01,  ..., -1.7356e-01,\n",
      "            5.6519e-02, -7.5941e-02],\n",
      "          [-3.4933e-01, -2.2267e-01, -7.0201e-01,  ..., -2.2657e-01,\n",
      "            2.0303e-03, -3.7403e-02],\n",
      "          ...,\n",
      "          [-3.8882e-01, -2.3834e-01, -6.9541e-01,  ..., -1.6555e-01,\n",
      "            9.8784e-02, -5.0291e-02],\n",
      "          [-3.6770e-01, -2.1923e-01, -6.5304e-01,  ..., -2.2312e-01,\n",
      "            7.0367e-02,  1.9718e-02],\n",
      "          [-4.3197e-01, -2.8776e-01, -6.6735e-01,  ..., -1.7174e-01,\n",
      "            2.0512e-01, -7.2520e-02]],\n",
      "\n",
      "         [[-3.0476e-01,  2.4917e-01, -5.2734e-02,  ...,  5.5916e-01,\n",
      "            2.4867e-01,  4.4917e-01],\n",
      "          [-2.3989e-01,  4.0787e-01,  1.7988e-02,  ...,  4.3673e-01,\n",
      "            2.3330e-01,  4.4337e-01],\n",
      "          [-2.6329e-01,  2.5795e-01, -5.9556e-02,  ...,  5.8986e-01,\n",
      "            2.4159e-01,  4.4934e-01],\n",
      "          ...,\n",
      "          [-2.9183e-01,  1.0531e-01, -1.2460e-01,  ...,  6.2758e-01,\n",
      "            2.5962e-01,  4.5742e-01],\n",
      "          [-2.6241e-01,  2.3127e-01, -5.2314e-02,  ...,  5.3980e-01,\n",
      "            3.0896e-01,  4.4941e-01],\n",
      "          [-3.1196e-01,  1.9590e-01, -5.1943e-02,  ...,  5.6253e-01,\n",
      "            2.9405e-01,  4.4954e-01]],\n",
      "\n",
      "         [[-2.2447e-01,  1.1165e-01,  2.8784e-01,  ...,  2.4070e-01,\n",
      "           -2.1294e-01, -4.9109e-01],\n",
      "          [-2.0287e-01,  6.3423e-02,  2.5288e-01,  ...,  1.8564e-01,\n",
      "           -2.0559e-01, -5.1553e-01],\n",
      "          [-2.2108e-01,  1.0244e-01,  2.6873e-01,  ...,  2.0356e-01,\n",
      "           -2.1697e-01, -4.7370e-01],\n",
      "          ...,\n",
      "          [-2.0848e-01,  5.1621e-02,  3.0370e-01,  ...,  1.8416e-01,\n",
      "           -1.8198e-01, -5.1019e-01],\n",
      "          [-2.4878e-01,  8.8696e-02,  3.5852e-01,  ...,  2.0235e-01,\n",
      "           -2.0229e-01, -5.0104e-01],\n",
      "          [-2.5032e-01,  3.0580e-02,  3.1746e-01,  ...,  2.1686e-01,\n",
      "           -2.1205e-01, -4.1202e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4254e-01,  7.1286e-01, -7.2733e-01,  ..., -8.6315e-02,\n",
      "            1.6682e-01, -1.7479e-01],\n",
      "          [ 4.1669e-01,  6.2705e-01, -7.1418e-01,  ..., -1.2150e-01,\n",
      "            1.1800e-01, -1.7591e-01],\n",
      "          [ 5.3432e-01,  7.0728e-01, -7.3025e-01,  ..., -1.0749e-01,\n",
      "            1.3755e-01, -1.5576e-01],\n",
      "          ...,\n",
      "          [ 4.6432e-01,  6.3659e-01, -6.9976e-01,  ..., -9.9655e-02,\n",
      "            1.2004e-01, -1.6560e-01],\n",
      "          [ 5.2300e-01,  7.0095e-01, -6.7160e-01,  ..., -1.2523e-01,\n",
      "            1.6717e-01, -1.8418e-01],\n",
      "          [ 5.3330e-01,  6.8148e-01, -7.0391e-01,  ..., -9.8388e-02,\n",
      "            1.5451e-01, -1.4206e-01]],\n",
      "\n",
      "         [[ 2.6549e-01, -4.4000e-01, -3.7185e-01,  ..., -8.9020e-03,\n",
      "           -3.4187e-01,  3.4684e-01],\n",
      "          [ 1.9017e-01, -4.5783e-01, -3.4428e-01,  ..., -9.5423e-03,\n",
      "           -3.5514e-01,  3.1534e-01],\n",
      "          [ 2.0279e-01, -4.5390e-01, -4.5679e-01,  ..., -4.3373e-02,\n",
      "           -3.5603e-01,  3.6990e-01],\n",
      "          ...,\n",
      "          [ 2.4748e-01, -4.2420e-01, -4.3404e-01,  ..., -7.5000e-02,\n",
      "           -3.6406e-01,  3.8214e-01],\n",
      "          [ 1.9036e-01, -4.3970e-01, -4.4391e-01,  ..., -8.2964e-02,\n",
      "           -3.2214e-01,  2.5856e-01],\n",
      "          [ 2.5411e-01, -4.0094e-01, -3.8543e-01,  ..., -6.1092e-02,\n",
      "           -3.1041e-01,  3.2251e-01]],\n",
      "\n",
      "         [[ 3.7537e-01,  3.3705e-01, -5.2401e-02,  ...,  4.3477e-01,\n",
      "           -2.2282e-01,  2.1277e-03],\n",
      "          [ 3.8529e-01,  3.4000e-01, -2.2187e-02,  ...,  3.9044e-01,\n",
      "           -2.6381e-01,  3.0821e-02],\n",
      "          [ 3.5658e-01,  3.6625e-01, -1.3980e-02,  ...,  4.4068e-01,\n",
      "           -2.4841e-01,  8.4467e-02],\n",
      "          ...,\n",
      "          [ 3.3849e-01,  3.2539e-01, -1.3455e-01,  ...,  4.6400e-01,\n",
      "           -2.6326e-01,  4.8105e-02],\n",
      "          [ 3.6251e-01,  3.1621e-01, -4.0335e-02,  ...,  4.2358e-01,\n",
      "           -2.3087e-01,  4.6706e-02],\n",
      "          [ 3.2395e-01,  3.0854e-01,  3.0982e-02,  ...,  3.9175e-01,\n",
      "           -2.3939e-01, -1.0041e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.6573e-01, -1.8963e-01, -7.3018e-01,  ..., -2.2825e-01,\n",
      "            8.0602e-02, -4.8978e-02],\n",
      "          [-4.5197e-01, -1.9586e-01, -7.6640e-01,  ..., -1.9981e-01,\n",
      "            1.7030e-01, -6.3970e-02],\n",
      "          [-4.0758e-01, -2.8044e-01, -7.4708e-01,  ..., -2.5875e-01,\n",
      "            1.1573e-01, -8.5825e-02],\n",
      "          ...,\n",
      "          [-3.4093e-01, -2.8067e-01, -7.4267e-01,  ..., -2.6890e-01,\n",
      "            1.4337e-01, -1.2693e-01],\n",
      "          [-3.3754e-01, -2.4098e-01, -7.8893e-01,  ..., -2.7439e-01,\n",
      "            2.0381e-01, -1.5640e-01],\n",
      "          [-3.4657e-01, -2.2675e-01, -7.5333e-01,  ..., -2.5709e-01,\n",
      "            1.8832e-01, -1.5209e-01]],\n",
      "\n",
      "         [[-1.3124e-01,  1.8968e-01, -1.3091e-01,  ...,  5.9970e-01,\n",
      "            2.6624e-01,  4.7137e-01],\n",
      "          [-1.6413e-01,  2.4777e-01, -3.9957e-02,  ...,  5.9897e-01,\n",
      "            2.5488e-01,  4.2068e-01],\n",
      "          [-1.6752e-01,  2.7547e-01, -1.0430e-01,  ...,  6.6127e-01,\n",
      "            3.0604e-01,  3.5819e-01],\n",
      "          ...,\n",
      "          [-1.3799e-01,  2.4404e-01, -1.0451e-01,  ...,  6.5517e-01,\n",
      "            2.5436e-01,  3.9913e-01],\n",
      "          [-2.1131e-01,  2.7209e-01, -1.1855e-01,  ...,  6.6004e-01,\n",
      "            2.6298e-01,  4.2595e-01],\n",
      "          [-2.0561e-01,  2.1741e-01, -6.9349e-02,  ...,  6.6566e-01,\n",
      "            2.1191e-01,  3.9104e-01]],\n",
      "\n",
      "         [[-2.6702e-01,  8.7532e-02,  9.1806e-02,  ...,  2.9385e-01,\n",
      "           -1.0180e-01, -4.9446e-01],\n",
      "          [-2.5383e-01,  1.2327e-01,  1.0214e-01,  ...,  2.9960e-01,\n",
      "           -7.6033e-02, -5.3938e-01],\n",
      "          [-2.2064e-01,  2.0073e-02,  1.0820e-01,  ...,  2.1182e-01,\n",
      "           -6.6067e-02, -5.5107e-01],\n",
      "          ...,\n",
      "          [-2.1410e-01,  9.6909e-02,  1.2601e-01,  ...,  3.8446e-01,\n",
      "           -1.0902e-01, -5.1452e-01],\n",
      "          [-2.2270e-01,  6.7335e-02,  1.6774e-01,  ...,  2.8249e-01,\n",
      "           -1.2855e-01, -6.0910e-01],\n",
      "          [-2.2985e-01,  6.1662e-02,  1.5590e-01,  ...,  3.0890e-01,\n",
      "           -1.0389e-01, -5.9023e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.9751e-01,  7.2665e-01, -7.9402e-01,  ..., -1.5433e-01,\n",
      "            2.8657e-01, -1.9612e-01],\n",
      "          [ 5.6473e-01,  7.2518e-01, -7.3509e-01,  ..., -2.0817e-01,\n",
      "            2.8834e-01, -1.7645e-01],\n",
      "          [ 5.3679e-01,  7.1568e-01, -7.6399e-01,  ..., -2.0668e-01,\n",
      "            2.5796e-01, -1.4857e-01],\n",
      "          ...,\n",
      "          [ 5.5867e-01,  6.9220e-01, -7.5750e-01,  ..., -1.4319e-01,\n",
      "            3.2859e-01, -1.8423e-01],\n",
      "          [ 5.8791e-01,  6.6717e-01, -7.2976e-01,  ..., -1.9990e-01,\n",
      "            2.9392e-01, -1.7012e-01],\n",
      "          [ 5.2191e-01,  7.2563e-01, -7.1149e-01,  ..., -2.2298e-01,\n",
      "            2.8274e-01, -1.4865e-01]],\n",
      "\n",
      "         [[ 6.3003e-02, -6.9853e-01, -3.0572e-01,  ..., -8.2210e-02,\n",
      "           -4.7934e-01,  2.0641e-01],\n",
      "          [-6.5429e-03, -6.8142e-01, -2.7306e-01,  ..., -1.3637e-01,\n",
      "           -4.8718e-01,  2.0039e-01],\n",
      "          [ 7.3407e-02, -6.7188e-01, -3.5538e-01,  ..., -8.8577e-02,\n",
      "           -4.1548e-01,  1.8544e-01],\n",
      "          ...,\n",
      "          [ 1.0545e-01, -6.1975e-01, -3.6247e-01,  ..., -4.9576e-02,\n",
      "           -3.6418e-01,  2.1061e-01],\n",
      "          [ 7.2774e-02, -6.7219e-01, -2.6500e-01,  ..., -7.2401e-02,\n",
      "           -4.2956e-01,  1.6442e-01],\n",
      "          [ 9.4898e-02, -6.7876e-01, -2.9502e-01,  ..., -1.3783e-01,\n",
      "           -4.6585e-01,  2.1641e-01]],\n",
      "\n",
      "         [[ 4.7617e-01,  3.0723e-01,  3.4898e-02,  ...,  4.6584e-01,\n",
      "           -1.8294e-01, -7.2283e-02],\n",
      "          [ 4.1694e-01,  3.1783e-01,  5.3745e-02,  ...,  4.6723e-01,\n",
      "           -2.8365e-01, -1.5266e-01],\n",
      "          [ 3.8594e-01,  3.5351e-01, -2.1507e-02,  ...,  4.3648e-01,\n",
      "           -1.8328e-01, -8.8503e-02],\n",
      "          ...,\n",
      "          [ 4.9465e-01,  2.3236e-01, -4.3664e-02,  ...,  4.8416e-01,\n",
      "           -1.9474e-01, -1.1192e-01],\n",
      "          [ 4.7671e-01,  3.4682e-01, -2.4984e-03,  ...,  5.5352e-01,\n",
      "           -2.7279e-01, -9.1354e-02],\n",
      "          [ 4.4675e-01,  2.4564e-01,  6.0686e-02,  ...,  4.7926e-01,\n",
      "           -1.9973e-01, -1.2552e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.8807e-01, -1.6584e-01, -5.3245e-01,  ..., -2.3326e-01,\n",
      "            1.8359e-01, -1.4966e-01],\n",
      "          [-3.7000e-01, -1.8710e-01, -5.3851e-01,  ..., -2.1639e-01,\n",
      "            2.0949e-01, -1.8573e-01],\n",
      "          [-4.4383e-01, -1.5884e-01, -5.7728e-01,  ..., -2.7383e-01,\n",
      "            1.3849e-01, -4.9647e-02],\n",
      "          ...,\n",
      "          [-4.6657e-01, -1.5818e-01, -5.7493e-01,  ..., -3.0793e-01,\n",
      "            1.5166e-01, -8.3056e-02],\n",
      "          [-3.9413e-01, -1.2947e-01, -5.9906e-01,  ..., -2.0789e-01,\n",
      "            3.1700e-01, -1.0943e-01],\n",
      "          [-4.4308e-01, -1.4796e-01, -6.3736e-01,  ..., -2.7824e-01,\n",
      "            2.1625e-01, -9.2266e-02]],\n",
      "\n",
      "         [[-2.7139e-01,  2.8299e-01, -1.8983e-02,  ...,  6.8746e-01,\n",
      "            2.3922e-01,  4.6789e-01],\n",
      "          [-3.4135e-01,  2.0603e-01,  1.6055e-03,  ...,  6.9545e-01,\n",
      "            2.5162e-01,  3.6702e-01],\n",
      "          [-3.1698e-01,  2.1418e-01, -2.6550e-02,  ...,  6.8720e-01,\n",
      "            1.3659e-01,  3.8167e-01],\n",
      "          ...,\n",
      "          [-2.7212e-01,  2.6659e-01, -2.4100e-02,  ...,  6.9605e-01,\n",
      "            1.7458e-01,  3.7861e-01],\n",
      "          [-3.2344e-01,  3.5452e-01,  1.1214e-02,  ...,  5.5157e-01,\n",
      "            1.7526e-01,  3.7996e-01],\n",
      "          [-2.5036e-01,  3.8362e-01, -2.1166e-02,  ...,  6.6102e-01,\n",
      "            1.1197e-01,  4.2197e-01]],\n",
      "\n",
      "         [[ 2.6975e-02,  1.1243e-02,  1.5003e-01,  ...,  1.0526e-02,\n",
      "           -1.4851e-01, -5.2309e-01],\n",
      "          [-1.3974e-02,  3.7489e-02,  2.2804e-01,  ...,  3.4473e-02,\n",
      "           -6.4706e-02, -5.1837e-01],\n",
      "          [-8.7424e-02,  3.5281e-02,  2.2579e-01,  ...,  1.9251e-01,\n",
      "           -1.1767e-01, -4.4084e-01],\n",
      "          ...,\n",
      "          [-1.3431e-02,  2.8227e-02,  3.0485e-01,  ...,  1.1803e-01,\n",
      "           -1.3410e-01, -4.2033e-01],\n",
      "          [-3.5591e-02, -9.9975e-04,  2.6680e-01,  ...,  1.8507e-01,\n",
      "           -1.9092e-01, -4.9639e-01],\n",
      "          [-8.8836e-02,  4.5705e-02,  3.1046e-01,  ...,  2.1531e-01,\n",
      "           -1.3264e-01, -4.3888e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6743e-01,  6.9837e-01, -9.6448e-01,  ..., -4.3298e-03,\n",
      "            1.5025e-01,  6.8146e-02],\n",
      "          [ 4.2080e-01,  7.0230e-01, -9.1684e-01,  ..., -8.7614e-02,\n",
      "            1.8626e-01,  9.1225e-02],\n",
      "          [ 4.3937e-01,  6.4405e-01, -8.8995e-01,  ..., -8.9297e-02,\n",
      "            1.7619e-01,  4.9238e-02],\n",
      "          ...,\n",
      "          [ 4.5949e-01,  6.5542e-01, -8.6784e-01,  ...,  4.7615e-03,\n",
      "            1.6795e-01,  2.5420e-02],\n",
      "          [ 4.5652e-01,  6.9179e-01, -8.5483e-01,  ..., -1.0001e-01,\n",
      "            1.9490e-01,  1.2603e-02],\n",
      "          [ 4.4785e-01,  7.1079e-01, -8.6383e-01,  ..., -9.1997e-02,\n",
      "            1.8158e-01, -2.8048e-02]],\n",
      "\n",
      "         [[ 2.2502e-01, -5.1621e-01, -3.3130e-01,  ..., -1.7909e-03,\n",
      "           -4.7860e-01,  3.7869e-01],\n",
      "          [ 1.6482e-01, -4.3819e-01, -3.8903e-01,  ...,  6.4376e-02,\n",
      "           -5.3356e-01,  3.0273e-01],\n",
      "          [ 1.7573e-01, -5.3459e-01, -2.9820e-01,  ..., -7.4778e-02,\n",
      "           -5.2806e-01,  2.9171e-01],\n",
      "          ...,\n",
      "          [ 1.8934e-01, -5.6497e-01, -2.9147e-01,  ..., -4.3391e-02,\n",
      "           -5.1477e-01,  2.5283e-01],\n",
      "          [ 2.2597e-01, -4.8813e-01, -3.8404e-01,  ...,  3.2117e-02,\n",
      "           -4.5757e-01,  1.9415e-01],\n",
      "          [ 1.8104e-01, -4.9043e-01, -4.0402e-01,  ...,  7.2820e-02,\n",
      "           -5.3130e-01,  2.5569e-01]],\n",
      "\n",
      "         [[ 2.8622e-01,  4.4894e-01, -1.4134e-01,  ...,  5.2818e-01,\n",
      "           -1.8480e-01,  1.1603e-01],\n",
      "          [ 2.5510e-01,  4.5962e-01, -8.0511e-02,  ...,  4.4220e-01,\n",
      "           -1.1446e-01,  8.1311e-02],\n",
      "          [ 2.4487e-01,  4.2300e-01, -1.3290e-01,  ...,  4.5870e-01,\n",
      "           -1.4551e-01, -3.9144e-03],\n",
      "          ...,\n",
      "          [ 2.9571e-01,  3.6660e-01, -7.6425e-02,  ...,  4.6238e-01,\n",
      "           -1.5977e-01,  8.1549e-02],\n",
      "          [ 2.5140e-01,  4.2919e-01, -1.3175e-01,  ...,  4.7506e-01,\n",
      "           -1.8407e-01,  7.9704e-02],\n",
      "          [ 3.1649e-01,  3.8257e-01, -1.4480e-01,  ...,  5.5389e-01,\n",
      "           -1.8932e-01,  7.6836e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.5635e-01, -2.4238e-01, -6.6687e-01,  ..., -1.9201e-01,\n",
      "            1.0007e-01, -8.7939e-02],\n",
      "          [-3.9913e-01, -2.5010e-01, -6.1323e-01,  ..., -1.4360e-01,\n",
      "            1.4284e-01, -1.2900e-01],\n",
      "          [-4.0932e-01, -2.6976e-01, -5.9520e-01,  ..., -1.7856e-01,\n",
      "            2.0684e-02,  3.2023e-04],\n",
      "          ...,\n",
      "          [-3.8404e-01, -2.9050e-01, -7.3586e-01,  ..., -1.4709e-01,\n",
      "            1.6682e-01, -1.8524e-01],\n",
      "          [-3.4810e-01, -2.4188e-01, -6.8430e-01,  ..., -2.2166e-01,\n",
      "            8.9065e-02, -9.0681e-02],\n",
      "          [-3.9293e-01, -2.5389e-01, -6.7036e-01,  ..., -1.0528e-01,\n",
      "            1.5595e-01, -1.3718e-01]],\n",
      "\n",
      "         [[-2.6239e-01,  3.0745e-01,  2.1486e-03,  ...,  5.4065e-01,\n",
      "            8.6521e-02,  3.7615e-01],\n",
      "          [-2.5934e-01,  2.5461e-01, -5.6459e-02,  ...,  6.0207e-01,\n",
      "            8.5551e-02,  3.5982e-01],\n",
      "          [-2.9048e-01,  3.0805e-01, -7.4810e-02,  ...,  4.9506e-01,\n",
      "            5.4295e-02,  3.7192e-01],\n",
      "          ...,\n",
      "          [-2.9282e-01,  2.1627e-01, -2.6638e-03,  ...,  5.6173e-01,\n",
      "            3.7953e-02,  3.7857e-01],\n",
      "          [-3.0307e-01,  3.2342e-01,  1.6516e-02,  ...,  5.5197e-01,\n",
      "            8.8585e-02,  4.0794e-01],\n",
      "          [-2.8171e-01,  2.0904e-01, -5.3947e-02,  ...,  5.5899e-01,\n",
      "            9.0900e-02,  3.6397e-01]],\n",
      "\n",
      "         [[-3.0617e-01,  9.5281e-02,  3.0879e-01,  ...,  9.9902e-02,\n",
      "           -3.7432e-02, -4.6793e-01],\n",
      "          [-3.2589e-01,  8.3518e-02,  2.9843e-01,  ...,  8.2261e-02,\n",
      "           -8.2027e-02, -5.0607e-01],\n",
      "          [-3.2560e-01,  1.0807e-01,  3.3586e-01,  ...,  4.0896e-02,\n",
      "           -5.7098e-02, -4.6172e-01],\n",
      "          ...,\n",
      "          [-3.1808e-01,  8.4509e-02,  3.8670e-01,  ...,  1.5288e-01,\n",
      "           -3.8071e-02, -4.7479e-01],\n",
      "          [-2.7713e-01,  5.4229e-02,  3.8464e-01,  ...,  1.2684e-01,\n",
      "           -4.5973e-02, -4.4385e-01],\n",
      "          [-3.4650e-01,  7.2257e-02,  3.6000e-01,  ...,  1.8006e-01,\n",
      "           -1.1550e-01, -5.3867e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1382e-01,  6.7899e-01, -8.3136e-01,  ..., -8.4927e-02,\n",
      "            9.0114e-02, -1.5437e-01],\n",
      "          [ 4.8904e-01,  6.6592e-01, -8.1875e-01,  ..., -3.9157e-02,\n",
      "            6.2934e-02, -2.3320e-01],\n",
      "          [ 4.4416e-01,  6.9577e-01, -8.1195e-01,  ..., -6.8109e-02,\n",
      "            1.5402e-01, -2.2496e-01],\n",
      "          ...,\n",
      "          [ 5.3481e-01,  6.5692e-01, -8.5360e-01,  ..., -7.5765e-02,\n",
      "            2.0045e-01, -2.6586e-01],\n",
      "          [ 4.7324e-01,  6.6077e-01, -7.5679e-01,  ..., -8.8678e-02,\n",
      "            1.3311e-01, -2.6257e-01],\n",
      "          [ 4.7299e-01,  6.8877e-01, -8.4013e-01,  ..., -9.6825e-02,\n",
      "            1.3572e-01, -2.5582e-01]],\n",
      "\n",
      "         [[ 1.1506e-01, -5.2071e-01, -2.7707e-01,  ..., -6.1954e-02,\n",
      "           -4.3673e-01,  3.9950e-01],\n",
      "          [ 1.6167e-01, -5.5453e-01, -2.9187e-01,  ..., -1.0003e-01,\n",
      "           -4.6662e-01,  3.5273e-01],\n",
      "          [ 8.8376e-02, -5.3937e-01, -2.3818e-01,  ..., -1.0470e-01,\n",
      "           -5.0811e-01,  3.2473e-01],\n",
      "          ...,\n",
      "          [ 6.5755e-02, -5.4028e-01, -3.6870e-01,  ..., -1.0594e-01,\n",
      "           -4.0564e-01,  2.8345e-01],\n",
      "          [ 1.3516e-01, -4.5514e-01, -3.2844e-01,  ..., -1.1477e-01,\n",
      "           -4.9844e-01,  3.3081e-01],\n",
      "          [ 8.5306e-02, -4.7475e-01, -3.1621e-01,  ..., -8.3071e-02,\n",
      "           -4.3715e-01,  3.2759e-01]],\n",
      "\n",
      "         [[ 1.7552e-01,  4.8344e-01, -1.4020e-02,  ...,  4.1879e-01,\n",
      "           -1.1757e-01, -4.0252e-02],\n",
      "          [ 2.7341e-01,  4.0423e-01, -1.0689e-01,  ...,  4.2769e-01,\n",
      "           -1.3742e-01, -7.9231e-02],\n",
      "          [ 1.6918e-01,  4.7749e-01, -8.5170e-03,  ...,  4.3303e-01,\n",
      "           -1.2840e-01, -1.7353e-02],\n",
      "          ...,\n",
      "          [ 2.2767e-01,  4.2774e-01, -1.2952e-02,  ...,  4.5241e-01,\n",
      "           -7.8157e-02, -9.7221e-02],\n",
      "          [ 2.8319e-01,  3.8994e-01, -4.1758e-02,  ...,  4.3777e-01,\n",
      "           -1.5028e-01, -9.7004e-02],\n",
      "          [ 2.5076e-01,  4.6523e-01, -1.5645e-02,  ...,  4.5953e-01,\n",
      "           -1.2429e-01, -4.3718e-02]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
