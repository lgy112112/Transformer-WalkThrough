{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[-0.5780, -1.2434,  1.0627],\n",
      "        [ 0.1772, -0.0171,  1.4353],\n",
      "        [ 1.3848,  0.0874, -0.0961]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征映射到新的特征空间。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[ 0.1329, -0.5956,  0.3338,  ...,  0.3210,  1.3748,  1.6286],\n",
      "         [ 1.4426, -1.2309,  0.3481,  ...,  0.5446,  0.7271,  0.4498],\n",
      "         [-1.1484,  0.0395,  0.3195,  ...,  0.0167,  0.2458,  0.6824],\n",
      "         ...,\n",
      "         [-1.0618,  0.5563,  0.5809,  ..., -0.1002,  1.3667,  0.0230],\n",
      "         [-0.4264,  0.4357, -0.4425,  ..., -1.0612,  0.8376,  1.2828],\n",
      "         [ 0.6882, -0.0167,  0.1640,  ..., -0.4887,  1.1286,  0.4324]],\n",
      "\n",
      "        [[-0.2511, -1.2095, -0.1933,  ...,  0.3575,  0.6016, -0.1305],\n",
      "         [-0.2073, -0.6856,  0.5131,  ...,  0.0320,  0.9685, -0.6938],\n",
      "         [ 0.3458, -0.7884,  0.0486,  ...,  0.0690,  0.2705,  0.0846],\n",
      "         ...,\n",
      "         [ 0.0483, -0.6916, -1.4946,  ...,  0.6309,  1.9592,  0.5444],\n",
      "         [-0.1675,  0.0823, -0.1910,  ...,  0.7805,  0.5113,  1.2717],\n",
      "         [ 0.7419,  0.1929, -0.0045,  ..., -0.6852,  0.3922,  0.6745]],\n",
      "\n",
      "        [[-0.4233, -1.5430, -0.2862,  ..., -0.4914, -1.2815, -0.1305],\n",
      "         [ 0.3619, -0.0170, -0.0179,  ..., -1.1555, -0.3944,  0.4468],\n",
      "         [ 0.0179, -0.0285,  0.6050,  ..., -0.8273,  0.5641, -0.2727],\n",
      "         ...,\n",
      "         [-0.8458, -0.8634, -0.3775,  ..., -0.4969, -0.1628,  0.6229],\n",
      "         [-0.4515,  0.6662, -0.2840,  ..., -0.0644,  1.2604,  0.1662],\n",
      "         [ 0.1584, -0.7305, -1.2785,  ..., -0.1890,  1.2511,  0.6177]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4664, -0.6672,  0.2806,  ..., -0.1647,  1.0109, -0.7361],\n",
      "         [-0.3664, -0.2558,  0.9921,  ...,  0.9345,  1.1735,  0.1668],\n",
      "         [-0.1415,  0.1941,  0.1130,  ..., -0.0228,  0.2306,  0.3111],\n",
      "         ...,\n",
      "         [-0.6205, -0.2095, -0.1951,  ..., -0.1263, -0.1273, -0.0156],\n",
      "         [ 0.1222, -0.4551,  0.2649,  ..., -0.5766,  0.2574, -0.2230],\n",
      "         [-0.0819, -1.0294,  0.3088,  ...,  0.7369,  0.9588,  1.4667]],\n",
      "\n",
      "        [[-0.7735,  0.5460,  0.3469,  ...,  1.6787,  1.0535,  1.0293],\n",
      "         [ 0.4994, -1.3289,  0.5733,  ...,  0.6980,  0.0063,  0.5003],\n",
      "         [-0.2834, -0.6330,  0.8038,  ...,  0.6637,  0.2203,  0.9364],\n",
      "         ...,\n",
      "         [-0.4603, -0.2974,  0.0467,  ..., -0.9819,  1.4110,  1.0667],\n",
      "         [ 0.1228, -0.4843,  0.8436,  ..., -0.8720,  0.4332,  0.6564],\n",
      "         [-0.4808,  0.9214,  0.9079,  ...,  0.4587,  1.1313,  0.5675]],\n",
      "\n",
      "        [[-0.7795, -1.0794, -0.1264,  ...,  0.7847,  0.1064,  0.5896],\n",
      "         [ 0.2458,  0.9323,  0.9775,  ...,  0.0978,  0.1301,  1.0724],\n",
      "         [ 0.8929, -0.9732,  0.3029,  ...,  0.9885,  0.5668, -0.1007],\n",
      "         ...,\n",
      "         [-0.4047, -1.5375,  0.4993,  ...,  0.4412,  0.9553,  1.4975],\n",
      "         [ 0.8614, -0.1954, -0.2242,  ..., -0.3662,  1.2837,  0.3136],\n",
      "         [ 0.1669,  0.2799,  0.1019,  ..., -0.0111,  0.1651,  0.9458]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[-1.3623e+00,  4.0273e-01, -2.9693e-01,  ..., -1.6305e+00,\n",
      "           5.9447e-01,  6.8421e-01],\n",
      "         [-1.8315e+00, -5.2110e-01, -5.7695e-01,  ..., -8.7999e-01,\n",
      "          -1.4210e-01,  1.3719e-01],\n",
      "         [-9.9173e-01,  6.2259e-01, -4.2681e-01,  ..., -2.0175e-01,\n",
      "          -1.1475e+00, -3.1233e-01],\n",
      "         ...,\n",
      "         [-6.8565e-01,  2.3941e-01,  1.0086e+00,  ..., -9.2465e-01,\n",
      "          -2.5398e-01,  1.1833e+00],\n",
      "         [-1.4584e-01, -5.6014e-01,  9.6908e-01,  ..., -1.6819e+00,\n",
      "           2.0272e-01,  1.3351e+00],\n",
      "         [-7.5446e-01, -6.0672e-01,  1.7573e+00,  ..., -1.4852e-03,\n",
      "           7.0850e-01,  1.2055e+00]],\n",
      "\n",
      "        [[ 9.6069e-01,  1.3305e+00, -6.3215e-01,  ...,  2.2441e-02,\n",
      "           7.5370e-01,  1.8988e-01],\n",
      "         [-7.8395e-01,  3.2882e-01, -3.5576e-01,  ..., -6.1975e-01,\n",
      "          -2.7138e-01, -5.4567e-01],\n",
      "         [-1.3393e+00,  3.3387e-01, -8.4583e-02,  ..., -7.7750e-01,\n",
      "          -6.8844e-01,  4.5908e-01],\n",
      "         ...,\n",
      "         [-5.0777e-01, -1.0275e-01,  2.9610e-01,  ..., -2.7812e-01,\n",
      "          -3.3843e-01,  4.9683e-02],\n",
      "         [-4.9094e-01, -1.2007e-01,  1.3146e-03,  ..., -1.1672e+00,\n",
      "           5.6601e-01,  1.1216e-01],\n",
      "         [ 2.7950e-01,  1.0050e-02,  1.1640e-01,  ..., -2.1235e-01,\n",
      "          -7.1977e-01,  5.8360e-01]],\n",
      "\n",
      "        [[-1.4662e+00,  8.6710e-01,  7.8908e-01,  ..., -2.9493e-01,\n",
      "           4.5666e-01, -5.5596e-01],\n",
      "         [-7.0393e-01, -1.7817e-01, -1.4353e+00,  ..., -6.7851e-01,\n",
      "           1.6833e-01,  4.7487e-01],\n",
      "         [-2.3281e-01, -1.0428e+00, -7.7041e-01,  ..., -2.3151e-01,\n",
      "          -6.2434e-01,  3.5567e-01],\n",
      "         ...,\n",
      "         [-1.3082e+00,  2.6573e-01,  5.4306e-01,  ..., -1.1303e+00,\n",
      "           1.4630e-01, -1.6005e-02],\n",
      "         [-1.1293e+00,  5.5555e-01, -6.3530e-02,  ..., -9.6944e-01,\n",
      "           1.0965e+00, -6.3803e-02],\n",
      "         [-8.1382e-01, -3.7003e-01,  1.9175e-01,  ..., -8.7909e-01,\n",
      "           8.3076e-01, -4.2106e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3488e-01,  1.0095e-01, -2.1548e+00,  ..., -1.0056e+00,\n",
      "           2.1651e-01,  4.1790e-03],\n",
      "         [-7.9857e-01,  9.0835e-02,  1.4369e-01,  ..., -1.7103e-01,\n",
      "           7.7128e-01,  3.1070e-01],\n",
      "         [-5.5573e-01, -1.5529e-01, -5.0839e-01,  ...,  1.0183e-01,\n",
      "           6.1523e-01,  1.1462e-01],\n",
      "         ...,\n",
      "         [-1.0705e+00, -9.4362e-02,  7.3424e-01,  ...,  5.7223e-01,\n",
      "          -2.6638e-01,  1.3842e+00],\n",
      "         [-9.7836e-01,  7.9933e-01,  7.0981e-01,  ..., -6.2434e-01,\n",
      "           2.7877e-01, -4.5100e-01],\n",
      "         [-7.3532e-01, -7.3214e-01, -1.1224e-01,  ..., -8.5059e-01,\n",
      "          -7.2550e-01, -1.2328e-01]],\n",
      "\n",
      "        [[-8.3332e-01,  1.1079e+00, -9.1454e-01,  ..., -5.9480e-01,\n",
      "          -9.5732e-01, -1.5259e-01],\n",
      "         [-7.1986e-01,  1.2336e-01, -4.7483e-01,  ..., -5.2665e-01,\n",
      "          -1.1670e+00, -2.3568e-02],\n",
      "         [-8.9566e-01,  8.7479e-01, -2.4161e-01,  ...,  1.3040e-01,\n",
      "          -5.2549e-01,  9.1104e-01],\n",
      "         ...,\n",
      "         [-7.1785e-01,  7.9654e-01,  2.9964e-02,  ..., -9.4673e-01,\n",
      "          -3.3029e-02,  1.5557e+00],\n",
      "         [-8.3094e-01,  9.4769e-01,  4.9518e-01,  ..., -6.2105e-01,\n",
      "           6.2299e-01, -5.6087e-01],\n",
      "         [-1.2420e+00, -1.0285e-01,  3.8760e-01,  ...,  4.5338e-02,\n",
      "           2.3620e-01,  4.6092e-01]],\n",
      "\n",
      "        [[ 2.0660e-01,  1.2430e+00,  2.6474e-01,  ..., -2.2785e-01,\n",
      "          -2.3254e-01, -3.3193e-01],\n",
      "         [-7.1321e-01,  1.5610e+00, -1.0373e+00,  ..., -2.6312e-01,\n",
      "          -1.8246e-01,  1.1279e+00],\n",
      "         [-4.2133e-01,  1.4246e-01, -7.5440e-01,  ...,  4.4920e-01,\n",
      "          -6.3252e-01, -3.3790e-01],\n",
      "         ...,\n",
      "         [ 4.5088e-01, -1.7256e-01, -7.1876e-01,  ..., -4.9155e-01,\n",
      "          -4.4430e-01,  8.9034e-01],\n",
      "         [-6.0621e-02,  6.2499e-01,  1.8087e+00,  ..., -1.5533e+00,\n",
      "          -4.4059e-01,  7.3793e-01],\n",
      "         [-6.4458e-01,  1.9181e-01,  1.6575e+00,  ..., -1.5649e+00,\n",
      "          -4.3263e-01,  1.2498e-01]]], grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[ 0.1855,  0.7302, -0.7874,  ..., -0.5446, -0.8272,  0.5998],\n",
      "         [-0.5279, -1.4892, -0.3156,  ..., -0.3031,  0.4026, -0.0673],\n",
      "         [ 0.1572, -0.2414, -0.0058,  ..., -0.6722, -0.7990,  0.1302],\n",
      "         ...,\n",
      "         [-0.2355, -1.1972,  0.1662,  ..., -1.3703, -0.1711,  0.6647],\n",
      "         [-0.1616,  0.3431, -0.7869,  ..., -0.6888, -0.2674, -0.5217],\n",
      "         [ 0.8664,  0.0755,  0.2319,  ..., -0.8704, -0.6277,  0.0274]],\n",
      "\n",
      "        [[-0.7280, -0.1051, -1.1411,  ..., -0.6702, -0.2983, -0.2191],\n",
      "         [-0.3167, -0.0951,  0.1751,  ..., -0.0796, -0.7978,  0.8114],\n",
      "         [ 0.1602,  1.0935, -1.0271,  ...,  0.0032, -0.4554, -0.0883],\n",
      "         ...,\n",
      "         [ 0.0256, -0.1773,  0.8322,  ..., -0.9736, -0.8243,  0.3466],\n",
      "         [ 0.8931, -0.1039, -1.0558,  ..., -0.6583, -0.2107, -0.5275],\n",
      "         [-0.7454, -0.4201,  1.2454,  ..., -0.5234,  0.6721, -0.0478]],\n",
      "\n",
      "        [[-0.0419,  1.2706, -0.3579,  ..., -0.1420, -0.6461,  1.3609],\n",
      "         [ 0.0503,  0.0017,  0.0594,  ..., -0.8140, -0.2032,  0.2923],\n",
      "         [ 0.3634,  0.3863, -0.2150,  ..., -0.2112, -0.3878,  0.2523],\n",
      "         ...,\n",
      "         [-0.6340, -0.2208,  0.3885,  ..., -0.6692, -0.1099,  0.7158],\n",
      "         [ 0.0433, -0.0859, -0.2866,  ..., -0.6167,  0.0843,  0.9663],\n",
      "         [-0.3729, -0.1642,  0.3091,  ...,  0.1686,  0.4445,  0.5517]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4675,  0.4298,  0.5323,  ..., -0.5736, -0.2453,  0.9277],\n",
      "         [ 0.2782, -0.6951,  0.6045,  ..., -1.5706, -1.6667,  0.0642],\n",
      "         [-0.2384,  0.3701, -0.7939,  ..., -0.6272, -0.5853, -0.3587],\n",
      "         ...,\n",
      "         [-0.0155, -0.4487, -0.4197,  ..., -1.3304,  0.4911,  0.4615],\n",
      "         [-0.2423,  0.2848,  0.2147,  ..., -0.2980, -0.6209,  0.0920],\n",
      "         [-0.2525,  0.5490, -0.3108,  ..., -0.1385,  0.1762,  0.4198]],\n",
      "\n",
      "        [[ 0.8028, -0.1205,  0.3734,  ..., -1.3920,  0.6222,  0.4866],\n",
      "         [ 0.0418,  0.3277,  1.0693,  ..., -0.9224,  0.6265, -0.2922],\n",
      "         [ 0.6155, -0.1855, -0.2858,  ..., -0.8044, -0.8475,  1.1724],\n",
      "         ...,\n",
      "         [-0.1106,  0.0721,  1.1558,  ..., -0.1499, -0.1314, -0.7365],\n",
      "         [ 0.7340,  0.0045,  0.5663,  ..., -0.5290,  0.1108, -0.5996],\n",
      "         [ 0.5555,  0.3918,  0.5025,  ..., -0.3031, -0.3362, -0.3196]],\n",
      "\n",
      "        [[ 0.4192, -0.9082,  0.5591,  ..., -0.7377,  0.1763, -0.4129],\n",
      "         [ 0.0246,  0.9614,  0.0244,  ..., -0.6293, -0.9300,  1.1505],\n",
      "         [ 0.1688,  0.6564, -0.8821,  ...,  0.0523, -1.3803, -0.7653],\n",
      "         ...,\n",
      "         [ 0.2196, -0.3188,  0.0778,  ..., -0.3403,  0.6812,  0.9361],\n",
      "         [ 0.4077, -0.3903, -0.2589,  ...,  0.5617,  0.1307, -0.0404],\n",
      "         [-0.1264,  0.2373,  1.1068,  ..., -0.5371,  0.5766, -0.7150]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **分割多头的目的**\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n",
    "\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim\n",
    "#　将`num_heads`维度提到前面，方便后续并行计算。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0265, 0.0187, 0.0142,  ..., 0.0152, 0.0231, 0.0219],\n",
      "          [0.0101, 0.0069, 0.0136,  ..., 0.0133, 0.0247, 0.0225],\n",
      "          [0.0322, 0.0225, 0.0185,  ..., 0.0114, 0.0155, 0.0176],\n",
      "          ...,\n",
      "          [0.0135, 0.0103, 0.0169,  ..., 0.0073, 0.0208, 0.0113],\n",
      "          [0.0202, 0.0117, 0.0260,  ..., 0.0115, 0.0093, 0.0112],\n",
      "          [0.0168, 0.0290, 0.0252,  ..., 0.0125, 0.0295, 0.0214]],\n",
      "\n",
      "         [[0.0243, 0.0167, 0.0392,  ..., 0.0158, 0.0278, 0.0250],\n",
      "          [0.0155, 0.0233, 0.0603,  ..., 0.0247, 0.0093, 0.0325],\n",
      "          [0.0203, 0.0211, 0.0318,  ..., 0.0186, 0.0152, 0.0195],\n",
      "          ...,\n",
      "          [0.0151, 0.0279, 0.0468,  ..., 0.0227, 0.0078, 0.0146],\n",
      "          [0.0140, 0.0189, 0.0337,  ..., 0.0309, 0.0211, 0.0345],\n",
      "          [0.0211, 0.0180, 0.0352,  ..., 0.0276, 0.0229, 0.0268]],\n",
      "\n",
      "         [[0.0181, 0.0281, 0.0149,  ..., 0.0235, 0.0277, 0.0148],\n",
      "          [0.0356, 0.0564, 0.0433,  ..., 0.0105, 0.0167, 0.0255],\n",
      "          [0.0242, 0.0320, 0.0224,  ..., 0.0207, 0.0262, 0.0167],\n",
      "          ...,\n",
      "          [0.0233, 0.0324, 0.0111,  ..., 0.0124, 0.0179, 0.0263],\n",
      "          [0.0169, 0.0161, 0.0142,  ..., 0.0229, 0.0133, 0.0229],\n",
      "          [0.0181, 0.0326, 0.0191,  ..., 0.0226, 0.0185, 0.0463]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0259, 0.0136, 0.0321,  ..., 0.0210, 0.0105, 0.0192],\n",
      "          [0.0254, 0.0092, 0.0420,  ..., 0.0173, 0.0189, 0.0211],\n",
      "          [0.0220, 0.0109, 0.0305,  ..., 0.0127, 0.0104, 0.0216],\n",
      "          ...,\n",
      "          [0.0085, 0.0198, 0.0569,  ..., 0.0222, 0.0326, 0.0158],\n",
      "          [0.0101, 0.0296, 0.0404,  ..., 0.0161, 0.0350, 0.0252],\n",
      "          [0.0128, 0.0135, 0.0717,  ..., 0.0116, 0.0456, 0.0204]],\n",
      "\n",
      "         [[0.0159, 0.0335, 0.0152,  ..., 0.0291, 0.0161, 0.0149],\n",
      "          [0.0110, 0.0325, 0.0192,  ..., 0.0260, 0.0154, 0.0131],\n",
      "          [0.0126, 0.0145, 0.0233,  ..., 0.0171, 0.0133, 0.0303],\n",
      "          ...,\n",
      "          [0.0126, 0.0121, 0.0347,  ..., 0.0374, 0.0251, 0.0226],\n",
      "          [0.0098, 0.0132, 0.0158,  ..., 0.0434, 0.0208, 0.0524],\n",
      "          [0.0141, 0.0115, 0.0216,  ..., 0.0250, 0.0221, 0.0202]],\n",
      "\n",
      "         [[0.0144, 0.0137, 0.0127,  ..., 0.0182, 0.0300, 0.0340],\n",
      "          [0.0405, 0.0201, 0.0313,  ..., 0.0132, 0.0212, 0.0351],\n",
      "          [0.0180, 0.0220, 0.0152,  ..., 0.0184, 0.0125, 0.0195],\n",
      "          ...,\n",
      "          [0.0261, 0.0137, 0.0162,  ..., 0.0082, 0.0064, 0.0141],\n",
      "          [0.0137, 0.0135, 0.0061,  ..., 0.0217, 0.0179, 0.0119],\n",
      "          [0.0136, 0.0082, 0.0055,  ..., 0.0102, 0.0102, 0.0094]]],\n",
      "\n",
      "\n",
      "        [[[0.0127, 0.0103, 0.0090,  ..., 0.0135, 0.0318, 0.0238],\n",
      "          [0.0189, 0.0253, 0.0202,  ..., 0.0257, 0.0156, 0.0160],\n",
      "          [0.0119, 0.0285, 0.0171,  ..., 0.0159, 0.0160, 0.0274],\n",
      "          ...,\n",
      "          [0.0175, 0.0228, 0.0157,  ..., 0.0171, 0.0204, 0.0117],\n",
      "          [0.0203, 0.0153, 0.0186,  ..., 0.0147, 0.0125, 0.0120],\n",
      "          [0.0212, 0.0294, 0.0236,  ..., 0.0156, 0.0208, 0.0166]],\n",
      "\n",
      "         [[0.0166, 0.0095, 0.0359,  ..., 0.0231, 0.0213, 0.0216],\n",
      "          [0.0155, 0.0078, 0.0467,  ..., 0.0397, 0.0169, 0.0165],\n",
      "          [0.0262, 0.0163, 0.0203,  ..., 0.0175, 0.0286, 0.0095],\n",
      "          ...,\n",
      "          [0.0213, 0.0136, 0.0136,  ..., 0.0165, 0.0285, 0.0151],\n",
      "          [0.0116, 0.0163, 0.0184,  ..., 0.0124, 0.0250, 0.0307],\n",
      "          [0.0220, 0.0156, 0.0129,  ..., 0.0142, 0.0221, 0.0134]],\n",
      "\n",
      "         [[0.0146, 0.0183, 0.0242,  ..., 0.0186, 0.0160, 0.0177],\n",
      "          [0.0176, 0.0120, 0.0433,  ..., 0.0220, 0.0124, 0.0236],\n",
      "          [0.0103, 0.0167, 0.0184,  ..., 0.0128, 0.0110, 0.0242],\n",
      "          ...,\n",
      "          [0.0168, 0.0300, 0.0148,  ..., 0.0156, 0.0488, 0.0120],\n",
      "          [0.0151, 0.0147, 0.0294,  ..., 0.0119, 0.0191, 0.0197],\n",
      "          [0.0280, 0.0331, 0.0121,  ..., 0.0151, 0.0299, 0.0115]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0176, 0.0142, 0.0152,  ..., 0.0125, 0.0544, 0.0436],\n",
      "          [0.0283, 0.0169, 0.0201,  ..., 0.0251, 0.0419, 0.0512],\n",
      "          [0.0259, 0.0189, 0.0414,  ..., 0.0248, 0.0175, 0.0304],\n",
      "          ...,\n",
      "          [0.0124, 0.0193, 0.0184,  ..., 0.0209, 0.0353, 0.0214],\n",
      "          [0.0084, 0.0159, 0.0101,  ..., 0.0185, 0.0208, 0.0237],\n",
      "          [0.0182, 0.0116, 0.0249,  ..., 0.0310, 0.0179, 0.0223]],\n",
      "\n",
      "         [[0.0141, 0.0158, 0.0245,  ..., 0.0384, 0.0148, 0.0234],\n",
      "          [0.0125, 0.0138, 0.0449,  ..., 0.0171, 0.0240, 0.0131],\n",
      "          [0.0120, 0.0110, 0.0307,  ..., 0.0079, 0.0085, 0.0141],\n",
      "          ...,\n",
      "          [0.0148, 0.0256, 0.0214,  ..., 0.0253, 0.0198, 0.0486],\n",
      "          [0.0166, 0.0190, 0.0203,  ..., 0.0229, 0.0332, 0.0147],\n",
      "          [0.0102, 0.0120, 0.0139,  ..., 0.0266, 0.0145, 0.0103]],\n",
      "\n",
      "         [[0.0182, 0.0233, 0.0238,  ..., 0.0206, 0.0206, 0.0272],\n",
      "          [0.0250, 0.0150, 0.0124,  ..., 0.0172, 0.0189, 0.0200],\n",
      "          [0.0140, 0.0208, 0.0247,  ..., 0.0429, 0.0075, 0.0136],\n",
      "          ...,\n",
      "          [0.0070, 0.0169, 0.0312,  ..., 0.0169, 0.0030, 0.0077],\n",
      "          [0.0256, 0.0283, 0.0263,  ..., 0.0181, 0.0105, 0.0215],\n",
      "          [0.0116, 0.0200, 0.0179,  ..., 0.0173, 0.0217, 0.0335]]],\n",
      "\n",
      "\n",
      "        [[[0.0170, 0.0352, 0.0042,  ..., 0.0110, 0.0191, 0.0115],\n",
      "          [0.0117, 0.0182, 0.0109,  ..., 0.0064, 0.0131, 0.0126],\n",
      "          [0.0201, 0.0263, 0.0112,  ..., 0.0124, 0.0072, 0.0127],\n",
      "          ...,\n",
      "          [0.0173, 0.0270, 0.0201,  ..., 0.0200, 0.0099, 0.0147],\n",
      "          [0.0248, 0.0527, 0.0110,  ..., 0.0142, 0.0089, 0.0104],\n",
      "          [0.0103, 0.0226, 0.0439,  ..., 0.0104, 0.0113, 0.0148]],\n",
      "\n",
      "         [[0.0206, 0.0167, 0.0161,  ..., 0.0189, 0.0170, 0.0104],\n",
      "          [0.0255, 0.0171, 0.0129,  ..., 0.0225, 0.0145, 0.0119],\n",
      "          [0.0084, 0.0143, 0.0458,  ..., 0.0273, 0.0079, 0.0208],\n",
      "          ...,\n",
      "          [0.0215, 0.0225, 0.0201,  ..., 0.0216, 0.0215, 0.0129],\n",
      "          [0.0212, 0.0136, 0.0170,  ..., 0.0135, 0.0324, 0.0188],\n",
      "          [0.0283, 0.0077, 0.0084,  ..., 0.0142, 0.0203, 0.0120]],\n",
      "\n",
      "         [[0.0174, 0.0233, 0.0252,  ..., 0.0197, 0.0315, 0.0190],\n",
      "          [0.0256, 0.0191, 0.0273,  ..., 0.0118, 0.0426, 0.0176],\n",
      "          [0.0259, 0.0208, 0.0208,  ..., 0.0198, 0.0168, 0.0240],\n",
      "          ...,\n",
      "          [0.0212, 0.0159, 0.0243,  ..., 0.0218, 0.0311, 0.0164],\n",
      "          [0.0274, 0.0152, 0.0205,  ..., 0.0242, 0.0122, 0.0177],\n",
      "          [0.0146, 0.0286, 0.0167,  ..., 0.0150, 0.0163, 0.0167]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0227, 0.0344, 0.0210,  ..., 0.0172, 0.0070, 0.0132],\n",
      "          [0.0335, 0.0436, 0.0367,  ..., 0.0146, 0.0042, 0.0101],\n",
      "          [0.0284, 0.0237, 0.0337,  ..., 0.0145, 0.0146, 0.0130],\n",
      "          ...,\n",
      "          [0.0220, 0.0146, 0.0320,  ..., 0.0179, 0.0104, 0.0161],\n",
      "          [0.0129, 0.0183, 0.0201,  ..., 0.0171, 0.0113, 0.0126],\n",
      "          [0.0049, 0.0195, 0.0254,  ..., 0.0146, 0.0116, 0.0437]],\n",
      "\n",
      "         [[0.0207, 0.0169, 0.0174,  ..., 0.0248, 0.0291, 0.0193],\n",
      "          [0.0303, 0.0149, 0.0189,  ..., 0.0283, 0.0414, 0.0150],\n",
      "          [0.0115, 0.0133, 0.0476,  ..., 0.0128, 0.0323, 0.0297],\n",
      "          ...,\n",
      "          [0.0263, 0.0198, 0.0393,  ..., 0.0251, 0.0280, 0.0247],\n",
      "          [0.0271, 0.0138, 0.0167,  ..., 0.0439, 0.0229, 0.0306],\n",
      "          [0.0186, 0.0115, 0.0103,  ..., 0.0194, 0.0183, 0.0215]],\n",
      "\n",
      "         [[0.0144, 0.0289, 0.0096,  ..., 0.0267, 0.0113, 0.0310],\n",
      "          [0.0116, 0.0133, 0.0124,  ..., 0.0136, 0.0172, 0.0258],\n",
      "          [0.0145, 0.0275, 0.0163,  ..., 0.0183, 0.0141, 0.0328],\n",
      "          ...,\n",
      "          [0.0100, 0.0140, 0.0157,  ..., 0.0198, 0.0333, 0.0185],\n",
      "          [0.0125, 0.0160, 0.0066,  ..., 0.0164, 0.0221, 0.0158],\n",
      "          [0.0300, 0.0076, 0.0162,  ..., 0.0112, 0.0179, 0.0250]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0143, 0.0200, 0.0169,  ..., 0.0209, 0.0370, 0.0171],\n",
      "          [0.0071, 0.0233, 0.0150,  ..., 0.0190, 0.0222, 0.0153],\n",
      "          [0.0143, 0.0275, 0.0107,  ..., 0.0276, 0.0275, 0.0135],\n",
      "          ...,\n",
      "          [0.0112, 0.0238, 0.0196,  ..., 0.0340, 0.0183, 0.0165],\n",
      "          [0.0206, 0.0280, 0.0143,  ..., 0.0166, 0.0242, 0.0100],\n",
      "          [0.0128, 0.0181, 0.0149,  ..., 0.0437, 0.0337, 0.0254]],\n",
      "\n",
      "         [[0.0230, 0.0205, 0.0119,  ..., 0.0362, 0.0097, 0.0179],\n",
      "          [0.0113, 0.0187, 0.0093,  ..., 0.0397, 0.0142, 0.0098],\n",
      "          [0.0172, 0.0232, 0.0200,  ..., 0.0196, 0.0153, 0.0144],\n",
      "          ...,\n",
      "          [0.0370, 0.0160, 0.0182,  ..., 0.0132, 0.0228, 0.0370],\n",
      "          [0.0305, 0.0190, 0.0198,  ..., 0.0132, 0.0134, 0.0096],\n",
      "          [0.0323, 0.0180, 0.0170,  ..., 0.0182, 0.0168, 0.0198]],\n",
      "\n",
      "         [[0.0123, 0.0103, 0.0150,  ..., 0.0297, 0.0126, 0.0367],\n",
      "          [0.0208, 0.0122, 0.0122,  ..., 0.0326, 0.0195, 0.0283],\n",
      "          [0.0071, 0.0254, 0.0177,  ..., 0.0417, 0.0140, 0.0345],\n",
      "          ...,\n",
      "          [0.0189, 0.0151, 0.0187,  ..., 0.0113, 0.0410, 0.0096],\n",
      "          [0.0172, 0.0160, 0.0198,  ..., 0.0092, 0.0138, 0.0181],\n",
      "          [0.0143, 0.0188, 0.0152,  ..., 0.0183, 0.0171, 0.0121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0090, 0.0209, 0.0258,  ..., 0.0179, 0.0282, 0.0358],\n",
      "          [0.0194, 0.0236, 0.0153,  ..., 0.0330, 0.0222, 0.0259],\n",
      "          [0.0092, 0.0160, 0.0157,  ..., 0.0116, 0.0221, 0.0294],\n",
      "          ...,\n",
      "          [0.0107, 0.0083, 0.0127,  ..., 0.0687, 0.0273, 0.0380],\n",
      "          [0.0146, 0.0154, 0.0152,  ..., 0.0194, 0.0294, 0.0324],\n",
      "          [0.0245, 0.0128, 0.0128,  ..., 0.0256, 0.0331, 0.0260]],\n",
      "\n",
      "         [[0.0750, 0.0072, 0.0126,  ..., 0.0240, 0.0145, 0.0249],\n",
      "          [0.0607, 0.0091, 0.0281,  ..., 0.0119, 0.0187, 0.0210],\n",
      "          [0.0297, 0.0086, 0.0100,  ..., 0.0140, 0.0184, 0.0222],\n",
      "          ...,\n",
      "          [0.0069, 0.0130, 0.0145,  ..., 0.0219, 0.0128, 0.0228],\n",
      "          [0.0117, 0.0103, 0.0218,  ..., 0.0159, 0.0137, 0.0170],\n",
      "          [0.0621, 0.0135, 0.0358,  ..., 0.0195, 0.0115, 0.0259]],\n",
      "\n",
      "         [[0.0135, 0.0164, 0.0127,  ..., 0.0326, 0.0099, 0.0222],\n",
      "          [0.0133, 0.0128, 0.0094,  ..., 0.0420, 0.0082, 0.0192],\n",
      "          [0.0243, 0.0254, 0.0126,  ..., 0.0329, 0.0107, 0.0153],\n",
      "          ...,\n",
      "          [0.0341, 0.0144, 0.0140,  ..., 0.0349, 0.0128, 0.0139],\n",
      "          [0.0165, 0.0163, 0.0212,  ..., 0.0129, 0.0165, 0.0188],\n",
      "          [0.0150, 0.0151, 0.0080,  ..., 0.0164, 0.0088, 0.0066]]],\n",
      "\n",
      "\n",
      "        [[[0.0336, 0.0244, 0.0254,  ..., 0.0118, 0.0135, 0.0185],\n",
      "          [0.0217, 0.0234, 0.0123,  ..., 0.0075, 0.0096, 0.0303],\n",
      "          [0.0198, 0.0199, 0.0120,  ..., 0.0247, 0.0168, 0.0194],\n",
      "          ...,\n",
      "          [0.0167, 0.0172, 0.0135,  ..., 0.0321, 0.0131, 0.0178],\n",
      "          [0.0242, 0.0352, 0.0153,  ..., 0.0322, 0.0144, 0.0182],\n",
      "          [0.0239, 0.0132, 0.0119,  ..., 0.0207, 0.0184, 0.0149]],\n",
      "\n",
      "         [[0.0142, 0.0186, 0.0246,  ..., 0.0153, 0.0155, 0.0356],\n",
      "          [0.0212, 0.0137, 0.0194,  ..., 0.0220, 0.0176, 0.0329],\n",
      "          [0.0208, 0.0172, 0.0386,  ..., 0.0136, 0.0118, 0.0374],\n",
      "          ...,\n",
      "          [0.0276, 0.0194, 0.0330,  ..., 0.0170, 0.0196, 0.0300],\n",
      "          [0.0404, 0.0226, 0.0243,  ..., 0.0173, 0.0171, 0.0219],\n",
      "          [0.0388, 0.0113, 0.0238,  ..., 0.0239, 0.0192, 0.0170]],\n",
      "\n",
      "         [[0.0114, 0.0234, 0.0089,  ..., 0.0304, 0.0381, 0.0157],\n",
      "          [0.0119, 0.0199, 0.0108,  ..., 0.0493, 0.0266, 0.0152],\n",
      "          [0.0229, 0.0200, 0.0274,  ..., 0.0277, 0.0184, 0.0104],\n",
      "          ...,\n",
      "          [0.0169, 0.0229, 0.0250,  ..., 0.0271, 0.0223, 0.0258],\n",
      "          [0.0078, 0.0179, 0.0146,  ..., 0.0208, 0.0202, 0.0249],\n",
      "          [0.0122, 0.0191, 0.0278,  ..., 0.0454, 0.0317, 0.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0172, 0.0157, 0.0216,  ..., 0.0225, 0.0163, 0.0166],\n",
      "          [0.0159, 0.0133, 0.0112,  ..., 0.0132, 0.0179, 0.0171],\n",
      "          [0.0143, 0.0131, 0.0198,  ..., 0.0160, 0.0104, 0.0103],\n",
      "          ...,\n",
      "          [0.0165, 0.0158, 0.0200,  ..., 0.0090, 0.0228, 0.0366],\n",
      "          [0.0183, 0.0168, 0.0203,  ..., 0.0123, 0.0448, 0.0341],\n",
      "          [0.0173, 0.0209, 0.0187,  ..., 0.0059, 0.0316, 0.0371]],\n",
      "\n",
      "         [[0.0152, 0.0187, 0.0163,  ..., 0.0082, 0.0101, 0.0090],\n",
      "          [0.0094, 0.0108, 0.0164,  ..., 0.0289, 0.0158, 0.0079],\n",
      "          [0.0109, 0.0205, 0.0232,  ..., 0.0101, 0.0191, 0.0122],\n",
      "          ...,\n",
      "          [0.0285, 0.0176, 0.0243,  ..., 0.0123, 0.0172, 0.0100],\n",
      "          [0.0228, 0.0189, 0.0275,  ..., 0.0127, 0.0141, 0.0095],\n",
      "          [0.0224, 0.0198, 0.0181,  ..., 0.0091, 0.0143, 0.0101]],\n",
      "\n",
      "         [[0.0134, 0.0184, 0.0524,  ..., 0.0162, 0.0217, 0.0113],\n",
      "          [0.0140, 0.0225, 0.0166,  ..., 0.0260, 0.0349, 0.0154],\n",
      "          [0.0090, 0.0262, 0.0186,  ..., 0.0359, 0.0197, 0.0212],\n",
      "          ...,\n",
      "          [0.0092, 0.0184, 0.0190,  ..., 0.0276, 0.0249, 0.0185],\n",
      "          [0.0069, 0.0119, 0.0119,  ..., 0.0409, 0.0202, 0.0140],\n",
      "          [0.0089, 0.0146, 0.0085,  ..., 0.0244, 0.0199, 0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0096, 0.0054, 0.0197,  ..., 0.0252, 0.0280, 0.0290],\n",
      "          [0.0167, 0.0147, 0.0119,  ..., 0.0148, 0.0155, 0.0128],\n",
      "          [0.0157, 0.0061, 0.0162,  ..., 0.0343, 0.0148, 0.0257],\n",
      "          ...,\n",
      "          [0.0095, 0.0037, 0.0151,  ..., 0.0147, 0.0241, 0.0134],\n",
      "          [0.0144, 0.0105, 0.0178,  ..., 0.0252, 0.0226, 0.0100],\n",
      "          [0.0207, 0.0118, 0.0147,  ..., 0.0169, 0.0122, 0.0163]],\n",
      "\n",
      "         [[0.0305, 0.0260, 0.0449,  ..., 0.0242, 0.0128, 0.0230],\n",
      "          [0.0145, 0.0251, 0.0294,  ..., 0.0265, 0.0120, 0.0116],\n",
      "          [0.0189, 0.0244, 0.0523,  ..., 0.0204, 0.0171, 0.0127],\n",
      "          ...,\n",
      "          [0.0172, 0.0288, 0.0233,  ..., 0.0242, 0.0253, 0.0139],\n",
      "          [0.0124, 0.0191, 0.0261,  ..., 0.0157, 0.0371, 0.0145],\n",
      "          [0.0082, 0.0141, 0.0149,  ..., 0.0118, 0.0116, 0.0142]],\n",
      "\n",
      "         [[0.0094, 0.0303, 0.0153,  ..., 0.0186, 0.0187, 0.0245],\n",
      "          [0.0181, 0.0214, 0.0516,  ..., 0.0157, 0.0262, 0.0292],\n",
      "          [0.0153, 0.0235, 0.0664,  ..., 0.0210, 0.0270, 0.0460],\n",
      "          ...,\n",
      "          [0.0204, 0.0149, 0.0052,  ..., 0.0126, 0.0235, 0.0292],\n",
      "          [0.0276, 0.0191, 0.0247,  ..., 0.0250, 0.0242, 0.0116],\n",
      "          [0.0119, 0.0176, 0.0104,  ..., 0.0128, 0.0274, 0.0271]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0145, 0.0072, 0.0196,  ..., 0.0179, 0.0119, 0.0164],\n",
      "          [0.0318, 0.0195, 0.0039,  ..., 0.0280, 0.0217, 0.0278],\n",
      "          [0.0176, 0.0274, 0.0138,  ..., 0.0217, 0.0147, 0.0108],\n",
      "          ...,\n",
      "          [0.0144, 0.0058, 0.0333,  ..., 0.0234, 0.0140, 0.0187],\n",
      "          [0.0221, 0.0123, 0.0125,  ..., 0.0226, 0.0273, 0.0147],\n",
      "          [0.0210, 0.0143, 0.0116,  ..., 0.0314, 0.0339, 0.0218]],\n",
      "\n",
      "         [[0.0226, 0.0547, 0.0172,  ..., 0.0184, 0.0101, 0.0114],\n",
      "          [0.0250, 0.0111, 0.0221,  ..., 0.0341, 0.0103, 0.0229],\n",
      "          [0.0358, 0.0225, 0.0247,  ..., 0.0274, 0.0092, 0.0172],\n",
      "          ...,\n",
      "          [0.0138, 0.0075, 0.0141,  ..., 0.0111, 0.0197, 0.0217],\n",
      "          [0.0232, 0.0134, 0.0119,  ..., 0.0113, 0.0172, 0.0276],\n",
      "          [0.0181, 0.0171, 0.0139,  ..., 0.0136, 0.0133, 0.0124]],\n",
      "\n",
      "         [[0.0116, 0.0079, 0.0359,  ..., 0.0546, 0.0284, 0.0131],\n",
      "          [0.0144, 0.0180, 0.0139,  ..., 0.0174, 0.0367, 0.0193],\n",
      "          [0.0152, 0.0174, 0.0231,  ..., 0.0227, 0.0183, 0.0265],\n",
      "          ...,\n",
      "          [0.0097, 0.0117, 0.0234,  ..., 0.0212, 0.0212, 0.0092],\n",
      "          [0.0149, 0.0135, 0.0150,  ..., 0.0085, 0.0270, 0.0070],\n",
      "          [0.0171, 0.0263, 0.0157,  ..., 0.0230, 0.0335, 0.0176]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-1.5035e-01, -2.7516e-01, -3.4544e-01,  ...,  2.2935e-01,\n",
      "            7.0910e-02, -1.1553e-01],\n",
      "          [-5.8306e-02, -3.3415e-01, -3.2840e-01,  ...,  2.0158e-01,\n",
      "            6.5207e-02, -1.8060e-01],\n",
      "          [-1.3762e-01, -3.1333e-01, -2.9794e-01,  ...,  2.3688e-01,\n",
      "            3.6206e-02, -1.5012e-01],\n",
      "          ...,\n",
      "          [-1.2832e-01, -2.9163e-01, -4.0652e-01,  ...,  1.4682e-01,\n",
      "            1.3075e-01, -1.7799e-01],\n",
      "          [-1.4818e-01, -3.2333e-01, -3.9357e-01,  ...,  2.1895e-01,\n",
      "           -1.3722e-02, -2.2066e-01],\n",
      "          [-1.6549e-01, -3.0701e-01, -3.6962e-01,  ...,  1.8887e-01,\n",
      "            1.2797e-02, -1.7915e-01]],\n",
      "\n",
      "         [[-6.4012e-03,  4.8073e-01, -1.5934e-02,  ..., -2.5138e-01,\n",
      "            6.3740e-01,  2.5416e-01],\n",
      "          [ 1.1690e-02,  4.4737e-01,  1.0041e-02,  ..., -2.4958e-01,\n",
      "            6.3983e-01,  2.7593e-01],\n",
      "          [ 7.1079e-02,  4.6230e-01, -5.9946e-04,  ..., -1.4933e-01,\n",
      "            6.0529e-01,  2.7376e-01],\n",
      "          ...,\n",
      "          [ 2.0548e-02,  4.1717e-01, -5.5771e-03,  ..., -1.7341e-01,\n",
      "            6.2065e-01,  3.0051e-01],\n",
      "          [ 7.6464e-02,  4.4759e-01, -7.1387e-03,  ..., -1.5257e-01,\n",
      "            5.8539e-01,  2.7106e-01],\n",
      "          [ 3.1908e-02,  4.9421e-01, -3.2783e-02,  ..., -2.1448e-01,\n",
      "            6.2787e-01,  2.7527e-01]],\n",
      "\n",
      "         [[ 3.0213e-01, -2.7181e-01, -1.2167e-01,  ...,  2.2108e-01,\n",
      "           -1.9985e-01,  1.4001e+00],\n",
      "          [ 2.6023e-01, -2.7739e-01, -2.4501e-02,  ...,  2.4124e-01,\n",
      "           -3.1409e-01,  1.4784e+00],\n",
      "          [ 3.2652e-01, -2.8906e-01, -1.0161e-01,  ...,  2.5414e-01,\n",
      "           -1.3461e-01,  1.4480e+00],\n",
      "          ...,\n",
      "          [ 3.9849e-01, -1.1732e-01, -3.9217e-02,  ...,  2.6892e-01,\n",
      "           -1.1001e-01,  1.4566e+00],\n",
      "          [ 3.4834e-01, -2.3976e-01, -1.6305e-01,  ...,  2.5840e-01,\n",
      "           -8.0930e-02,  1.3935e+00],\n",
      "          [ 3.1258e-01, -1.7593e-01, -1.0036e-01,  ...,  2.7534e-01,\n",
      "           -2.1064e-01,  1.4267e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4074e-01, -3.9145e-01, -8.5820e-02,  ...,  1.8180e-01,\n",
      "            2.7717e-01,  4.0600e-01],\n",
      "          [-1.7582e-01, -3.5912e-01, -6.3682e-02,  ...,  2.1685e-01,\n",
      "            3.3144e-01,  4.6870e-01],\n",
      "          [-2.0652e-01, -3.3809e-01, -8.1959e-02,  ...,  1.5387e-01,\n",
      "            2.8158e-01,  4.0196e-01],\n",
      "          ...,\n",
      "          [-2.2251e-01, -3.4917e-01, -3.2901e-02,  ...,  1.7956e-01,\n",
      "            2.7358e-01,  4.8244e-01],\n",
      "          [-1.8539e-01, -4.2327e-01, -1.4527e-01,  ...,  8.9792e-02,\n",
      "            2.3114e-01,  4.4217e-01],\n",
      "          [-3.0211e-01, -3.4091e-01, -8.8120e-02,  ...,  1.4895e-01,\n",
      "            2.5384e-01,  4.8307e-01]],\n",
      "\n",
      "         [[-4.0193e-01, -2.1712e-01, -8.5559e-01,  ...,  5.7337e-01,\n",
      "            2.8936e-02, -5.3555e-02],\n",
      "          [-4.2900e-01, -2.4302e-01, -7.9244e-01,  ...,  5.2195e-01,\n",
      "           -3.2313e-02, -3.2446e-03],\n",
      "          [-4.1780e-01, -2.4917e-01, -8.0619e-01,  ...,  5.4865e-01,\n",
      "           -4.3986e-02, -4.7476e-02],\n",
      "          ...,\n",
      "          [-4.2266e-01, -2.7729e-01, -7.3626e-01,  ...,  5.3227e-01,\n",
      "           -3.4512e-02, -6.8106e-02],\n",
      "          [-4.2969e-01, -2.7616e-01, -7.4472e-01,  ...,  5.4744e-01,\n",
      "           -4.8397e-02,  2.6439e-02],\n",
      "          [-4.1307e-01, -2.5262e-01, -7.6711e-01,  ...,  5.1723e-01,\n",
      "            1.4415e-02,  2.9284e-03]],\n",
      "\n",
      "         [[-4.2798e-01,  2.1902e-01,  5.9104e-01,  ..., -3.5580e-01,\n",
      "           -3.2823e-01,  3.2864e-01],\n",
      "          [-4.3524e-01,  2.1848e-01,  5.9397e-01,  ..., -3.4915e-01,\n",
      "           -3.6065e-01,  2.8088e-01],\n",
      "          [-4.2475e-01,  2.1583e-01,  5.5510e-01,  ..., -3.7969e-01,\n",
      "           -3.1887e-01,  3.1800e-01],\n",
      "          ...,\n",
      "          [-4.1389e-01,  2.5311e-01,  5.8125e-01,  ..., -3.2239e-01,\n",
      "           -3.0321e-01,  3.7975e-01],\n",
      "          [-4.9161e-01,  2.8530e-01,  5.1852e-01,  ..., -3.5154e-01,\n",
      "           -3.1490e-01,  3.7757e-01],\n",
      "          [-4.2593e-01,  2.4823e-01,  5.5306e-01,  ..., -3.7190e-01,\n",
      "           -3.6387e-01,  3.3638e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3779e-01, -3.2870e-01, -2.0425e-01,  ..., -1.1167e-01,\n",
      "            1.1003e-01,  8.9982e-02],\n",
      "          [-2.2530e-01, -2.5547e-01, -8.6211e-02,  ..., -4.1605e-02,\n",
      "            1.3181e-02,  7.1175e-02],\n",
      "          [-1.8647e-01, -2.7873e-01, -1.3362e-01,  ..., -5.5361e-02,\n",
      "            2.7054e-02,  7.6492e-02],\n",
      "          ...,\n",
      "          [-1.5203e-01, -2.8111e-01, -2.3950e-01,  ..., -7.9329e-02,\n",
      "            8.7425e-02,  1.0622e-01],\n",
      "          [-1.3057e-01, -3.4749e-01, -1.9943e-01,  ..., -5.4055e-02,\n",
      "            1.0944e-01,  9.7536e-02],\n",
      "          [-1.9577e-01, -2.8204e-01, -1.5529e-01,  ...,  5.3945e-03,\n",
      "            1.2113e-01,  7.6523e-02]],\n",
      "\n",
      "         [[-9.3048e-02,  4.9096e-01,  8.8540e-02,  ..., -2.7727e-01,\n",
      "            5.0781e-01,  1.0067e-01],\n",
      "          [-1.2468e-01,  4.5862e-01,  1.0610e-01,  ..., -2.0062e-01,\n",
      "            5.6153e-01,  1.0480e-01],\n",
      "          [-1.7126e-01,  4.9095e-01,  7.3004e-02,  ..., -2.3828e-01,\n",
      "            5.1659e-01,  1.7082e-01],\n",
      "          ...,\n",
      "          [-1.6968e-01,  5.3695e-01,  4.4718e-02,  ..., -1.9172e-01,\n",
      "            5.4643e-01,  5.4539e-02],\n",
      "          [-1.0102e-01,  4.9574e-01,  5.9092e-02,  ..., -2.1160e-01,\n",
      "            5.5484e-01,  1.0205e-01],\n",
      "          [-9.5703e-02,  5.2327e-01,  7.7499e-02,  ..., -2.5148e-01,\n",
      "            5.1906e-01,  1.4168e-01]],\n",
      "\n",
      "         [[ 3.4240e-01, -2.4210e-01, -1.8210e-01,  ...,  3.5595e-01,\n",
      "           -1.5462e-01,  1.3000e+00],\n",
      "          [ 3.7504e-01, -2.7121e-01, -1.5497e-01,  ...,  4.0474e-01,\n",
      "           -1.1132e-01,  1.3164e+00],\n",
      "          [ 2.8220e-01, -2.7145e-01, -1.7423e-01,  ...,  3.2154e-01,\n",
      "           -5.7738e-02,  1.3231e+00],\n",
      "          ...,\n",
      "          [ 2.6906e-01, -2.1991e-01, -1.9483e-01,  ...,  3.7496e-01,\n",
      "           -1.1275e-01,  1.2834e+00],\n",
      "          [ 3.4173e-01, -1.8656e-01, -1.8273e-01,  ...,  3.4358e-01,\n",
      "           -4.9772e-02,  1.3460e+00],\n",
      "          [ 2.6743e-01, -2.5484e-01, -1.8201e-01,  ...,  4.1271e-01,\n",
      "           -1.3556e-01,  1.2940e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8409e-01, -3.3700e-01, -1.8538e-01,  ...,  4.0844e-01,\n",
      "            2.9379e-01,  4.4967e-01],\n",
      "          [-5.3424e-01, -3.2809e-01, -2.5229e-01,  ...,  4.1356e-01,\n",
      "            2.8817e-01,  4.2582e-01],\n",
      "          [-4.8473e-01, -4.7102e-01, -8.7164e-02,  ...,  4.6195e-01,\n",
      "            2.8773e-01,  4.3726e-01],\n",
      "          ...,\n",
      "          [-4.1581e-01, -4.4100e-01, -1.2838e-01,  ...,  4.2034e-01,\n",
      "            3.2771e-01,  5.1508e-01],\n",
      "          [-4.7256e-01, -4.5005e-01, -1.7175e-01,  ...,  4.5724e-01,\n",
      "            2.5058e-01,  4.5192e-01],\n",
      "          [-4.3530e-01, -4.2830e-01, -1.6493e-01,  ...,  4.7415e-01,\n",
      "            2.2973e-01,  4.3635e-01]],\n",
      "\n",
      "         [[-4.7805e-01, -1.2088e-01, -8.7749e-01,  ...,  4.1895e-01,\n",
      "            1.5074e-01,  6.2213e-02],\n",
      "          [-5.1894e-01, -7.4051e-02, -8.9891e-01,  ...,  4.4333e-01,\n",
      "            9.4920e-02,  6.4076e-02],\n",
      "          [-5.7816e-01, -7.3962e-02, -8.4356e-01,  ...,  3.8121e-01,\n",
      "            2.7272e-02,  2.0930e-02],\n",
      "          ...,\n",
      "          [-4.5722e-01, -1.2217e-01, -8.7110e-01,  ...,  4.5392e-01,\n",
      "            9.0703e-02,  1.7630e-01],\n",
      "          [-4.7882e-01, -1.1804e-01, -9.2237e-01,  ...,  4.2918e-01,\n",
      "            5.9152e-02,  9.5024e-02],\n",
      "          [-5.3190e-01, -9.8453e-02, -8.9806e-01,  ...,  4.1783e-01,\n",
      "            6.3281e-02,  9.1455e-02]],\n",
      "\n",
      "         [[-3.2764e-01,  2.8166e-01,  6.3190e-01,  ..., -2.9243e-01,\n",
      "            4.0001e-02,  1.8457e-01],\n",
      "          [-3.5439e-01,  2.7320e-01,  6.0592e-01,  ..., -3.3240e-01,\n",
      "            1.2624e-01,  1.4087e-01],\n",
      "          [-3.2380e-01,  3.0442e-01,  5.9964e-01,  ..., -2.2473e-01,\n",
      "           -2.4370e-02,  2.1388e-01],\n",
      "          ...,\n",
      "          [-4.0230e-01,  2.7486e-01,  6.0795e-01,  ..., -1.9835e-01,\n",
      "            6.5270e-02,  1.9002e-01],\n",
      "          [-3.3329e-01,  3.0339e-01,  5.7414e-01,  ..., -2.6946e-01,\n",
      "            6.8669e-02,  1.8309e-01],\n",
      "          [-3.7908e-01,  2.8416e-01,  5.8582e-01,  ..., -2.5314e-01,\n",
      "            7.4302e-02,  1.7244e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4892e-02, -3.7503e-01, -4.4181e-01,  ..., -8.1802e-03,\n",
      "            2.8447e-03, -2.4041e-02],\n",
      "          [-6.5267e-02, -2.6387e-01, -2.9543e-01,  ...,  4.9203e-02,\n",
      "            4.7027e-02,  1.6179e-01],\n",
      "          [-5.8447e-02, -2.6465e-01, -3.5160e-01,  ...,  2.1582e-02,\n",
      "            6.3024e-02,  1.5709e-02],\n",
      "          ...,\n",
      "          [-2.6362e-02, -2.6203e-01, -3.3682e-01,  ...,  7.9836e-02,\n",
      "            2.0024e-02,  3.0662e-02],\n",
      "          [-8.5857e-02, -2.0518e-01, -2.7022e-01,  ...,  5.4251e-03,\n",
      "           -8.5853e-02,  4.9894e-02],\n",
      "          [ 1.2406e-02, -2.2735e-01, -3.7942e-01,  ...,  4.4911e-02,\n",
      "            6.0520e-02,  2.6625e-04]],\n",
      "\n",
      "         [[-7.3583e-02,  4.8534e-01, -2.4530e-02,  ..., -1.4884e-01,\n",
      "            6.6145e-01,  3.1739e-02],\n",
      "          [-4.5797e-02,  4.9069e-01, -7.0558e-02,  ..., -1.0107e-01,\n",
      "            7.1100e-01, -1.0116e-02],\n",
      "          [-1.0257e-01,  4.3558e-01, -7.5356e-02,  ..., -1.9534e-01,\n",
      "            5.9698e-01, -3.8584e-02],\n",
      "          ...,\n",
      "          [-6.2186e-02,  4.5617e-01,  4.1176e-02,  ..., -1.7271e-01,\n",
      "            6.3029e-01, -2.2773e-02],\n",
      "          [-1.3677e-01,  5.0459e-01, -3.1228e-03,  ..., -1.7486e-01,\n",
      "            7.2093e-01,  3.4729e-04],\n",
      "          [ 1.4725e-02,  4.4998e-01,  1.0582e-01,  ..., -2.0595e-01,\n",
      "            5.9268e-01,  6.5505e-02]],\n",
      "\n",
      "         [[ 3.1632e-01, -2.5295e-01,  1.0412e-01,  ...,  2.9340e-01,\n",
      "           -1.0067e-01,  1.2510e+00],\n",
      "          [ 3.4165e-01, -2.7908e-01,  1.6193e-01,  ...,  2.9696e-01,\n",
      "           -7.2535e-02,  1.2258e+00],\n",
      "          [ 3.2900e-01, -2.3573e-01,  1.6733e-01,  ...,  3.0090e-01,\n",
      "           -3.4162e-02,  1.3186e+00],\n",
      "          ...,\n",
      "          [ 3.1057e-01, -2.3019e-01,  8.1403e-02,  ...,  3.2695e-01,\n",
      "           -1.1494e-01,  1.2415e+00],\n",
      "          [ 2.4982e-01, -2.4105e-01,  1.4220e-01,  ...,  3.0482e-01,\n",
      "           -9.0255e-02,  1.1634e+00],\n",
      "          [ 2.7841e-01, -2.4148e-01,  1.3007e-01,  ...,  3.0963e-01,\n",
      "           -9.9673e-02,  1.1392e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7707e-01, -4.7673e-01, -3.2691e-02,  ...,  1.7059e-01,\n",
      "            2.4932e-01,  3.7531e-01],\n",
      "          [-2.3730e-01, -4.8382e-01, -2.8601e-02,  ...,  1.3589e-01,\n",
      "            2.4396e-01,  4.4411e-01],\n",
      "          [-2.2248e-01, -4.9906e-01, -1.4674e-02,  ...,  1.6181e-01,\n",
      "            3.2347e-01,  4.1100e-01],\n",
      "          ...,\n",
      "          [-3.4506e-01, -4.5634e-01, -2.6819e-02,  ...,  5.6547e-02,\n",
      "            2.5773e-01,  3.6990e-01],\n",
      "          [-3.7563e-01, -4.9403e-01, -4.7810e-02,  ...,  1.8867e-01,\n",
      "            2.5419e-01,  4.0581e-01],\n",
      "          [-3.4618e-01, -4.4759e-01, -1.5358e-01,  ...,  1.7297e-01,\n",
      "            2.4968e-01,  4.6562e-01]],\n",
      "\n",
      "         [[-4.8596e-01, -3.3906e-01, -1.0488e+00,  ...,  4.6673e-01,\n",
      "            1.2365e-01,  1.5604e-01],\n",
      "          [-4.5700e-01, -3.6276e-01, -1.0491e+00,  ...,  4.8757e-01,\n",
      "            8.2874e-02,  1.0045e-01],\n",
      "          [-5.0867e-01, -3.4560e-01, -1.0668e+00,  ...,  4.9860e-01,\n",
      "            7.2005e-02,  1.6192e-01],\n",
      "          ...,\n",
      "          [-4.8933e-01, -3.1689e-01, -1.0742e+00,  ...,  4.5856e-01,\n",
      "            6.3151e-02,  1.0436e-01],\n",
      "          [-4.4868e-01, -3.6700e-01, -1.0556e+00,  ...,  4.7977e-01,\n",
      "            6.4779e-02,  9.0128e-02],\n",
      "          [-4.7978e-01, -3.0958e-01, -1.0915e+00,  ...,  4.3599e-01,\n",
      "            1.1171e-01,  5.7790e-02]],\n",
      "\n",
      "         [[-3.0710e-01,  2.1396e-01,  5.4183e-01,  ..., -2.8296e-01,\n",
      "           -1.0356e-01,  5.4756e-01],\n",
      "          [-3.2224e-01,  2.2996e-01,  6.5846e-01,  ..., -2.7805e-01,\n",
      "           -6.5637e-02,  4.7165e-01],\n",
      "          [-3.6401e-01,  1.9278e-01,  6.4940e-01,  ..., -3.0375e-01,\n",
      "           -8.6344e-02,  5.3162e-01],\n",
      "          ...,\n",
      "          [-3.9074e-01,  2.4252e-01,  6.5315e-01,  ..., -2.2087e-01,\n",
      "           -3.5316e-02,  4.7694e-01],\n",
      "          [-3.7502e-01,  2.5039e-01,  6.6844e-01,  ..., -2.6915e-01,\n",
      "           -4.4601e-02,  5.3042e-01],\n",
      "          [-3.8203e-01,  2.6418e-01,  6.7342e-01,  ..., -2.2872e-01,\n",
      "           -4.8986e-02,  4.9560e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.1708e-03, -2.5261e-01, -3.1722e-01,  ...,  9.5539e-02,\n",
      "            2.9935e-05, -4.3924e-02],\n",
      "          [-1.3039e-02, -1.8058e-01, -2.9851e-01,  ...,  1.0446e-01,\n",
      "           -1.0723e-02, -7.5737e-02],\n",
      "          [-6.2763e-03, -2.0550e-01, -2.8213e-01,  ...,  8.7606e-02,\n",
      "           -3.9342e-02, -8.2026e-02],\n",
      "          ...,\n",
      "          [-7.0411e-02, -2.1181e-01, -3.5891e-01,  ...,  1.3475e-01,\n",
      "            3.1715e-02, -1.2192e-01],\n",
      "          [-1.7181e-02, -2.3196e-01, -3.3382e-01,  ...,  1.2805e-01,\n",
      "            5.2496e-02, -6.4644e-02],\n",
      "          [ 5.5145e-02, -1.7511e-01, -3.6654e-01,  ...,  6.5433e-02,\n",
      "            6.2664e-03, -6.3194e-02]],\n",
      "\n",
      "         [[-3.4676e-02,  4.9442e-01,  1.8200e-01,  ..., -2.0905e-01,\n",
      "            6.1862e-01,  1.3147e-01],\n",
      "          [-6.2283e-02,  4.2589e-01,  1.5809e-01,  ..., -1.6748e-01,\n",
      "            5.6957e-01,  1.2479e-01],\n",
      "          [-5.3011e-02,  4.3237e-01,  1.1593e-01,  ..., -1.8383e-01,\n",
      "            6.2418e-01,  1.6071e-01],\n",
      "          ...,\n",
      "          [-8.4430e-02,  4.4553e-01,  1.0744e-01,  ..., -1.8582e-01,\n",
      "            5.5615e-01,  1.4676e-01],\n",
      "          [-2.1404e-02,  4.3993e-01,  1.4759e-01,  ..., -2.1272e-01,\n",
      "            6.5187e-01,  1.7442e-01],\n",
      "          [-7.2699e-02,  4.8431e-01,  1.5170e-01,  ..., -1.7821e-01,\n",
      "            5.7701e-01,  1.7103e-01]],\n",
      "\n",
      "         [[ 3.6614e-01, -2.3968e-01, -9.2333e-02,  ...,  3.7876e-01,\n",
      "           -3.5893e-02,  1.3795e+00],\n",
      "          [ 3.6061e-01, -2.1784e-01, -8.0280e-02,  ...,  3.8618e-01,\n",
      "           -5.9834e-02,  1.3923e+00],\n",
      "          [ 2.9554e-01, -2.7172e-01, -9.3603e-02,  ...,  3.7314e-01,\n",
      "           -1.4006e-01,  1.4343e+00],\n",
      "          ...,\n",
      "          [ 3.2187e-01, -2.3628e-01, -9.1003e-02,  ...,  3.9848e-01,\n",
      "           -7.9740e-02,  1.3324e+00],\n",
      "          [ 3.3396e-01, -2.3475e-01, -9.2521e-02,  ...,  4.1613e-01,\n",
      "           -7.7778e-02,  1.2856e+00],\n",
      "          [ 3.0940e-01, -2.0701e-01, -7.8523e-02,  ...,  3.8034e-01,\n",
      "           -1.0831e-01,  1.3322e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3478e-01, -3.4641e-01, -8.0529e-02,  ...,  2.0996e-01,\n",
      "            3.6540e-01,  3.0852e-01],\n",
      "          [-2.3162e-01, -3.8698e-01, -7.3905e-02,  ...,  2.4840e-01,\n",
      "            3.1357e-01,  2.7710e-01],\n",
      "          [-2.7712e-01, -3.3085e-01, -9.8461e-02,  ...,  1.9317e-01,\n",
      "            3.6392e-01,  3.1625e-01],\n",
      "          ...,\n",
      "          [-2.7940e-01, -2.0678e-01, -1.2809e-01,  ...,  1.8603e-01,\n",
      "            3.0679e-01,  1.9296e-01],\n",
      "          [-2.4646e-01, -4.0222e-01, -5.8207e-02,  ...,  2.4223e-01,\n",
      "            2.8804e-01,  2.5277e-01],\n",
      "          [-2.0055e-01, -3.5016e-01, -1.1869e-01,  ...,  3.8719e-01,\n",
      "            3.1489e-01,  2.1043e-01]],\n",
      "\n",
      "         [[-4.0290e-01, -2.4077e-01, -8.2719e-01,  ...,  5.5050e-01,\n",
      "           -2.0440e-01,  1.9134e-01],\n",
      "          [-4.7500e-01, -2.5526e-01, -8.9707e-01,  ...,  5.0423e-01,\n",
      "           -2.5623e-01,  1.4322e-01],\n",
      "          [-4.1321e-01, -2.3555e-01, -8.8299e-01,  ...,  5.6007e-01,\n",
      "           -1.1585e-01,  2.1165e-01],\n",
      "          ...,\n",
      "          [-5.9271e-01, -2.3485e-01, -9.5517e-01,  ...,  5.8566e-01,\n",
      "           -7.8728e-02,  2.3254e-01],\n",
      "          [-4.6183e-01, -2.3925e-01, -8.7162e-01,  ...,  6.0809e-01,\n",
      "           -6.6337e-02,  2.3219e-01],\n",
      "          [-3.9313e-01, -2.1016e-01, -9.2074e-01,  ...,  4.4353e-01,\n",
      "           -2.7682e-01,  1.8750e-01]],\n",
      "\n",
      "         [[-5.5487e-01,  3.4695e-01,  6.0578e-01,  ..., -3.4379e-01,\n",
      "           -2.2361e-01,  3.8622e-01],\n",
      "          [-5.0423e-01,  3.5446e-01,  6.0702e-01,  ..., -3.0392e-01,\n",
      "           -2.2402e-01,  4.2789e-01],\n",
      "          [-4.9021e-01,  3.5570e-01,  5.9819e-01,  ..., -3.1599e-01,\n",
      "           -2.6917e-01,  3.9768e-01],\n",
      "          ...,\n",
      "          [-5.1508e-01,  4.0299e-01,  6.8813e-01,  ..., -2.8176e-01,\n",
      "           -2.4053e-01,  4.1987e-01],\n",
      "          [-4.8639e-01,  3.6279e-01,  6.2442e-01,  ..., -3.6929e-01,\n",
      "           -2.4777e-01,  4.5249e-01],\n",
      "          [-5.1565e-01,  3.4236e-01,  6.9414e-01,  ..., -3.5542e-01,\n",
      "           -2.7359e-01,  5.9891e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.3925e-02, -3.1411e-01, -3.1822e-01,  ...,  2.0840e-01,\n",
      "           -3.0842e-03, -6.7818e-02],\n",
      "          [-9.2230e-03, -3.0647e-01, -3.1462e-01,  ...,  9.7032e-02,\n",
      "            7.0689e-02, -1.1519e-02],\n",
      "          [-8.6990e-03, -3.1899e-01, -3.0374e-01,  ...,  1.7464e-01,\n",
      "            2.7705e-02, -4.1443e-02],\n",
      "          ...,\n",
      "          [-1.5398e-02, -3.5131e-01, -3.0988e-01,  ...,  1.9017e-01,\n",
      "           -5.2378e-02, -9.5866e-02],\n",
      "          [-1.0354e-02, -3.2184e-01, -1.9822e-01,  ...,  1.6406e-01,\n",
      "            1.5945e-02, -8.8046e-02],\n",
      "          [-2.5353e-02, -3.0342e-01, -3.1213e-01,  ...,  1.9943e-01,\n",
      "            6.8960e-03, -5.9271e-02]],\n",
      "\n",
      "         [[ 7.9906e-02,  5.7394e-01, -6.0324e-02,  ..., -2.4334e-01,\n",
      "            7.2026e-01,  1.7457e-01],\n",
      "          [-1.1679e-02,  5.2482e-01,  8.8812e-03,  ..., -3.6385e-01,\n",
      "            7.1600e-01,  1.8548e-01],\n",
      "          [ 3.1020e-02,  5.5218e-01, -2.7984e-02,  ..., -3.9135e-01,\n",
      "            6.7803e-01,  1.3232e-01],\n",
      "          ...,\n",
      "          [ 8.0240e-02,  5.2311e-01, -3.9201e-02,  ..., -3.0949e-01,\n",
      "            7.4572e-01,  1.6828e-01],\n",
      "          [ 8.8565e-02,  5.4962e-01, -1.5897e-02,  ..., -3.3751e-01,\n",
      "            7.6318e-01,  1.6426e-01],\n",
      "          [ 1.6485e-02,  5.4528e-01, -1.5504e-02,  ..., -4.0342e-01,\n",
      "            7.7678e-01,  2.2131e-01]],\n",
      "\n",
      "         [[ 3.4717e-01, -1.0113e-01, -1.8136e-01,  ...,  2.8380e-01,\n",
      "           -1.3594e-02,  1.2035e+00],\n",
      "          [ 3.0327e-01, -1.1455e-01, -2.3908e-01,  ...,  3.4089e-01,\n",
      "           -6.1559e-02,  1.2413e+00],\n",
      "          [ 3.5501e-01, -1.2453e-01, -1.3666e-01,  ...,  3.6724e-01,\n",
      "           -1.1171e-01,  1.1806e+00],\n",
      "          ...,\n",
      "          [ 3.4830e-01, -4.6568e-02, -1.5906e-01,  ...,  2.9728e-01,\n",
      "           -4.0138e-02,  1.2235e+00],\n",
      "          [ 2.7214e-01, -7.6402e-02, -2.0034e-01,  ...,  2.7743e-01,\n",
      "           -8.2513e-02,  1.2280e+00],\n",
      "          [ 3.3878e-01, -1.4401e-01, -2.2878e-01,  ...,  2.9588e-01,\n",
      "           -2.0918e-02,  1.2994e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7430e-01, -2.2259e-01, -1.1027e-01,  ...,  2.4942e-01,\n",
      "            2.1210e-01,  5.2163e-01],\n",
      "          [-2.2402e-01, -2.3958e-01, -9.1791e-02,  ...,  2.5435e-01,\n",
      "            1.9601e-01,  5.2630e-01],\n",
      "          [-1.4800e-01, -2.1731e-01, -1.0672e-01,  ...,  2.3636e-01,\n",
      "            1.6340e-01,  5.3629e-01],\n",
      "          ...,\n",
      "          [-1.2352e-01, -2.3287e-01, -9.5883e-02,  ...,  1.8624e-01,\n",
      "            2.1853e-01,  4.4726e-01],\n",
      "          [-7.9140e-02, -2.4159e-01, -1.0659e-01,  ...,  2.4675e-01,\n",
      "            1.6685e-01,  4.6735e-01],\n",
      "          [-1.6309e-01, -2.2786e-01, -6.8044e-02,  ...,  2.6766e-01,\n",
      "            1.5818e-01,  4.5361e-01]],\n",
      "\n",
      "         [[-5.1686e-01, -3.8051e-01, -8.7280e-01,  ...,  5.2182e-01,\n",
      "            1.1169e-02,  2.6246e-01],\n",
      "          [-4.8252e-01, -4.0380e-01, -9.4707e-01,  ...,  4.9325e-01,\n",
      "            2.9079e-02,  3.0238e-01],\n",
      "          [-5.7720e-01, -3.4896e-01, -8.6688e-01,  ...,  5.6454e-01,\n",
      "            4.9859e-02,  2.5000e-01],\n",
      "          ...,\n",
      "          [-5.0363e-01, -3.3677e-01, -8.6474e-01,  ...,  5.0641e-01,\n",
      "            6.0942e-02,  2.6749e-01],\n",
      "          [-4.8803e-01, -3.9745e-01, -8.8552e-01,  ...,  4.9941e-01,\n",
      "            5.8872e-02,  2.8227e-01],\n",
      "          [-5.1174e-01, -3.7675e-01, -8.9312e-01,  ...,  4.9772e-01,\n",
      "            3.7797e-02,  2.7182e-01]],\n",
      "\n",
      "         [[-4.4865e-01,  3.2502e-01,  5.9436e-01,  ..., -4.7468e-01,\n",
      "           -2.4323e-01,  2.8442e-01],\n",
      "          [-4.3698e-01,  3.5704e-01,  5.6993e-01,  ..., -4.3422e-01,\n",
      "           -1.6372e-01,  2.6379e-01],\n",
      "          [-4.1920e-01,  3.5506e-01,  5.3536e-01,  ..., -4.4140e-01,\n",
      "           -1.5485e-01,  2.7806e-01],\n",
      "          ...,\n",
      "          [-4.5118e-01,  3.5884e-01,  5.5418e-01,  ..., -3.9316e-01,\n",
      "           -1.2988e-01,  3.0280e-01],\n",
      "          [-4.3620e-01,  3.7377e-01,  5.3634e-01,  ..., -3.7358e-01,\n",
      "           -2.0569e-01,  2.8695e-01],\n",
      "          [-4.5950e-01,  3.3305e-01,  5.2137e-01,  ..., -3.8933e-01,\n",
      "           -2.2241e-01,  2.8089e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1689e-01, -3.1038e-01, -2.2673e-01,  ...,  9.1894e-02,\n",
      "            1.9930e-01, -1.6183e-01],\n",
      "          [-1.1504e-01, -2.5608e-01, -2.1952e-01,  ...,  5.3584e-02,\n",
      "            1.5467e-01, -2.0597e-01],\n",
      "          [-1.2482e-01, -2.9663e-01, -2.5912e-01,  ...,  1.3404e-01,\n",
      "            2.5526e-01, -1.6772e-01],\n",
      "          ...,\n",
      "          [-1.6814e-02, -3.4922e-01, -2.6235e-01,  ..., -8.5940e-03,\n",
      "            1.9599e-01, -2.3367e-01],\n",
      "          [-1.8467e-01, -2.9700e-01, -2.5448e-01,  ...,  1.0337e-01,\n",
      "            1.9291e-01, -1.9514e-01],\n",
      "          [-7.9809e-02, -3.4820e-01, -2.6056e-01,  ...,  6.1807e-02,\n",
      "            1.7238e-01, -2.4357e-01]],\n",
      "\n",
      "         [[ 1.0303e-02,  4.9489e-01, -2.0287e-02,  ..., -2.4692e-01,\n",
      "            5.7122e-01,  1.2475e-01],\n",
      "          [ 9.3228e-02,  4.6894e-01, -1.8519e-03,  ..., -2.1383e-01,\n",
      "            6.2104e-01,  1.0371e-01],\n",
      "          [ 6.8279e-02,  4.7894e-01, -7.1562e-03,  ..., -2.4605e-01,\n",
      "            6.4495e-01,  1.1313e-01],\n",
      "          ...,\n",
      "          [ 3.8250e-02,  5.0247e-01, -3.9160e-02,  ..., -2.3971e-01,\n",
      "            6.1682e-01,  9.5708e-02],\n",
      "          [ 7.9465e-02,  4.9346e-01, -4.1841e-02,  ..., -2.4328e-01,\n",
      "            6.7866e-01,  4.9822e-02],\n",
      "          [ 6.9116e-02,  5.2978e-01, -4.5366e-02,  ..., -1.4118e-01,\n",
      "            5.9753e-01,  1.4647e-01]],\n",
      "\n",
      "         [[ 2.9808e-01, -1.6647e-01,  7.8468e-02,  ...,  3.3870e-01,\n",
      "           -1.3010e-01,  1.2910e+00],\n",
      "          [ 3.2715e-01, -1.8806e-01,  4.8811e-02,  ...,  2.7889e-01,\n",
      "           -2.2343e-01,  1.2204e+00],\n",
      "          [ 3.3839e-01, -1.3294e-01,  9.9630e-02,  ...,  3.0227e-01,\n",
      "           -1.9900e-01,  1.2428e+00],\n",
      "          ...,\n",
      "          [ 2.5579e-01, -1.4794e-01, -1.5884e-02,  ...,  3.2981e-01,\n",
      "           -1.8359e-01,  1.2755e+00],\n",
      "          [ 3.1701e-01, -1.5243e-01,  1.0684e-01,  ...,  3.1933e-01,\n",
      "           -9.1051e-02,  1.2525e+00],\n",
      "          [ 2.7038e-01, -1.4488e-01,  4.9360e-02,  ...,  3.2610e-01,\n",
      "           -1.5765e-01,  1.2744e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2894e-01, -5.1664e-01, -2.4321e-01,  ...,  4.0297e-01,\n",
      "            3.4041e-01,  3.9900e-01],\n",
      "          [-4.0096e-01, -5.0571e-01, -2.1440e-01,  ...,  3.3289e-01,\n",
      "            3.0463e-01,  3.8431e-01],\n",
      "          [-4.3174e-01, -5.0498e-01, -1.1270e-01,  ...,  3.7998e-01,\n",
      "            2.5646e-01,  4.0917e-01],\n",
      "          ...,\n",
      "          [-3.2897e-01, -5.6925e-01, -2.1446e-01,  ...,  3.9397e-01,\n",
      "            4.1122e-01,  4.0671e-01],\n",
      "          [-3.3319e-01, -5.7197e-01, -1.5601e-01,  ...,  3.4715e-01,\n",
      "            3.2246e-01,  4.5898e-01],\n",
      "          [-3.5542e-01, -5.3274e-01, -1.7597e-01,  ...,  3.8396e-01,\n",
      "            3.4290e-01,  3.7569e-01]],\n",
      "\n",
      "         [[-4.2025e-01, -1.7245e-01, -8.4879e-01,  ...,  5.1024e-01,\n",
      "            3.2119e-02,  2.1434e-01],\n",
      "          [-3.7471e-01, -1.6549e-01, -8.4891e-01,  ...,  5.7592e-01,\n",
      "            2.6288e-02,  1.6035e-01],\n",
      "          [-3.7090e-01, -1.3945e-01, -8.0702e-01,  ...,  5.5628e-01,\n",
      "            3.0017e-02,  2.5076e-01],\n",
      "          ...,\n",
      "          [-4.4913e-01, -2.2330e-01, -8.7426e-01,  ...,  6.4521e-01,\n",
      "            1.3627e-02,  2.2071e-01],\n",
      "          [-4.0629e-01, -2.5501e-01, -8.5142e-01,  ...,  6.4849e-01,\n",
      "           -4.2082e-02,  2.5890e-01],\n",
      "          [-4.4788e-01, -2.6378e-01, -7.5961e-01,  ...,  6.2522e-01,\n",
      "            5.3939e-02,  2.2101e-01]],\n",
      "\n",
      "         [[-4.0804e-01,  3.1764e-01,  4.6060e-01,  ..., -4.3795e-01,\n",
      "           -5.6029e-02,  4.5372e-02],\n",
      "          [-3.6222e-01,  2.8570e-01,  4.9858e-01,  ..., -4.2402e-01,\n",
      "           -2.2760e-02,  1.4626e-01],\n",
      "          [-4.0393e-01,  3.2903e-01,  5.6930e-01,  ..., -4.3114e-01,\n",
      "           -8.9412e-02,  1.0709e-01],\n",
      "          ...,\n",
      "          [-4.4625e-01,  3.0296e-01,  4.9948e-01,  ..., -3.7850e-01,\n",
      "           -3.8147e-02,  1.4707e-01],\n",
      "          [-3.6985e-01,  3.4101e-01,  5.6122e-01,  ..., -4.3351e-01,\n",
      "           -6.1322e-02,  1.3171e-01],\n",
      "          [-3.6814e-01,  2.7484e-01,  5.3476e-01,  ..., -4.4357e-01,\n",
      "           -3.6322e-02,  1.3822e-01]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要进行残差连接？\n",
    "也许你已经注意到，前馈神经网络的输出`ffn_output 的形状： torch.Size([32, 50, 512])`与预处理之后的数据`x = preprocessor(input_ids)`、多头注意力的输出`拼接后的 output 的形状： torch.Size([32, 50, 512])`的形状一致。这让我们想到也许能够将其进行相加之类的操作。\n",
    "\n",
    "**（1）保留原始信息**\n",
    "- 多头注意力机制已经捕捉了序列中元素之间的关系。\n",
    "- 残差连接确保这些信息不会被前馈神经网络完全覆盖，保留原始特征。\n",
    "\n",
    "**（2）缓解梯度消失**\n",
    "- 深层网络中，梯度在反向传播时容易消失。\n",
    "- 残差连接提供了一条“捷径”，使梯度可以直接传播到浅层，缓解梯度消失问题。\n",
    "\n",
    "**（3）增强模型表达能力**\n",
    "- 前馈神经网络引入了非线性变换，增强了模型的表达能力。\n",
    "- 残差连接将这种非线性变换与原始特征结合，进一步提升模型性能。\n",
    "\n",
    "**（4）加速训练**\n",
    "- 残差连接使模型更容易优化，加速训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "norm1 = nn.LayerNorm(d_model)\n",
    "norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "x = norm1(x + projected_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在此之后，这个`x`再经过我们之前提到过的`Feed-Forward`得到的`ffn_output = ffn(projected_output)`进行残差链接，最后将`x`进行层归一化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "x = norm2(x + ffn_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 为什么要层归一化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，主要用于加速训练过程并提高模型的稳定性。以下是详细解释：\n",
    "\n",
    "**. 层归一化的作用**\n",
    "- **归一化特征**：\n",
    "  - 对每个样本的特征进行归一化，使其均值为0，方差为1。\n",
    "  - 减少内部协变量偏移（Internal Covariate Shift），使训练更稳定。\n",
    "- **加速收敛**：\n",
    "  - 归一化后的特征分布更稳定，有助于加速模型收敛。\n",
    "- **适用于不同任务**：\n",
    "  - 特别适合处理变长序列（如NLP任务）和小批量数据。\n",
    "\n",
    "**. 层归一化的公式**\n",
    "层归一化的计算公式如下：\n",
    "```python\n",
    "y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "```\n",
    "- **`x`**：输入特征。\n",
    "- **`mean`**：输入特征的均值。\n",
    "- **`var`**：输入特征的方差。\n",
    "- **`eps`**：一个小常数，用于数值稳定性（默认`1e-5`）。\n",
    "- **`gamma`**：可学习的缩放参数（权重）。\n",
    "- **`beta`**：可学习的偏移参数（偏置）。\n",
    "\n",
    "---\n",
    "\n",
    "**. 层归一化的特点**\n",
    "- **独立于批量大小**：\n",
    "  - 与批量归一化（Batch Normalization）不同，层归一化不依赖于批量大小，适合处理小批量或变长序列。\n",
    "- **逐样本归一化**：\n",
    "  - 对每个样本的特征进行归一化，而不是跨样本归一化。\n",
    "- **可学习的参数**：\n",
    "  - `gamma` 和 `beta` 是可学习的参数，允许模型调整归一化后的特征分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是`Encoder`中一个`EncoderLayer`的全部内容，为了获取`Encoder`，我们需要将`EncoderLayer`堆叠起来。你可以在`transformer_encoder.py`中找到完整的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Decoder 🐱 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **3.1. 解码器的核心职能**\n",
    "解码器的主要任务是**根据编码器的输出和已生成的部分序列，预测下一个单词**。具体来说：\n",
    "- **序列生成**：\n",
    "  - 在机器翻译、文本生成等任务中，解码器逐步生成目标序列。\n",
    "  - 每次生成一个单词，直到生成完整的序列。\n",
    "- **捕捉上下文**：\n",
    "  - 通过自注意力机制捕捉已生成序列的内部关系。\n",
    "  - 通过编码器-解码器注意力机制捕捉源序列与目标序列之间的关系。\n",
    "\n",
    "\n",
    "### **3.2. 解码器的设计原理**\n",
    "解码器的设计基于以下几个关键点：\n",
    "#### **（1）掩码多头自注意力机制（Masked Multi-Head Self-Attention）**\n",
    "- **作用**：\n",
    "  - 捕捉已生成序列的内部关系。\n",
    "  - 通过掩码防止模型看到未来的信息，确保生成过程是自回归的。\n",
    "- **实现**：\n",
    "  - 使用上三角掩码（`tril`）屏蔽未来位置的信息。\n",
    "  - 公式：\n",
    "    ```python\n",
    "    attention = softmax(Q @ K.T / sqrt(d_k) + mask) @ V\n",
    "    ```\n",
    "#### **（2）编码器-解码器注意力机制（Encoder-Decoder Attention）**\n",
    "- **作用**：\n",
    "  - 捕捉源序列（编码器输出）与目标序列之间的关系。\n",
    "  - 帮助解码器更好地理解源序列的信息。\n",
    "- **实现**：\n",
    "  - 查询（Query）来自解码器，键（Key）和值（Value）来自编码器。\n",
    "  - 公式：\n",
    "    ```python\n",
    "    attention = softmax(Q @ K.T / sqrt(d_k)) @ V\n",
    "    ```\n",
    "\n",
    "#### **（3）前馈神经网络（Feed Forward Network）**\n",
    "- **作用**：\n",
    "  - 对注意力机制的输出进行非线性变换。\n",
    "  - 增强模型的表达能力。\n",
    "- **实现**：\n",
    "  - 两层全连接网络，中间使用ReLU激活函数。\n",
    "\n",
    "#### **（4）残差连接和层归一化（Residual Connection & Layer Normalization）**\n",
    "- **作用**：\n",
    "  - 帮助梯度更好地传播，缓解梯度消失问题。\n",
    "  - 加速模型训练，提高稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **3.3. 解码器的工作流程**\n",
    "1. **输入**：\n",
    "   - 目标序列的嵌入表示（已生成的部分序列）。\n",
    "   - 编码器的输出（源序列的特征表示）。\n",
    "2. **掩码多头自注意力**：\n",
    "   - 捕捉已生成序列的内部关系。\n",
    "3. **编码器-解码器注意力**：\n",
    "   - 捕捉源序列与目标序列之间的关系。\n",
    "4. **前馈神经网络**：\n",
    "   - 对注意力机制的输出进行非线性变换。\n",
    "5. **输出**：\n",
    "   - 解码器的最终输出，用于预测下一个单词。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
