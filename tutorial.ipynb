{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[-0.3599, -0.0214,  0.9957],\n",
      "        [ 1.6007, -0.3134,  0.1352],\n",
      "        [-1.1625,  0.0830, -0.1577]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "现在让我们把刚刚提到的`Embedding`和`PositionalEncoding`组合成一个完整的预处理模块`TransformerPreprocessor`。你可以在`transformer_preprocess.py`中找到一样的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征**映射到新的特征空间**。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。我认为在这里，以实用的角度来讲，不必深究`Q`、`K`、`V`的意义，只需要知道它们是通过线性变换得到的即可。`x`的形状是什么，那么`Q`、`K`、`V`的形状就是什么，只不过这些矩阵将他们分别映射到了不同的特征空间。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 原有的输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇在这里，我们使用提到的`TransformermerPreprocessor`来对输入进行预处理，获取一个`x`以便给我们接下来的*线性变换*做示范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformer_preprocess import TransformerPreprocessor\n",
    "\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V，这三个东西（`query`, `key`, `value`）就是我们刚刚提到的三个权重矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[-8.5776e-01, -7.6972e-02,  2.8558e-01,  ..., -8.6545e-01,\n",
      "           4.4641e-01, -5.2227e-02],\n",
      "         [-9.9098e-01,  5.7687e-01, -2.4973e-01,  ..., -7.7310e-01,\n",
      "           1.1275e+00,  3.4499e-01],\n",
      "         [-9.8510e-01, -2.9718e-01,  9.5774e-01,  ...,  5.1740e-01,\n",
      "          -5.0614e-03,  7.0193e-01],\n",
      "         ...,\n",
      "         [-1.4901e+00, -5.8039e-01, -2.5542e-01,  ..., -8.3091e-01,\n",
      "           1.5258e-01, -1.0729e+00],\n",
      "         [-9.7606e-01,  1.5911e+00,  3.2265e-01,  ...,  3.4836e-01,\n",
      "           5.0753e-01, -1.7705e-01],\n",
      "         [-9.5084e-01,  5.7633e-01,  8.1573e-01,  ..., -2.3309e-01,\n",
      "           7.4026e-01,  2.7596e-01]],\n",
      "\n",
      "        [[-1.1441e+00,  9.4711e-01, -1.3115e-01,  ..., -7.5622e-01,\n",
      "           1.0352e+00, -1.0846e+00],\n",
      "         [-9.7510e-01,  4.4153e-01, -4.5516e-01,  ..., -7.3823e-01,\n",
      "           9.6071e-01, -5.7474e-01],\n",
      "         [-1.5991e+00, -3.2219e-01, -8.3190e-01,  ..., -1.2287e+00,\n",
      "           1.7735e+00,  7.2830e-02],\n",
      "         ...,\n",
      "         [-6.5941e-01, -2.8221e-01,  1.7784e+00,  ..., -7.0212e-01,\n",
      "           1.6975e-01, -9.6943e-01],\n",
      "         [-1.5895e+00,  1.2035e-01,  9.6370e-01,  ...,  2.8087e-01,\n",
      "           1.7347e-01, -6.6429e-01],\n",
      "         [-5.5559e-01,  7.1631e-01,  2.1499e-01,  ..., -3.0096e-01,\n",
      "           1.3810e-03, -9.1631e-02]],\n",
      "\n",
      "        [[-2.2485e-01, -2.5269e-01,  4.8770e-01,  ..., -5.4318e-01,\n",
      "           1.0185e+00,  4.2234e-01],\n",
      "         [-1.1508e+00,  3.9385e-02, -5.2324e-02,  ..., -2.5881e-01,\n",
      "          -3.6895e-01,  8.2168e-01],\n",
      "         [-5.1023e-01,  1.7142e-02, -8.9059e-01,  ..., -1.6973e+00,\n",
      "           1.3297e+00, -9.0483e-01],\n",
      "         ...,\n",
      "         [-7.0788e-01,  3.0016e-02,  5.4803e-01,  ...,  2.8622e-01,\n",
      "          -1.7622e-01,  2.1203e-02],\n",
      "         [-1.0590e+00,  2.3386e-01,  2.9449e-01,  ..., -1.2568e+00,\n",
      "          -6.4832e-01, -7.9050e-01],\n",
      "         [-1.9010e+00,  4.5941e-01,  5.2342e-01,  ...,  2.3202e-01,\n",
      "           1.3061e-01, -1.6064e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1793e+00,  2.9622e-01,  2.2546e-01,  ..., -9.2551e-01,\n",
      "           5.6973e-01, -2.4407e-01],\n",
      "         [-9.7140e-01,  3.4212e-01,  2.0710e-01,  ..., -5.2315e-01,\n",
      "           3.5833e-01,  3.1864e-01],\n",
      "         [-1.1400e+00,  1.2939e+00, -6.2289e-02,  ...,  4.6251e-01,\n",
      "           6.7841e-01,  2.4192e-01],\n",
      "         ...,\n",
      "         [-1.3136e+00,  2.6252e-01,  6.8254e-01,  ..., -7.5310e-01,\n",
      "           2.0835e-02, -4.6816e-01],\n",
      "         [-7.1687e-01,  5.1440e-01, -6.9386e-01,  ...,  4.0823e-01,\n",
      "           6.7291e-01, -1.1046e+00],\n",
      "         [-1.0032e+00, -9.1211e-01, -1.9553e-01,  ...,  7.8141e-02,\n",
      "           3.1115e-01, -5.2637e-01]],\n",
      "\n",
      "        [[-7.5653e-01,  3.5612e-01, -1.5124e-01,  ..., -5.1700e-01,\n",
      "           1.1600e+00,  6.7163e-01],\n",
      "         [-9.4307e-01,  3.3543e-01, -1.0387e+00,  ..., -6.3023e-01,\n",
      "           8.5864e-02,  9.7131e-01],\n",
      "         [-1.1123e+00,  2.0244e+00,  6.3792e-01,  ...,  3.1637e-01,\n",
      "           5.1620e-01,  5.2664e-02],\n",
      "         ...,\n",
      "         [-1.2968e+00,  4.4886e-01,  8.8202e-01,  ..., -2.2552e-01,\n",
      "          -2.8429e-01,  9.3865e-01],\n",
      "         [-9.9460e-01, -6.0186e-01,  1.1743e+00,  ..., -1.4696e-01,\n",
      "          -3.4131e-02, -4.0044e-01],\n",
      "         [-1.2678e+00, -9.7055e-01,  3.8597e-01,  ...,  6.8540e-02,\n",
      "          -1.0385e+00,  2.8249e-01]],\n",
      "\n",
      "        [[-1.7129e+00,  6.8516e-01,  4.1531e-01,  ...,  2.1418e-01,\n",
      "          -1.7657e-02, -6.7713e-01],\n",
      "         [-3.0421e-01,  1.0053e+00, -2.0264e-01,  ..., -1.4788e+00,\n",
      "           1.3474e+00, -2.2222e-01],\n",
      "         [-7.3214e-01,  3.6199e-01,  3.2718e-01,  ...,  3.5032e-01,\n",
      "           7.5576e-01,  1.0162e+00],\n",
      "         ...,\n",
      "         [-1.4892e+00, -2.7818e-01,  1.1576e+00,  ...,  3.1028e-01,\n",
      "           3.0979e-01, -2.4512e-01],\n",
      "         [ 9.1923e-02,  2.8989e-01,  1.3658e+00,  ...,  1.8389e-01,\n",
      "           1.4570e+00,  5.1869e-01],\n",
      "         [-6.0732e-01,  8.3569e-01,  8.8240e-01,  ..., -1.1992e-01,\n",
      "           4.5501e-01, -9.1151e-02]]], grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[-8.5625e-01, -7.0683e-02,  2.0474e-01,  ...,  2.8418e-01,\n",
      "          -5.5728e-01,  5.8978e-02],\n",
      "         [ 1.7286e-01,  1.8412e-01,  7.7099e-01,  ..., -4.5525e-01,\n",
      "           1.8215e-01,  2.9697e-01],\n",
      "         [-1.5496e-01,  8.2147e-01, -2.1320e-01,  ...,  1.7644e-01,\n",
      "           7.1941e-01,  2.1800e-01],\n",
      "         ...,\n",
      "         [-6.2824e-01, -8.4762e-01,  6.7453e-01,  ..., -1.3878e-01,\n",
      "          -3.9232e-01, -4.9515e-01],\n",
      "         [ 9.4137e-02, -9.9228e-02, -1.0389e-01,  ..., -9.1974e-02,\n",
      "          -7.8743e-01,  5.7779e-01],\n",
      "         [ 7.5900e-01,  1.3192e+00,  9.4914e-01,  ...,  4.1263e-01,\n",
      "          -1.1173e+00, -2.4979e-01]],\n",
      "\n",
      "        [[-1.2149e+00,  5.9425e-02,  2.1356e-02,  ..., -5.3410e-01,\n",
      "           1.0273e+00, -7.4068e-01],\n",
      "         [-6.3079e-01, -3.2922e-01,  1.2377e+00,  ..., -1.2052e+00,\n",
      "          -7.4165e-03, -8.7035e-01],\n",
      "         [-1.1368e+00,  4.9737e-01,  8.9798e-01,  ..., -8.1960e-01,\n",
      "           6.5268e-01, -5.5647e-01],\n",
      "         ...,\n",
      "         [-1.1885e-01, -1.4465e-01, -1.9263e-02,  ...,  9.8770e-01,\n",
      "           6.7441e-01,  1.1100e-02],\n",
      "         [ 1.3134e-03,  1.2615e+00,  9.3676e-01,  ...,  7.0573e-01,\n",
      "          -1.2661e-01,  9.2449e-01],\n",
      "         [ 1.0827e+00,  1.9559e-01, -4.8314e-01,  ...,  1.5398e+00,\n",
      "          -6.8292e-01,  5.5729e-01]],\n",
      "\n",
      "        [[ 7.7673e-01,  1.9515e-03,  8.2396e-01,  ...,  1.1251e+00,\n",
      "          -4.2959e-01, -3.3544e-01],\n",
      "         [-1.9794e-01, -1.2724e+00,  7.9465e-01,  ...,  4.6893e-02,\n",
      "          -7.1927e-02, -3.4511e-01],\n",
      "         [-7.3621e-01, -3.0274e-01,  8.1935e-01,  ..., -1.8757e-01,\n",
      "           6.7591e-01, -7.5460e-02],\n",
      "         ...,\n",
      "         [ 4.8079e-01,  7.3016e-01,  1.7421e+00,  ...,  4.6656e-01,\n",
      "          -1.7532e-01, -8.1412e-01],\n",
      "         [ 7.8468e-03,  1.2183e+00, -4.8844e-01,  ...,  1.1448e+00,\n",
      "           6.5926e-01,  2.0242e-01],\n",
      "         [ 4.2884e-01,  1.0220e-01, -6.3610e-01,  ...,  2.6767e-01,\n",
      "          -1.0875e+00,  6.5819e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8.2201e-01,  3.1763e-02,  3.2788e-01,  ..., -5.9610e-01,\n",
      "           1.2065e+00,  3.2211e-01],\n",
      "         [-3.5196e-01, -7.8400e-01, -2.0682e-01,  ...,  4.1155e-02,\n",
      "           8.8412e-01, -4.4900e-01],\n",
      "         [-7.2405e-01, -1.1174e-01, -7.5192e-01,  ..., -7.8260e-01,\n",
      "           1.0979e-01,  2.3100e-01],\n",
      "         ...,\n",
      "         [ 6.7884e-01,  1.8513e-02,  5.6388e-01,  ..., -1.3022e-01,\n",
      "          -1.9812e-01,  4.0057e-01],\n",
      "         [ 7.6721e-01,  7.0930e-01,  2.8267e-01,  ...,  3.5782e-01,\n",
      "           8.7113e-01, -2.6667e-01],\n",
      "         [ 7.8871e-01,  5.6969e-01,  4.6959e-01,  ..., -5.2547e-01,\n",
      "          -1.4533e+00, -3.5601e-03]],\n",
      "\n",
      "        [[ 3.0763e-01, -2.6168e-01, -6.5360e-01,  ..., -6.7849e-01,\n",
      "          -5.3049e-01, -3.6295e-01],\n",
      "         [ 1.9178e-01, -3.9322e-02,  6.0044e-01,  ..., -8.3324e-01,\n",
      "           9.1934e-02, -5.0038e-01],\n",
      "         [ 4.4801e-01,  7.0234e-03,  4.8769e-01,  ..., -7.3763e-01,\n",
      "           7.8395e-01,  4.3606e-01],\n",
      "         ...,\n",
      "         [ 1.2620e+00,  1.1696e+00,  1.7901e-02,  ...,  4.3280e-01,\n",
      "          -4.7169e-01,  5.6411e-01],\n",
      "         [-2.7216e-01,  9.1128e-02, -7.0922e-02,  ..., -3.3054e-01,\n",
      "          -4.6610e-01,  8.4282e-01],\n",
      "         [-1.4818e-01,  1.9649e-01,  5.0984e-01,  ...,  7.2760e-01,\n",
      "          -3.3459e-01, -1.2862e+00]],\n",
      "\n",
      "        [[ 9.2525e-02, -5.9169e-01, -2.5859e-01,  ..., -1.1132e-01,\n",
      "          -3.9413e-01, -2.7029e-01],\n",
      "         [-1.6376e-01,  1.9922e-01,  1.2921e+00,  ...,  6.0549e-01,\n",
      "           1.7208e-01, -1.3719e+00],\n",
      "         [-7.3220e-01, -6.9492e-02,  8.1207e-01,  ..., -8.2451e-01,\n",
      "          -1.0451e+00,  4.3561e-01],\n",
      "         ...,\n",
      "         [-2.7230e-01,  1.6550e-01, -2.9073e-01,  ..., -7.5313e-01,\n",
      "           9.9037e-01,  4.0870e-01],\n",
      "         [ 1.0791e-01, -3.8652e-01,  7.6520e-01,  ...,  9.9309e-01,\n",
      "          -3.3259e-01, -1.0058e+00],\n",
      "         [ 6.2120e-01, -9.6021e-01,  4.3997e-01,  ..., -2.7142e-01,\n",
      "          -6.0369e-01, -9.9234e-02]]], grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[ 5.4836e-01,  5.8954e-01, -6.9160e-01,  ..., -2.4894e-01,\n",
      "           6.5563e-01,  6.0935e-01],\n",
      "         [ 1.1487e+00,  7.2434e-01, -2.0657e+00,  ..., -1.3062e-01,\n",
      "          -7.6899e-01,  6.5420e-01],\n",
      "         [ 2.5838e-03,  3.8615e-01, -1.3386e+00,  ..., -9.3806e-01,\n",
      "           1.3058e-01,  5.4977e-01],\n",
      "         ...,\n",
      "         [-2.8215e-01,  3.3489e-01, -9.6489e-01,  ...,  1.0408e-01,\n",
      "          -1.2239e+00, -8.7672e-01],\n",
      "         [ 7.8919e-01, -4.8723e-01, -8.3637e-01,  ..., -4.5979e-01,\n",
      "           4.0188e-01, -9.2975e-01],\n",
      "         [-4.6051e-01, -2.1624e-02, -6.2448e-01,  ...,  2.9688e-01,\n",
      "          -9.1944e-01, -1.3732e+00]],\n",
      "\n",
      "        [[ 9.4222e-01,  4.1867e-01, -4.1899e-01,  ...,  3.4325e-01,\n",
      "           9.2813e-02,  8.0209e-02],\n",
      "         [-1.5394e-01, -4.1523e-01, -2.0433e+00,  ...,  9.0473e-01,\n",
      "          -9.5208e-01, -1.2277e-01],\n",
      "         [ 1.0072e-01, -6.6373e-01, -1.9853e+00,  ...,  1.2306e+00,\n",
      "          -2.1293e-01,  1.3420e+00],\n",
      "         ...,\n",
      "         [ 1.9932e-01, -9.9383e-01,  6.9256e-02,  ...,  6.1482e-01,\n",
      "           2.5353e-01, -1.3039e-03],\n",
      "         [ 4.3032e-01, -6.0446e-01, -7.7939e-01,  ..., -5.9913e-01,\n",
      "           4.8851e-02, -7.5105e-02],\n",
      "         [-7.4214e-02, -1.4105e+00, -4.9702e-01,  ...,  7.0217e-01,\n",
      "           8.7267e-01, -9.4189e-01]],\n",
      "\n",
      "        [[ 4.2758e-01,  5.1459e-01, -1.6969e-01,  ...,  7.1570e-02,\n",
      "           4.1841e-01,  7.1560e-01],\n",
      "         [ 5.9539e-01,  1.3200e+00, -2.4024e+00,  ..., -6.3172e-02,\n",
      "          -2.4816e-01, -1.3230e+00],\n",
      "         [-1.8508e-02,  1.9892e-01, -5.9342e-01,  ..., -5.9099e-02,\n",
      "           2.0441e-01, -8.4854e-01],\n",
      "         ...,\n",
      "         [-3.8233e-01,  3.1381e-01,  3.1775e-01,  ..., -6.9738e-01,\n",
      "           1.2948e-01, -1.4329e+00],\n",
      "         [-7.8678e-02, -1.2685e+00, -1.3326e-01,  ..., -1.8166e-01,\n",
      "           2.6293e-01, -9.2947e-02],\n",
      "         [-1.1935e-01, -1.0564e+00,  1.8709e-01,  ..., -9.7615e-01,\n",
      "          -7.1877e-01, -1.1055e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.3960e-01, -4.2571e-01, -4.4908e-01,  ..., -5.3288e-01,\n",
      "           7.1299e-01,  8.0681e-01],\n",
      "         [-3.7700e-01, -8.8254e-01, -1.1911e+00,  ..., -4.4681e-01,\n",
      "          -5.0240e-01, -8.2205e-01],\n",
      "         [-4.9426e-01,  9.0124e-01, -6.8012e-01,  ..., -3.9258e-01,\n",
      "          -4.7550e-01, -2.7432e-01],\n",
      "         ...,\n",
      "         [ 8.9827e-01, -1.5236e+00,  1.4293e-01,  ...,  5.0914e-01,\n",
      "          -9.6661e-02,  3.5546e-01],\n",
      "         [ 6.1562e-01, -1.9414e-01, -1.0622e+00,  ...,  2.0039e-01,\n",
      "          -5.0957e-01, -2.7915e-01],\n",
      "         [ 3.6960e-01, -8.3765e-01, -6.5769e-01,  ..., -6.4034e-01,\n",
      "          -4.3770e-02, -1.4603e+00]],\n",
      "\n",
      "        [[ 7.9057e-02,  6.0322e-01, -1.9171e+00,  ..., -4.1254e-01,\n",
      "           1.0967e+00, -5.4324e-01],\n",
      "         [ 5.2956e-01,  7.7001e-01, -1.2337e+00,  ..., -2.2588e-01,\n",
      "          -2.2205e-01,  6.7475e-01],\n",
      "         [-3.1293e-01,  4.3403e-01, -1.7192e+00,  ..., -4.0194e-01,\n",
      "          -1.3739e-01,  3.6896e-01],\n",
      "         ...,\n",
      "         [-3.4126e-01,  5.7239e-01, -1.2430e-02,  ..., -2.8013e-01,\n",
      "          -3.4469e-01, -6.6363e-01],\n",
      "         [ 2.4366e-01,  1.9787e-01, -3.2841e-01,  ...,  2.8971e-01,\n",
      "           4.3424e-01,  6.5282e-01],\n",
      "         [-3.5586e-01, -8.4608e-01,  3.7812e-01,  ..., -1.0419e+00,\n",
      "          -6.3380e-01, -4.8799e-01]],\n",
      "\n",
      "        [[ 2.0239e-01, -3.6481e-01, -4.4157e-01,  ..., -1.0896e-01,\n",
      "           1.1771e+00,  7.4697e-02],\n",
      "         [ 1.2942e-01,  1.2751e-01, -1.2166e+00,  ...,  6.0094e-01,\n",
      "          -1.3836e-01,  9.1814e-02],\n",
      "         [-4.6080e-01,  1.6287e-02, -1.6932e+00,  ...,  6.3411e-01,\n",
      "          -1.1189e+00,  9.5210e-01],\n",
      "         ...,\n",
      "         [-5.3407e-01, -4.3548e-01,  1.6711e-01,  ...,  7.4189e-01,\n",
      "           6.3821e-01, -1.4528e+00],\n",
      "         [-5.0191e-01,  1.4452e-01,  1.8792e-01,  ...,  7.7101e-01,\n",
      "           4.4467e-01, -1.1674e+00],\n",
      "         [ 6.4409e-01, -9.7501e-01, -3.5448e-01,  ..., -3.9703e-01,\n",
      "           8.9274e-01, -1.8638e+00]]], grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### 1. **分割多头的目的**\n",
    "通过观察我们知道，刚刚得到的`Q/K/V`都是形如`torch.Size([32, 50, 512])`的矩阵。我们当然可以直接将它们作为输入，但这样做会导致模型关注相同的特征，而无法捕捉不同特征。Personally，我认为Attention原论文作者不想这么做的原因是：\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。这是transformer称霸NLP界的一个重要原因，它能高效利用GPU计算。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成，我们在这里确认一下。\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这段代码中，`transpose(1, 2)`的作用是交换张量的第二个和第三个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim。在这我使用view方法对张量进行调整。\n",
    "# view之后，我们得到一个形状为(batch_size, seq_len, num_heads, head_dim)的张量。也就是torch.Size([32, 50, 16, 32])\n",
    "# 使用transpose将`num_heads`维度提到前面，方便后续并行计算，这时形状为(batch_size, num_heads, seq_len, head_dim)，也就是torch.Size([32, 16, 50, 32])。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "通过转置K，我们使得Q的每一行可以与K的每一列进行点积，从而计算出每个位置与其他所有位置的相关性得分。这正是自注意力机制所需要的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 解释缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0252, 0.0352, 0.0151,  ..., 0.0190, 0.0220, 0.0201],\n",
      "          [0.0154, 0.0126, 0.0109,  ..., 0.0392, 0.0211, 0.0095],\n",
      "          [0.0162, 0.0274, 0.0122,  ..., 0.0382, 0.0191, 0.0147],\n",
      "          ...,\n",
      "          [0.0085, 0.0266, 0.0199,  ..., 0.0293, 0.0301, 0.0116],\n",
      "          [0.0133, 0.0308, 0.0142,  ..., 0.0434, 0.0341, 0.0114],\n",
      "          [0.0077, 0.0538, 0.0419,  ..., 0.0139, 0.0208, 0.0159]],\n",
      "\n",
      "         [[0.0112, 0.0162, 0.0132,  ..., 0.0365, 0.0228, 0.0372],\n",
      "          [0.0108, 0.0160, 0.0071,  ..., 0.0206, 0.0314, 0.0278],\n",
      "          [0.0065, 0.0087, 0.0072,  ..., 0.0224, 0.0454, 0.0220],\n",
      "          ...,\n",
      "          [0.0295, 0.0145, 0.0373,  ..., 0.0113, 0.0108, 0.0135],\n",
      "          [0.0168, 0.0115, 0.0423,  ..., 0.0120, 0.0108, 0.0118],\n",
      "          [0.0133, 0.0069, 0.0251,  ..., 0.0096, 0.0156, 0.0078]],\n",
      "\n",
      "         [[0.0110, 0.0242, 0.0174,  ..., 0.0172, 0.0197, 0.0174],\n",
      "          [0.0153, 0.0209, 0.0131,  ..., 0.0170, 0.0179, 0.0078],\n",
      "          [0.0289, 0.0407, 0.0070,  ..., 0.0156, 0.0188, 0.0112],\n",
      "          ...,\n",
      "          [0.0102, 0.0259, 0.0112,  ..., 0.0151, 0.0132, 0.0117],\n",
      "          [0.0101, 0.0318, 0.0105,  ..., 0.0115, 0.0178, 0.0097],\n",
      "          [0.0175, 0.0339, 0.0196,  ..., 0.0264, 0.0183, 0.0227]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0347, 0.0438, 0.0169,  ..., 0.0058, 0.0171, 0.0157],\n",
      "          [0.0169, 0.0162, 0.0241,  ..., 0.0145, 0.0111, 0.0186],\n",
      "          [0.0151, 0.0358, 0.0195,  ..., 0.0068, 0.0217, 0.0095],\n",
      "          ...,\n",
      "          [0.0211, 0.0258, 0.0173,  ..., 0.0115, 0.0460, 0.0206],\n",
      "          [0.0157, 0.0173, 0.0083,  ..., 0.0198, 0.0147, 0.0348],\n",
      "          [0.0243, 0.0092, 0.0103,  ..., 0.0163, 0.0123, 0.0228]],\n",
      "\n",
      "         [[0.0137, 0.0139, 0.0211,  ..., 0.0184, 0.0082, 0.0242],\n",
      "          [0.0202, 0.0202, 0.0252,  ..., 0.0146, 0.0121, 0.0249],\n",
      "          [0.0186, 0.0127, 0.0257,  ..., 0.0120, 0.0151, 0.0137],\n",
      "          ...,\n",
      "          [0.0223, 0.0140, 0.0326,  ..., 0.0265, 0.0105, 0.0111],\n",
      "          [0.0163, 0.0136, 0.0358,  ..., 0.0180, 0.0085, 0.0149],\n",
      "          [0.0209, 0.0127, 0.0375,  ..., 0.0374, 0.0060, 0.0085]],\n",
      "\n",
      "         [[0.0113, 0.0136, 0.0162,  ..., 0.0204, 0.0234, 0.0104],\n",
      "          [0.0125, 0.0135, 0.0378,  ..., 0.0306, 0.0283, 0.0202],\n",
      "          [0.0104, 0.0174, 0.0336,  ..., 0.0363, 0.0259, 0.0151],\n",
      "          ...,\n",
      "          [0.0057, 0.0218, 0.0145,  ..., 0.0289, 0.0192, 0.0146],\n",
      "          [0.0111, 0.0241, 0.0199,  ..., 0.0125, 0.0179, 0.0161],\n",
      "          [0.0114, 0.0191, 0.0086,  ..., 0.0182, 0.0167, 0.0183]]],\n",
      "\n",
      "\n",
      "        [[[0.0090, 0.0249, 0.0181,  ..., 0.0210, 0.0266, 0.0223],\n",
      "          [0.0220, 0.0241, 0.0252,  ..., 0.0208, 0.0333, 0.0162],\n",
      "          [0.0110, 0.0143, 0.0384,  ..., 0.0171, 0.0263, 0.0170],\n",
      "          ...,\n",
      "          [0.0397, 0.0200, 0.0139,  ..., 0.0252, 0.0152, 0.0263],\n",
      "          [0.0276, 0.0333, 0.0209,  ..., 0.0160, 0.0081, 0.0236],\n",
      "          [0.0594, 0.0187, 0.0194,  ..., 0.0328, 0.0190, 0.0341]],\n",
      "\n",
      "         [[0.0136, 0.0278, 0.0201,  ..., 0.0156, 0.0233, 0.0639],\n",
      "          [0.0185, 0.0132, 0.0228,  ..., 0.0149, 0.0173, 0.0171],\n",
      "          [0.0109, 0.0097, 0.0164,  ..., 0.0107, 0.0220, 0.0180],\n",
      "          ...,\n",
      "          [0.0184, 0.0162, 0.0245,  ..., 0.0101, 0.0144, 0.0312],\n",
      "          [0.0076, 0.0110, 0.0155,  ..., 0.0116, 0.0156, 0.0123],\n",
      "          [0.0125, 0.0151, 0.0123,  ..., 0.0143, 0.0151, 0.0236]],\n",
      "\n",
      "         [[0.0238, 0.0363, 0.0102,  ..., 0.0123, 0.0144, 0.0129],\n",
      "          [0.0188, 0.0119, 0.0088,  ..., 0.0199, 0.0213, 0.0204],\n",
      "          [0.0101, 0.0203, 0.0209,  ..., 0.0148, 0.0130, 0.0034],\n",
      "          ...,\n",
      "          [0.0087, 0.0175, 0.0096,  ..., 0.0254, 0.0399, 0.0181],\n",
      "          [0.0150, 0.0129, 0.0096,  ..., 0.0151, 0.0108, 0.0112],\n",
      "          [0.0177, 0.0305, 0.0224,  ..., 0.0122, 0.0179, 0.0112]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0356, 0.0191, 0.0132,  ..., 0.0132, 0.0176, 0.0256],\n",
      "          [0.0469, 0.0439, 0.0132,  ..., 0.0122, 0.0134, 0.0157],\n",
      "          [0.0224, 0.0249, 0.0098,  ..., 0.0131, 0.0114, 0.0119],\n",
      "          ...,\n",
      "          [0.0325, 0.0204, 0.0076,  ..., 0.0256, 0.0106, 0.0165],\n",
      "          [0.0131, 0.0197, 0.0146,  ..., 0.0236, 0.0171, 0.0131],\n",
      "          [0.0386, 0.0394, 0.0115,  ..., 0.0207, 0.0093, 0.0059]],\n",
      "\n",
      "         [[0.0251, 0.0166, 0.0098,  ..., 0.0354, 0.0336, 0.0220],\n",
      "          [0.0143, 0.0273, 0.0116,  ..., 0.0149, 0.0118, 0.0174],\n",
      "          [0.0099, 0.0125, 0.0064,  ..., 0.0199, 0.0144, 0.0131],\n",
      "          ...,\n",
      "          [0.0276, 0.0106, 0.0058,  ..., 0.0227, 0.0090, 0.0156],\n",
      "          [0.0098, 0.0088, 0.0092,  ..., 0.0194, 0.0320, 0.0182],\n",
      "          [0.0377, 0.0102, 0.0176,  ..., 0.0168, 0.0057, 0.0111]],\n",
      "\n",
      "         [[0.0295, 0.0319, 0.0190,  ..., 0.0137, 0.0148, 0.0153],\n",
      "          [0.0152, 0.0132, 0.0232,  ..., 0.0150, 0.0246, 0.0137],\n",
      "          [0.0196, 0.0173, 0.0135,  ..., 0.0147, 0.0162, 0.0097],\n",
      "          ...,\n",
      "          [0.0252, 0.0175, 0.0167,  ..., 0.0085, 0.0144, 0.0121],\n",
      "          [0.0265, 0.0177, 0.0363,  ..., 0.0098, 0.0281, 0.0182],\n",
      "          [0.0289, 0.0165, 0.0273,  ..., 0.0131, 0.0121, 0.0209]]],\n",
      "\n",
      "\n",
      "        [[[0.0251, 0.0269, 0.0292,  ..., 0.0275, 0.0113, 0.0292],\n",
      "          [0.0275, 0.0219, 0.0188,  ..., 0.0220, 0.0102, 0.0302],\n",
      "          [0.0141, 0.0053, 0.0068,  ..., 0.0113, 0.0066, 0.0085],\n",
      "          ...,\n",
      "          [0.0147, 0.0124, 0.0147,  ..., 0.0264, 0.0154, 0.0217],\n",
      "          [0.0119, 0.0444, 0.0355,  ..., 0.0107, 0.0172, 0.0119],\n",
      "          [0.0201, 0.0356, 0.0191,  ..., 0.0366, 0.0142, 0.0173]],\n",
      "\n",
      "         [[0.0133, 0.0303, 0.0464,  ..., 0.0143, 0.0194, 0.0252],\n",
      "          [0.0084, 0.0206, 0.0201,  ..., 0.0253, 0.0125, 0.0258],\n",
      "          [0.0127, 0.0153, 0.0301,  ..., 0.0328, 0.0252, 0.0307],\n",
      "          ...,\n",
      "          [0.0129, 0.0155, 0.0102,  ..., 0.0137, 0.0093, 0.0089],\n",
      "          [0.0099, 0.0106, 0.0187,  ..., 0.0273, 0.0207, 0.0201],\n",
      "          [0.0105, 0.0080, 0.0182,  ..., 0.0195, 0.0294, 0.0169]],\n",
      "\n",
      "         [[0.0107, 0.0052, 0.0233,  ..., 0.0094, 0.0143, 0.0083],\n",
      "          [0.0206, 0.0110, 0.0148,  ..., 0.0343, 0.0192, 0.0163],\n",
      "          [0.0152, 0.0083, 0.0300,  ..., 0.0309, 0.0147, 0.0189],\n",
      "          ...,\n",
      "          [0.0100, 0.0088, 0.0194,  ..., 0.0115, 0.0156, 0.0162],\n",
      "          [0.0121, 0.0095, 0.0145,  ..., 0.0225, 0.0251, 0.0162],\n",
      "          [0.0165, 0.0070, 0.0176,  ..., 0.0227, 0.0100, 0.0155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0228, 0.0321, 0.0203,  ..., 0.0283, 0.0124, 0.0181],\n",
      "          [0.0146, 0.0203, 0.0202,  ..., 0.0342, 0.0169, 0.0129],\n",
      "          [0.0100, 0.0232, 0.0214,  ..., 0.0339, 0.0205, 0.0184],\n",
      "          ...,\n",
      "          [0.0080, 0.0220, 0.0084,  ..., 0.0239, 0.0124, 0.0230],\n",
      "          [0.0086, 0.0223, 0.0097,  ..., 0.0161, 0.0120, 0.0325],\n",
      "          [0.0055, 0.0371, 0.0089,  ..., 0.0345, 0.0144, 0.0294]],\n",
      "\n",
      "         [[0.0213, 0.0198, 0.0223,  ..., 0.0086, 0.0155, 0.0278],\n",
      "          [0.0209, 0.0168, 0.0187,  ..., 0.0093, 0.0198, 0.0138],\n",
      "          [0.0225, 0.0113, 0.0135,  ..., 0.0182, 0.0180, 0.0240],\n",
      "          ...,\n",
      "          [0.0155, 0.0070, 0.0477,  ..., 0.0070, 0.0127, 0.0093],\n",
      "          [0.0125, 0.0225, 0.0147,  ..., 0.0274, 0.0148, 0.0529],\n",
      "          [0.0110, 0.0142, 0.0159,  ..., 0.0169, 0.0146, 0.0190]],\n",
      "\n",
      "         [[0.0088, 0.0108, 0.0136,  ..., 0.0169, 0.0163, 0.0186],\n",
      "          [0.0140, 0.0195, 0.0253,  ..., 0.0150, 0.0126, 0.0092],\n",
      "          [0.0137, 0.0214, 0.0201,  ..., 0.0087, 0.0283, 0.0326],\n",
      "          ...,\n",
      "          [0.0136, 0.0273, 0.0378,  ..., 0.0213, 0.0254, 0.0533],\n",
      "          [0.0289, 0.0553, 0.0151,  ..., 0.0103, 0.0263, 0.0321],\n",
      "          [0.0165, 0.0256, 0.0131,  ..., 0.0108, 0.0243, 0.0133]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0309, 0.0168, 0.0218,  ..., 0.0229, 0.0190, 0.0181],\n",
      "          [0.0173, 0.0122, 0.0208,  ..., 0.0170, 0.0201, 0.0195],\n",
      "          [0.0325, 0.0110, 0.0353,  ..., 0.0172, 0.0356, 0.0129],\n",
      "          ...,\n",
      "          [0.0088, 0.0255, 0.0254,  ..., 0.0224, 0.0159, 0.0096],\n",
      "          [0.0182, 0.0204, 0.0155,  ..., 0.0356, 0.0253, 0.0156],\n",
      "          [0.0398, 0.0209, 0.0132,  ..., 0.0148, 0.0077, 0.0174]],\n",
      "\n",
      "         [[0.0144, 0.0197, 0.0233,  ..., 0.0259, 0.0217, 0.0267],\n",
      "          [0.0235, 0.0150, 0.0195,  ..., 0.0226, 0.0236, 0.0186],\n",
      "          [0.0203, 0.0149, 0.0207,  ..., 0.0182, 0.0272, 0.0293],\n",
      "          ...,\n",
      "          [0.0210, 0.0101, 0.0151,  ..., 0.0200, 0.0212, 0.0272],\n",
      "          [0.0235, 0.0203, 0.0220,  ..., 0.0263, 0.0198, 0.0189],\n",
      "          [0.0247, 0.0171, 0.0169,  ..., 0.0125, 0.0189, 0.0202]],\n",
      "\n",
      "         [[0.0187, 0.0297, 0.0105,  ..., 0.0202, 0.0203, 0.0295],\n",
      "          [0.0061, 0.0064, 0.0108,  ..., 0.0398, 0.0305, 0.0308],\n",
      "          [0.0286, 0.0173, 0.0264,  ..., 0.0261, 0.0189, 0.0143],\n",
      "          ...,\n",
      "          [0.0213, 0.0138, 0.0074,  ..., 0.0141, 0.0096, 0.0206],\n",
      "          [0.0103, 0.0128, 0.0097,  ..., 0.0164, 0.0076, 0.0134],\n",
      "          [0.0081, 0.0097, 0.0103,  ..., 0.0297, 0.0269, 0.0154]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0220, 0.0239, 0.0254,  ..., 0.0145, 0.0113, 0.0150],\n",
      "          [0.0133, 0.0221, 0.0246,  ..., 0.0144, 0.0115, 0.0234],\n",
      "          [0.0157, 0.0100, 0.0366,  ..., 0.0273, 0.0286, 0.0258],\n",
      "          ...,\n",
      "          [0.0129, 0.0087, 0.0217,  ..., 0.0163, 0.0120, 0.0067],\n",
      "          [0.0319, 0.0262, 0.0474,  ..., 0.0151, 0.0093, 0.0088],\n",
      "          [0.0093, 0.0258, 0.0875,  ..., 0.0110, 0.0102, 0.0045]],\n",
      "\n",
      "         [[0.0145, 0.0171, 0.0219,  ..., 0.0211, 0.0093, 0.0240],\n",
      "          [0.0134, 0.0146, 0.0122,  ..., 0.0143, 0.0166, 0.0417],\n",
      "          [0.0284, 0.0172, 0.0299,  ..., 0.0147, 0.0109, 0.0239],\n",
      "          ...,\n",
      "          [0.0271, 0.0170, 0.0184,  ..., 0.0156, 0.0126, 0.0260],\n",
      "          [0.0261, 0.0188, 0.0130,  ..., 0.0207, 0.0136, 0.0279],\n",
      "          [0.0264, 0.0205, 0.0160,  ..., 0.0167, 0.0092, 0.0151]],\n",
      "\n",
      "         [[0.0118, 0.0126, 0.0131,  ..., 0.0175, 0.0374, 0.0169],\n",
      "          [0.0146, 0.0144, 0.0148,  ..., 0.0133, 0.0255, 0.0201],\n",
      "          [0.0192, 0.0174, 0.0099,  ..., 0.0150, 0.0145, 0.0084],\n",
      "          ...,\n",
      "          [0.0485, 0.0121, 0.0224,  ..., 0.0214, 0.0576, 0.0255],\n",
      "          [0.0266, 0.0171, 0.0280,  ..., 0.0193, 0.0454, 0.0300],\n",
      "          [0.0095, 0.0150, 0.0179,  ..., 0.0143, 0.0190, 0.0308]]],\n",
      "\n",
      "\n",
      "        [[[0.0076, 0.0193, 0.0200,  ..., 0.0242, 0.0260, 0.0270],\n",
      "          [0.0181, 0.0162, 0.0229,  ..., 0.0222, 0.0148, 0.0274],\n",
      "          [0.0236, 0.0099, 0.0274,  ..., 0.0066, 0.0150, 0.0189],\n",
      "          ...,\n",
      "          [0.0284, 0.0247, 0.0168,  ..., 0.0140, 0.0159, 0.0267],\n",
      "          [0.0356, 0.0071, 0.0129,  ..., 0.0171, 0.0172, 0.0232],\n",
      "          [0.0213, 0.0205, 0.0152,  ..., 0.0181, 0.0192, 0.0246]],\n",
      "\n",
      "         [[0.0167, 0.0258, 0.0150,  ..., 0.0305, 0.0240, 0.0327],\n",
      "          [0.0187, 0.0128, 0.0213,  ..., 0.0185, 0.0287, 0.0217],\n",
      "          [0.0129, 0.0170, 0.0100,  ..., 0.0160, 0.0184, 0.0187],\n",
      "          ...,\n",
      "          [0.0104, 0.0056, 0.0135,  ..., 0.0315, 0.0190, 0.0208],\n",
      "          [0.0110, 0.0134, 0.0179,  ..., 0.0222, 0.0230, 0.0122],\n",
      "          [0.0161, 0.0116, 0.0113,  ..., 0.0368, 0.0389, 0.0256]],\n",
      "\n",
      "         [[0.0198, 0.0180, 0.0147,  ..., 0.0235, 0.0202, 0.0118],\n",
      "          [0.0238, 0.0202, 0.0145,  ..., 0.0200, 0.0236, 0.0129],\n",
      "          [0.0143, 0.0220, 0.0044,  ..., 0.0151, 0.0157, 0.0093],\n",
      "          ...,\n",
      "          [0.0077, 0.0133, 0.0327,  ..., 0.0463, 0.0120, 0.0189],\n",
      "          [0.0122, 0.0193, 0.0527,  ..., 0.0196, 0.0085, 0.0147],\n",
      "          [0.0117, 0.0202, 0.0140,  ..., 0.0151, 0.0322, 0.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0176, 0.0137, 0.0242,  ..., 0.0158, 0.0175, 0.0206],\n",
      "          [0.0245, 0.0209, 0.0308,  ..., 0.0127, 0.0144, 0.0185],\n",
      "          [0.0122, 0.0132, 0.0319,  ..., 0.0175, 0.0094, 0.0263],\n",
      "          ...,\n",
      "          [0.0435, 0.0208, 0.0267,  ..., 0.0157, 0.0241, 0.0194],\n",
      "          [0.0176, 0.0133, 0.0259,  ..., 0.0128, 0.0260, 0.0130],\n",
      "          [0.0226, 0.0140, 0.0277,  ..., 0.0125, 0.0179, 0.0112]],\n",
      "\n",
      "         [[0.0244, 0.0213, 0.0199,  ..., 0.0121, 0.0224, 0.0158],\n",
      "          [0.0197, 0.0144, 0.0154,  ..., 0.0160, 0.0183, 0.0409],\n",
      "          [0.0138, 0.0139, 0.0126,  ..., 0.0106, 0.0105, 0.0133],\n",
      "          ...,\n",
      "          [0.0265, 0.0279, 0.0121,  ..., 0.0168, 0.0098, 0.0133],\n",
      "          [0.0104, 0.0114, 0.0187,  ..., 0.0208, 0.0089, 0.0334],\n",
      "          [0.0144, 0.0099, 0.0086,  ..., 0.0254, 0.0284, 0.0244]],\n",
      "\n",
      "         [[0.0086, 0.0153, 0.0134,  ..., 0.0107, 0.0048, 0.0123],\n",
      "          [0.0074, 0.0188, 0.0202,  ..., 0.0180, 0.0144, 0.0256],\n",
      "          [0.0064, 0.0273, 0.0076,  ..., 0.0117, 0.0097, 0.0144],\n",
      "          ...,\n",
      "          [0.0113, 0.0338, 0.0149,  ..., 0.0203, 0.0192, 0.0114],\n",
      "          [0.0169, 0.0124, 0.0094,  ..., 0.0116, 0.0235, 0.0151],\n",
      "          [0.0042, 0.0219, 0.0102,  ..., 0.0181, 0.0199, 0.0113]]],\n",
      "\n",
      "\n",
      "        [[[0.0297, 0.0161, 0.0275,  ..., 0.0257, 0.0162, 0.0243],\n",
      "          [0.0320, 0.0199, 0.0268,  ..., 0.0282, 0.0177, 0.0293],\n",
      "          [0.0248, 0.0260, 0.0392,  ..., 0.0203, 0.0235, 0.0234],\n",
      "          ...,\n",
      "          [0.0320, 0.0256, 0.0223,  ..., 0.0169, 0.0353, 0.0296],\n",
      "          [0.0174, 0.0200, 0.0195,  ..., 0.0214, 0.0321, 0.0265],\n",
      "          [0.0128, 0.0384, 0.0215,  ..., 0.0063, 0.0554, 0.0167]],\n",
      "\n",
      "         [[0.0129, 0.0167, 0.0244,  ..., 0.0442, 0.0173, 0.0263],\n",
      "          [0.0186, 0.0101, 0.0320,  ..., 0.0257, 0.0207, 0.0269],\n",
      "          [0.0159, 0.0135, 0.0117,  ..., 0.0484, 0.0130, 0.0214],\n",
      "          ...,\n",
      "          [0.0110, 0.0132, 0.0206,  ..., 0.0120, 0.0215, 0.0148],\n",
      "          [0.0143, 0.0188, 0.0124,  ..., 0.0119, 0.0202, 0.0095],\n",
      "          [0.0166, 0.0202, 0.0205,  ..., 0.0150, 0.0118, 0.0101]],\n",
      "\n",
      "         [[0.0095, 0.0172, 0.0081,  ..., 0.0171, 0.0246, 0.0156],\n",
      "          [0.0090, 0.0068, 0.0096,  ..., 0.0192, 0.0397, 0.0158],\n",
      "          [0.0126, 0.0088, 0.0122,  ..., 0.0156, 0.0275, 0.0109],\n",
      "          ...,\n",
      "          [0.0182, 0.0159, 0.0269,  ..., 0.0139, 0.0210, 0.0165],\n",
      "          [0.0198, 0.0106, 0.0217,  ..., 0.0186, 0.0250, 0.0183],\n",
      "          [0.0164, 0.0189, 0.0194,  ..., 0.0233, 0.0230, 0.0235]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0175, 0.0350, 0.0183,  ..., 0.0281, 0.0195, 0.0246],\n",
      "          [0.0197, 0.0150, 0.0110,  ..., 0.0164, 0.0141, 0.0102],\n",
      "          [0.0102, 0.0211, 0.0423,  ..., 0.0252, 0.0186, 0.0166],\n",
      "          ...,\n",
      "          [0.0090, 0.0137, 0.0114,  ..., 0.0327, 0.0223, 0.0321],\n",
      "          [0.0190, 0.0296, 0.0297,  ..., 0.0285, 0.0192, 0.0359],\n",
      "          [0.0145, 0.0190, 0.0222,  ..., 0.0235, 0.0207, 0.0154]],\n",
      "\n",
      "         [[0.0259, 0.0094, 0.0182,  ..., 0.0176, 0.0178, 0.0182],\n",
      "          [0.0203, 0.0236, 0.0148,  ..., 0.0250, 0.0108, 0.0191],\n",
      "          [0.0057, 0.0174, 0.0158,  ..., 0.0282, 0.0109, 0.0105],\n",
      "          ...,\n",
      "          [0.0167, 0.0152, 0.0188,  ..., 0.0308, 0.0354, 0.0198],\n",
      "          [0.0252, 0.0172, 0.0133,  ..., 0.0106, 0.0193, 0.0135],\n",
      "          [0.0176, 0.0140, 0.0149,  ..., 0.0351, 0.0169, 0.0187]],\n",
      "\n",
      "         [[0.0232, 0.0314, 0.0231,  ..., 0.0099, 0.0120, 0.0156],\n",
      "          [0.0221, 0.0163, 0.0166,  ..., 0.0190, 0.0314, 0.0125],\n",
      "          [0.0159, 0.0138, 0.0115,  ..., 0.0170, 0.0302, 0.0123],\n",
      "          ...,\n",
      "          [0.0079, 0.0257, 0.0187,  ..., 0.0374, 0.0264, 0.0147],\n",
      "          [0.0045, 0.0133, 0.0215,  ..., 0.0891, 0.0278, 0.0290],\n",
      "          [0.0140, 0.0103, 0.0087,  ..., 0.0310, 0.0293, 0.0096]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-1.4780e-01, -2.0830e-02,  2.2532e-01,  ...,  1.9412e-01,\n",
      "            2.3491e-01,  2.8689e-01],\n",
      "          [-1.2379e-01,  2.8061e-02,  1.9882e-01,  ...,  1.7084e-01,\n",
      "            2.2565e-01,  3.6139e-01],\n",
      "          [-1.2705e-01,  9.7746e-03,  1.9996e-01,  ...,  2.0510e-01,\n",
      "            3.0674e-01,  3.6109e-01],\n",
      "          ...,\n",
      "          [-1.9457e-01,  9.6368e-04,  2.4310e-01,  ...,  1.5016e-01,\n",
      "            2.6818e-01,  3.6753e-01],\n",
      "          [-1.8779e-01,  8.5408e-02,  2.0369e-01,  ...,  1.4861e-01,\n",
      "            2.3098e-01,  3.6226e-01],\n",
      "          [-1.8667e-01, -3.0256e-02,  2.5151e-01,  ...,  1.2126e-01,\n",
      "            3.1988e-01,  3.1703e-01]],\n",
      "\n",
      "         [[-1.4466e-01, -1.9303e-01,  7.5087e-02,  ...,  2.5457e-01,\n",
      "            2.7631e-01,  4.8869e-01],\n",
      "          [-1.4140e-01, -1.8209e-01,  8.7062e-02,  ...,  2.7390e-01,\n",
      "            2.8159e-01,  4.5841e-01],\n",
      "          [-1.6360e-01, -1.8907e-01,  3.4351e-02,  ...,  2.6978e-01,\n",
      "            2.6838e-01,  4.8595e-01],\n",
      "          ...,\n",
      "          [-1.7809e-01, -1.5742e-01,  1.0781e-01,  ...,  1.5486e-01,\n",
      "            2.1626e-01,  5.3335e-01],\n",
      "          [-1.5847e-01, -1.6755e-01,  8.9011e-02,  ...,  2.0523e-01,\n",
      "            2.2247e-01,  5.2177e-01],\n",
      "          [-2.4000e-01, -1.6247e-01,  5.6999e-02,  ...,  1.8991e-01,\n",
      "            2.2896e-01,  5.3241e-01]],\n",
      "\n",
      "         [[ 4.4015e-01,  3.1472e-01,  3.7585e-02,  ...,  5.6797e-01,\n",
      "            2.9727e-01,  1.4803e-01],\n",
      "          [ 4.1395e-01,  2.5075e-01,  2.8894e-02,  ...,  5.6886e-01,\n",
      "            2.9337e-01,  1.7972e-01],\n",
      "          [ 3.7006e-01,  2.7961e-01,  3.4663e-02,  ...,  5.5641e-01,\n",
      "            2.3021e-01,  2.0665e-01],\n",
      "          ...,\n",
      "          [ 4.1995e-01,  2.7371e-01,  5.0196e-02,  ...,  4.6895e-01,\n",
      "            2.5738e-01,  2.9608e-01],\n",
      "          [ 4.0932e-01,  2.8871e-01,  4.4941e-02,  ...,  5.4058e-01,\n",
      "            2.1852e-01,  1.8059e-01],\n",
      "          [ 4.0244e-01,  3.0974e-01,  3.4660e-02,  ...,  5.4377e-01,\n",
      "            2.9097e-01,  2.0985e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0530e-01,  5.4609e-02, -7.7754e-01,  ..., -1.1927e-01,\n",
      "           -2.4528e-01, -7.1897e-03],\n",
      "          [-1.5088e-01,  4.6968e-02, -7.6597e-01,  ..., -2.2798e-01,\n",
      "           -3.0549e-01, -1.2834e-02],\n",
      "          [-8.9970e-02,  5.3378e-02, -7.3934e-01,  ..., -4.2539e-02,\n",
      "           -2.6926e-01, -5.0749e-03],\n",
      "          ...,\n",
      "          [-1.2116e-01,  1.1460e-01, -7.8838e-01,  ..., -1.5448e-01,\n",
      "           -2.7789e-01, -3.5801e-02],\n",
      "          [-7.8981e-02,  1.4286e-01, -7.3251e-01,  ..., -1.2666e-01,\n",
      "           -2.5311e-01,  1.0904e-02],\n",
      "          [-1.1355e-01,  1.4435e-01, -7.2052e-01,  ..., -9.3969e-02,\n",
      "           -2.6803e-01, -3.8994e-02]],\n",
      "\n",
      "         [[ 6.4780e-01,  4.7128e-01, -5.1516e-01,  ...,  3.0963e-01,\n",
      "           -3.4045e-01,  7.3587e-01],\n",
      "          [ 6.8921e-01,  5.3029e-01, -4.7792e-01,  ...,  2.7065e-01,\n",
      "           -2.9508e-01,  8.1337e-01],\n",
      "          [ 6.2945e-01,  4.8814e-01, -5.1416e-01,  ...,  3.2361e-01,\n",
      "           -2.4812e-01,  8.0580e-01],\n",
      "          ...,\n",
      "          [ 7.3839e-01,  4.9277e-01, -4.7950e-01,  ...,  2.4552e-01,\n",
      "           -3.6108e-01,  8.6327e-01],\n",
      "          [ 6.8973e-01,  5.2371e-01, -5.0414e-01,  ...,  2.6781e-01,\n",
      "           -3.3523e-01,  8.5552e-01],\n",
      "          [ 6.9331e-01,  4.7863e-01, -4.7211e-01,  ...,  2.7836e-01,\n",
      "           -3.5041e-01,  8.5569e-01]],\n",
      "\n",
      "         [[ 1.1154e-01, -2.4256e-01, -2.0451e-01,  ...,  1.6120e-01,\n",
      "           -5.3400e-02,  5.2798e-01],\n",
      "          [ 9.1434e-02, -2.8647e-01, -1.3597e-01,  ...,  1.1191e-01,\n",
      "           -1.3945e-02,  5.1069e-01],\n",
      "          [ 9.6408e-02, -2.7689e-01, -2.7595e-01,  ...,  1.7600e-01,\n",
      "            1.3259e-02,  5.0712e-01],\n",
      "          ...,\n",
      "          [ 1.0196e-01, -1.4546e-01, -1.6474e-01,  ...,  1.3673e-02,\n",
      "           -6.3181e-02,  4.4265e-01],\n",
      "          [ 5.3004e-02, -1.9874e-01, -1.5059e-01,  ...,  3.6079e-02,\n",
      "           -3.7254e-02,  5.0619e-01],\n",
      "          [ 1.6606e-01, -1.5463e-01, -1.5162e-01,  ...,  1.1713e-01,\n",
      "           -3.0453e-02,  4.7983e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.9314e-01,  1.7436e-01,  1.4180e-01,  ...,  4.0921e-01,\n",
      "            3.3005e-01,  2.5252e-01],\n",
      "          [-3.1114e-01,  1.9964e-01,  1.3198e-01,  ...,  3.7861e-01,\n",
      "            3.3025e-01,  2.9374e-01],\n",
      "          [-3.5199e-01,  2.2266e-01,  1.7253e-01,  ...,  3.8541e-01,\n",
      "            3.3835e-01,  2.2592e-01],\n",
      "          ...,\n",
      "          [-3.1564e-01,  1.9527e-01,  1.1065e-01,  ...,  3.4599e-01,\n",
      "            3.0694e-01,  2.4031e-01],\n",
      "          [-3.2542e-01,  1.9914e-01,  1.4009e-01,  ...,  4.0170e-01,\n",
      "            3.0539e-01,  1.9669e-01],\n",
      "          [-3.1722e-01,  1.9299e-01,  9.7614e-02,  ...,  3.8331e-01,\n",
      "            2.9092e-01,  2.6966e-01]],\n",
      "\n",
      "         [[-2.0546e-01, -2.6420e-01, -3.4058e-02,  ...,  2.8639e-01,\n",
      "            2.9556e-01,  7.2499e-01],\n",
      "          [-1.6797e-01, -2.1476e-01,  1.4671e-02,  ...,  2.8599e-01,\n",
      "            3.6611e-01,  8.1124e-01],\n",
      "          [-2.3925e-01, -1.7113e-01,  7.7396e-02,  ...,  3.3034e-01,\n",
      "            3.8142e-01,  8.0337e-01],\n",
      "          ...,\n",
      "          [-1.6674e-01, -2.4599e-01,  3.2784e-02,  ...,  3.0579e-01,\n",
      "            3.3597e-01,  8.4879e-01],\n",
      "          [-2.1519e-01, -2.5641e-01,  9.9049e-02,  ...,  2.9990e-01,\n",
      "            3.7231e-01,  8.2843e-01],\n",
      "          [-1.8999e-01, -1.7805e-01,  8.0228e-02,  ...,  2.8904e-01,\n",
      "            3.8247e-01,  8.4623e-01]],\n",
      "\n",
      "         [[ 4.9696e-01,  2.9177e-01,  1.3797e-01,  ...,  5.7480e-01,\n",
      "            5.4270e-02,  2.3969e-01],\n",
      "          [ 4.6941e-01,  2.7919e-01,  2.3273e-01,  ...,  5.8608e-01,\n",
      "            5.8823e-02,  2.4113e-01],\n",
      "          [ 4.9413e-01,  3.1991e-01,  9.8254e-02,  ...,  5.5358e-01,\n",
      "            2.5341e-02,  2.4285e-01],\n",
      "          ...,\n",
      "          [ 5.1555e-01,  3.5447e-01,  3.5210e-01,  ...,  6.0291e-01,\n",
      "            4.7955e-02,  1.4525e-01],\n",
      "          [ 4.5086e-01,  2.8079e-01,  1.2174e-01,  ...,  4.9669e-01,\n",
      "            4.1729e-02,  2.0308e-01],\n",
      "          [ 5.2713e-01,  3.3098e-01,  1.4361e-01,  ...,  5.6599e-01,\n",
      "            2.5541e-02,  2.3479e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3665e-01,  1.2233e-01, -6.6962e-01,  ..., -1.9274e-01,\n",
      "           -1.4377e-01, -2.1730e-01],\n",
      "          [-2.1122e-01,  8.8769e-02, -7.1617e-01,  ..., -1.7104e-01,\n",
      "           -1.6559e-01, -1.9554e-01],\n",
      "          [-2.5909e-01,  1.6164e-01, -7.4012e-01,  ..., -1.2836e-01,\n",
      "           -1.5230e-01, -2.4348e-01],\n",
      "          ...,\n",
      "          [-1.2213e-01,  1.6725e-01, -7.0735e-01,  ..., -1.8667e-01,\n",
      "           -1.9413e-01, -1.3664e-01],\n",
      "          [-1.8716e-01,  1.4776e-01, -6.9259e-01,  ..., -1.0370e-01,\n",
      "           -1.4062e-01, -1.4417e-01],\n",
      "          [-1.7201e-01,  2.0426e-01, -6.9645e-01,  ..., -2.2738e-01,\n",
      "           -2.0619e-01, -1.3787e-01]],\n",
      "\n",
      "         [[ 7.1178e-01,  1.8423e-01, -3.5765e-01,  ...,  2.1235e-01,\n",
      "           -3.4076e-01,  6.4945e-01],\n",
      "          [ 6.9093e-01,  2.2922e-01, -3.2311e-01,  ...,  2.6212e-01,\n",
      "           -3.9082e-01,  6.5023e-01],\n",
      "          [ 6.9716e-01,  2.6149e-01, -3.4999e-01,  ...,  1.9565e-01,\n",
      "           -3.7304e-01,  5.4838e-01],\n",
      "          ...,\n",
      "          [ 7.0796e-01,  3.0033e-01, -3.6853e-01,  ...,  2.2510e-01,\n",
      "           -3.7921e-01,  6.2805e-01],\n",
      "          [ 7.1592e-01,  2.7976e-01, -3.8463e-01,  ...,  2.7106e-01,\n",
      "           -3.5946e-01,  6.2125e-01],\n",
      "          [ 6.5702e-01,  2.7228e-01, -3.6166e-01,  ...,  2.2069e-01,\n",
      "           -3.7838e-01,  6.5829e-01]],\n",
      "\n",
      "         [[-4.8370e-04, -2.8950e-01, -1.1913e-01,  ...,  1.9939e-01,\n",
      "           -2.6632e-02,  7.4408e-01],\n",
      "          [ 4.9393e-02, -2.3029e-01, -1.4823e-01,  ...,  1.2515e-01,\n",
      "           -4.2171e-03,  7.6286e-01],\n",
      "          [ 2.2226e-02, -3.4592e-01, -1.5997e-01,  ...,  1.7668e-01,\n",
      "           -6.2905e-02,  7.1315e-01],\n",
      "          ...,\n",
      "          [-6.7534e-03, -2.7284e-01, -1.6522e-01,  ...,  2.0494e-01,\n",
      "            4.4472e-02,  7.8148e-01],\n",
      "          [-2.8372e-02, -1.6721e-01, -1.1748e-01,  ...,  1.3106e-01,\n",
      "            7.9734e-02,  7.7990e-01],\n",
      "          [ 1.5468e-02, -2.2265e-01, -8.1429e-02,  ...,  1.6896e-01,\n",
      "           -4.0161e-02,  7.3636e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.3499e-01,  6.1077e-02,  2.7448e-01,  ...,  1.1756e-01,\n",
      "            4.1565e-01,  2.2234e-01],\n",
      "          [-3.1546e-01,  8.4455e-02,  3.5800e-01,  ...,  1.5006e-01,\n",
      "            3.4475e-01,  2.9127e-01],\n",
      "          [-3.4171e-01,  1.1241e-01,  3.6380e-01,  ...,  1.5739e-01,\n",
      "            3.8999e-01,  3.5032e-01],\n",
      "          ...,\n",
      "          [-3.5560e-01,  4.4051e-02,  3.1578e-01,  ...,  1.8277e-01,\n",
      "            2.9094e-01,  2.0704e-01],\n",
      "          [-3.1423e-01,  1.0165e-01,  3.0072e-01,  ...,  1.4175e-01,\n",
      "            3.1255e-01,  2.1454e-01],\n",
      "          [-2.5574e-01,  6.2202e-02,  4.1781e-01,  ...,  1.5540e-01,\n",
      "            2.9074e-01,  2.4699e-01]],\n",
      "\n",
      "         [[-2.9114e-01, -1.6302e-01,  2.7067e-02,  ...,  2.7574e-01,\n",
      "            3.4893e-01,  7.5203e-01],\n",
      "          [-3.1065e-01, -2.1177e-01,  1.0338e-02,  ...,  3.2906e-01,\n",
      "            3.6491e-01,  8.0467e-01],\n",
      "          [-3.0747e-01, -2.0713e-01,  2.4751e-02,  ...,  2.9057e-01,\n",
      "            3.6098e-01,  7.4837e-01],\n",
      "          ...,\n",
      "          [-3.2589e-01, -1.0898e-01, -2.3799e-03,  ...,  2.7794e-01,\n",
      "            4.0930e-01,  7.4447e-01],\n",
      "          [-3.3308e-01, -1.5143e-01,  1.9392e-02,  ...,  2.2375e-01,\n",
      "            4.2166e-01,  7.6021e-01],\n",
      "          [-4.1765e-01, -1.8944e-01,  2.8883e-02,  ...,  3.4666e-01,\n",
      "            3.2295e-01,  7.3652e-01]],\n",
      "\n",
      "         [[ 3.1111e-01,  3.9588e-01,  7.8351e-02,  ...,  6.7457e-01,\n",
      "            1.9430e-01,  7.2290e-02],\n",
      "          [ 3.1170e-01,  4.8972e-01,  1.0548e-01,  ...,  6.8805e-01,\n",
      "            2.7677e-01,  2.4051e-02],\n",
      "          [ 3.6386e-01,  4.3043e-01,  1.0376e-01,  ...,  6.5067e-01,\n",
      "            2.7928e-01,  7.9367e-02],\n",
      "          ...,\n",
      "          [ 2.9724e-01,  4.0701e-01,  1.2754e-01,  ...,  7.0191e-01,\n",
      "            2.5985e-01,  5.7392e-02],\n",
      "          [ 2.4564e-01,  4.4966e-01,  1.4683e-01,  ...,  7.5664e-01,\n",
      "            2.1781e-01,  6.5229e-03],\n",
      "          [ 2.6545e-01,  4.1871e-01,  8.7036e-02,  ...,  6.9333e-01,\n",
      "            2.7534e-01,  2.7863e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2291e-01,  1.0368e-01, -5.9727e-01,  ..., -1.7744e-01,\n",
      "           -2.3083e-01, -2.5089e-01],\n",
      "          [-6.2298e-02,  1.0837e-01, -5.4510e-01,  ..., -2.3750e-01,\n",
      "           -2.6163e-01, -3.1832e-01],\n",
      "          [-9.2708e-02,  8.2553e-02, -5.6836e-01,  ..., -1.9761e-01,\n",
      "           -2.2958e-01, -2.8832e-01],\n",
      "          ...,\n",
      "          [-9.6782e-02,  1.5210e-01, -5.4210e-01,  ..., -2.1469e-01,\n",
      "           -2.3101e-01, -2.9157e-01],\n",
      "          [-1.6612e-01,  1.4696e-01, -5.3298e-01,  ..., -2.3845e-01,\n",
      "           -2.9928e-01, -3.5698e-01],\n",
      "          [-8.9616e-02,  1.0271e-01, -5.3783e-01,  ..., -2.5375e-01,\n",
      "           -2.9128e-01, -2.9171e-01]],\n",
      "\n",
      "         [[ 7.9370e-01,  4.3510e-01, -4.2128e-01,  ...,  3.6252e-01,\n",
      "           -3.0760e-01,  8.7160e-01],\n",
      "          [ 8.0213e-01,  3.9265e-01, -4.0463e-01,  ...,  3.1493e-01,\n",
      "           -3.6986e-01,  8.6033e-01],\n",
      "          [ 7.8734e-01,  3.7018e-01, -4.4258e-01,  ...,  3.1812e-01,\n",
      "           -3.5183e-01,  9.7449e-01],\n",
      "          ...,\n",
      "          [ 8.8072e-01,  3.5129e-01, -2.8818e-01,  ...,  3.8835e-01,\n",
      "           -3.0751e-01,  9.7320e-01],\n",
      "          [ 7.7607e-01,  3.7732e-01, -4.8146e-01,  ...,  2.8268e-01,\n",
      "           -2.5179e-01,  9.0091e-01],\n",
      "          [ 7.8954e-01,  3.3586e-01, -3.9806e-01,  ...,  2.8456e-01,\n",
      "           -2.9142e-01,  8.5763e-01]],\n",
      "\n",
      "         [[ 9.6182e-02, -1.7589e-02, -2.2217e-01,  ...,  8.7034e-02,\n",
      "            3.3141e-02,  4.1539e-01],\n",
      "          [ 1.0842e-01, -3.8592e-02, -2.3508e-01,  ...,  3.5058e-02,\n",
      "            3.3431e-02,  4.5975e-01],\n",
      "          [ 9.6905e-02, -6.1183e-02, -7.8506e-02,  ...,  1.2346e-01,\n",
      "            1.2102e-01,  4.5330e-01],\n",
      "          ...,\n",
      "          [ 1.4528e-01, -7.7664e-02, -1.9632e-01,  ...,  6.2699e-02,\n",
      "            1.0541e-01,  4.4554e-01],\n",
      "          [ 6.2731e-02, -1.5420e-01, -6.4403e-02,  ...,  9.7498e-02,\n",
      "            1.2723e-01,  4.1431e-01],\n",
      "          [ 5.6305e-02, -1.5025e-01, -1.4316e-01,  ...,  1.2383e-01,\n",
      "            1.3595e-01,  4.8164e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.3428e-01, -5.1388e-02,  1.4673e-01,  ...,  1.9128e-01,\n",
      "            3.7264e-01,  4.8242e-01],\n",
      "          [-4.4868e-01, -1.0339e-01,  2.4668e-01,  ...,  2.0093e-01,\n",
      "            3.0523e-01,  4.6506e-01],\n",
      "          [-4.1767e-01, -6.1350e-02,  2.3251e-01,  ...,  1.4015e-01,\n",
      "            3.6665e-01,  5.1537e-01],\n",
      "          ...,\n",
      "          [-3.5939e-01, -1.4557e-02,  1.9870e-01,  ...,  1.9009e-01,\n",
      "            3.5477e-01,  4.5931e-01],\n",
      "          [-4.1801e-01, -7.3111e-02,  1.7319e-01,  ...,  1.7950e-01,\n",
      "            3.3929e-01,  5.2154e-01],\n",
      "          [-4.2956e-01, -5.7251e-02,  2.0375e-01,  ...,  2.3261e-01,\n",
      "            3.4055e-01,  5.4436e-01]],\n",
      "\n",
      "         [[-1.3011e-01, -1.5131e-01, -1.2633e-03,  ...,  7.5094e-02,\n",
      "            2.9210e-01,  7.0202e-01],\n",
      "          [-1.1250e-01, -1.6314e-01,  1.3328e-02,  ...,  8.2464e-02,\n",
      "            2.6606e-01,  6.7189e-01],\n",
      "          [-9.7393e-02, -1.5433e-01,  1.5001e-02,  ...,  6.0458e-02,\n",
      "            2.6293e-01,  7.3608e-01],\n",
      "          ...,\n",
      "          [-1.4970e-01, -2.4230e-01, -2.5357e-02,  ...,  5.8403e-02,\n",
      "            2.6137e-01,  6.4909e-01],\n",
      "          [-7.9786e-02, -2.1242e-01,  5.5151e-02,  ...,  1.3305e-03,\n",
      "            2.4268e-01,  7.3949e-01],\n",
      "          [-9.7089e-02, -2.2554e-01,  3.9616e-02,  ...,  8.0329e-02,\n",
      "            2.7981e-01,  7.1081e-01]],\n",
      "\n",
      "         [[ 4.6785e-01,  1.9126e-01,  1.2383e-01,  ...,  5.2048e-01,\n",
      "            2.6486e-01,  9.8912e-02],\n",
      "          [ 4.7545e-01,  2.4408e-01,  1.6559e-01,  ...,  5.3353e-01,\n",
      "            2.7320e-01,  5.7938e-02],\n",
      "          [ 4.6488e-01,  2.3978e-01,  6.8712e-02,  ...,  5.0647e-01,\n",
      "            2.7960e-01,  6.1765e-02],\n",
      "          ...,\n",
      "          [ 4.3161e-01,  2.1647e-01,  2.3131e-02,  ...,  4.7885e-01,\n",
      "            2.7250e-01,  1.0308e-01],\n",
      "          [ 5.2785e-01,  1.9939e-01,  9.1241e-04,  ...,  4.8283e-01,\n",
      "            1.9162e-01,  1.9229e-01],\n",
      "          [ 6.0207e-01,  2.5205e-01,  1.5727e-01,  ...,  6.2051e-01,\n",
      "            2.9860e-01,  9.0024e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2054e-02,  8.5801e-02, -7.6023e-01,  ..., -2.3723e-01,\n",
      "           -1.7698e-01, -1.3637e-01],\n",
      "          [-9.0006e-02,  1.2003e-01, -8.0168e-01,  ..., -2.1470e-01,\n",
      "           -1.9792e-01, -6.6096e-02],\n",
      "          [-1.0522e-01,  1.3584e-01, -7.4707e-01,  ..., -2.1009e-01,\n",
      "           -1.0700e-01, -1.3027e-01],\n",
      "          ...,\n",
      "          [-6.6960e-02,  2.4456e-01, -7.8894e-01,  ..., -2.0187e-01,\n",
      "           -1.7307e-01, -4.8281e-02],\n",
      "          [-1.1864e-01,  2.4765e-01, -8.9396e-01,  ..., -2.3433e-01,\n",
      "           -1.7470e-01, -1.3222e-01],\n",
      "          [-9.3417e-02,  2.5549e-01, -8.2983e-01,  ..., -1.8490e-01,\n",
      "           -2.2771e-01, -5.2637e-02]],\n",
      "\n",
      "         [[ 6.0265e-01,  2.6242e-01, -3.4411e-01,  ...,  2.5389e-01,\n",
      "           -2.5124e-01,  9.3412e-01],\n",
      "          [ 5.8297e-01,  3.0066e-01, -2.8633e-01,  ...,  2.6355e-01,\n",
      "           -3.2284e-01,  8.4596e-01],\n",
      "          [ 6.2284e-01,  2.8357e-01, -3.4845e-01,  ...,  2.5100e-01,\n",
      "           -2.6138e-01,  9.3718e-01],\n",
      "          ...,\n",
      "          [ 6.2349e-01,  2.6280e-01, -3.3202e-01,  ...,  3.0001e-01,\n",
      "           -2.5547e-01,  9.8472e-01],\n",
      "          [ 6.1693e-01,  2.3479e-01, -3.3162e-01,  ...,  2.6950e-01,\n",
      "           -2.3201e-01,  9.9145e-01],\n",
      "          [ 6.7033e-01,  2.5686e-01, -3.3239e-01,  ...,  2.6726e-01,\n",
      "           -2.3527e-01,  9.6639e-01]],\n",
      "\n",
      "         [[ 3.5325e-01, -3.3386e-02, -2.1692e-01,  ...,  1.2294e-01,\n",
      "            7.9723e-02,  7.8615e-01],\n",
      "          [ 3.5609e-01, -1.1788e-02, -1.5653e-01,  ...,  5.7563e-02,\n",
      "            6.6461e-02,  7.3017e-01],\n",
      "          [ 3.3843e-01, -4.9902e-02, -1.5980e-01,  ...,  9.8202e-02,\n",
      "            1.6008e-02,  7.7257e-01],\n",
      "          ...,\n",
      "          [ 2.9334e-01,  8.0398e-03, -1.9252e-01,  ...,  1.8156e-01,\n",
      "            1.0616e-01,  7.6733e-01],\n",
      "          [ 2.1234e-01, -1.3323e-02, -1.8586e-01,  ...,  1.3407e-01,\n",
      "            5.2672e-02,  8.0427e-01],\n",
      "          [ 3.2679e-01, -7.1465e-02, -2.4875e-01,  ...,  1.7313e-01,\n",
      "            8.2286e-02,  8.3097e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.8627e-01, -3.1070e-02,  2.7640e-01,  ...,  2.0481e-01,\n",
      "            4.5071e-01,  4.3713e-01],\n",
      "          [-2.7949e-01, -5.1150e-02,  2.1169e-01,  ...,  1.7602e-01,\n",
      "            4.5014e-01,  5.0118e-01],\n",
      "          [-4.2910e-01, -2.2607e-02,  2.8781e-01,  ...,  2.4688e-01,\n",
      "            4.6564e-01,  3.8488e-01],\n",
      "          ...,\n",
      "          [-3.0118e-01, -1.0186e-01,  2.4453e-01,  ...,  1.5907e-01,\n",
      "            3.5218e-01,  4.7019e-01],\n",
      "          [-3.4461e-01, -4.2212e-02,  2.7846e-01,  ...,  2.2245e-01,\n",
      "            4.4033e-01,  4.3132e-01],\n",
      "          [-2.3527e-01, -6.3215e-02,  2.5172e-01,  ...,  1.8623e-01,\n",
      "            3.9043e-01,  5.1050e-01]],\n",
      "\n",
      "         [[-3.0724e-01, -2.0699e-01,  1.4936e-01,  ...,  1.4803e-01,\n",
      "            3.6703e-01,  6.0875e-01],\n",
      "          [-3.8459e-01, -3.1284e-01,  2.1287e-02,  ...,  2.6045e-01,\n",
      "            4.0016e-01,  5.8809e-01],\n",
      "          [-3.4563e-01, -1.4798e-01,  1.1810e-01,  ...,  2.0908e-01,\n",
      "            2.8896e-01,  6.6988e-01],\n",
      "          ...,\n",
      "          [-2.8543e-01, -1.9503e-01,  1.6970e-01,  ...,  1.6966e-01,\n",
      "            2.8477e-01,  5.9678e-01],\n",
      "          [-3.1664e-01, -2.0589e-01,  7.5599e-02,  ...,  2.4716e-01,\n",
      "            3.6682e-01,  6.1223e-01],\n",
      "          [-3.2035e-01, -1.7737e-01,  2.5995e-02,  ...,  2.2889e-01,\n",
      "            3.1742e-01,  6.9533e-01]],\n",
      "\n",
      "         [[ 5.4289e-01,  2.2410e-01,  1.6172e-01,  ...,  5.6088e-01,\n",
      "            1.8854e-01,  1.4188e-01],\n",
      "          [ 5.1487e-01,  2.4805e-01,  1.8815e-01,  ...,  6.1727e-01,\n",
      "            2.1423e-01,  1.0431e-01],\n",
      "          [ 5.3107e-01,  2.8262e-01,  2.6858e-01,  ...,  5.8773e-01,\n",
      "            2.2672e-01,  5.1098e-02],\n",
      "          ...,\n",
      "          [ 5.1537e-01,  2.4173e-01,  1.0252e-01,  ...,  5.5440e-01,\n",
      "            1.8485e-01,  1.7286e-01],\n",
      "          [ 5.2905e-01,  2.3766e-01,  2.9829e-02,  ...,  5.3854e-01,\n",
      "            1.7570e-01,  2.4105e-01],\n",
      "          [ 5.0655e-01,  2.3323e-01,  1.1191e-01,  ...,  5.6769e-01,\n",
      "            1.9978e-01,  6.9155e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8443e-01,  1.4575e-01, -6.3044e-01,  ..., -3.1525e-01,\n",
      "           -2.0802e-01, -1.0086e-01],\n",
      "          [-2.1748e-01,  1.9849e-01, -6.0476e-01,  ..., -3.3241e-01,\n",
      "           -1.4881e-01, -1.3843e-01],\n",
      "          [-2.5106e-01,  1.6277e-01, -5.7628e-01,  ..., -3.7650e-01,\n",
      "           -2.0059e-01, -7.5211e-02],\n",
      "          ...,\n",
      "          [-2.0822e-01,  1.8050e-01, -6.4841e-01,  ..., -3.1156e-01,\n",
      "           -1.5881e-01, -1.0616e-01],\n",
      "          [-2.7090e-01,  2.1958e-01, -6.1062e-01,  ..., -3.3716e-01,\n",
      "           -1.7961e-01, -1.2989e-01],\n",
      "          [-2.5553e-01,  2.0152e-01, -6.9902e-01,  ..., -2.9732e-01,\n",
      "           -1.8110e-01, -1.0233e-01]],\n",
      "\n",
      "         [[ 6.9514e-01,  4.1327e-01, -4.1631e-01,  ...,  2.9221e-01,\n",
      "           -3.2294e-01,  7.5008e-01],\n",
      "          [ 6.5443e-01,  4.0068e-01, -4.6144e-01,  ...,  1.8979e-01,\n",
      "           -3.5713e-01,  6.9697e-01],\n",
      "          [ 6.5880e-01,  4.3797e-01, -4.0648e-01,  ...,  2.8767e-01,\n",
      "           -3.6609e-01,  7.6184e-01],\n",
      "          ...,\n",
      "          [ 5.8724e-01,  3.7573e-01, -3.4459e-01,  ...,  2.2190e-01,\n",
      "           -3.4640e-01,  7.2141e-01],\n",
      "          [ 6.8885e-01,  3.6171e-01, -4.2510e-01,  ...,  3.1137e-01,\n",
      "           -2.9785e-01,  6.3815e-01],\n",
      "          [ 6.8724e-01,  3.8949e-01, -3.4627e-01,  ...,  2.0953e-01,\n",
      "           -3.3251e-01,  6.7978e-01]],\n",
      "\n",
      "         [[ 1.5959e-01, -3.4654e-01, -2.4909e-01,  ...,  4.2491e-02,\n",
      "            6.4406e-02,  5.7274e-01],\n",
      "          [ 8.1280e-02, -3.0028e-01, -1.6459e-01,  ...,  4.0934e-02,\n",
      "            9.0316e-02,  5.6804e-01],\n",
      "          [ 1.8370e-01, -2.8762e-01, -2.5036e-01,  ...,  1.1761e-02,\n",
      "            4.3867e-02,  5.9066e-01],\n",
      "          ...,\n",
      "          [ 1.0989e-01, -3.0129e-01, -1.5848e-01,  ..., -2.1748e-03,\n",
      "            1.0954e-01,  5.1584e-01],\n",
      "          [ 1.3618e-01, -2.2249e-01, -7.8846e-02,  ...,  5.1200e-02,\n",
      "            1.1392e-01,  5.2364e-01],\n",
      "          [ 6.6666e-02, -2.8645e-01, -1.9358e-01,  ...,  2.9997e-02,\n",
      "            6.3758e-02,  7.0119e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5382e-01,  7.8759e-02,  1.3891e-01,  ...,  1.1978e-01,\n",
      "            2.8516e-01,  3.0203e-01],\n",
      "          [-3.4579e-01,  7.2956e-02,  1.2793e-01,  ...,  1.2862e-01,\n",
      "            2.7011e-01,  2.3303e-01],\n",
      "          [-3.1494e-01,  5.2837e-02,  1.1388e-01,  ...,  1.7192e-01,\n",
      "            2.6224e-01,  1.8410e-01],\n",
      "          ...,\n",
      "          [-3.2125e-01,  7.9094e-02,  5.7139e-02,  ...,  1.6459e-01,\n",
      "            2.8768e-01,  2.7207e-01],\n",
      "          [-3.0768e-01, -6.6084e-03,  6.9390e-02,  ...,  1.3883e-01,\n",
      "            3.1535e-01,  3.1565e-01],\n",
      "          [-2.3444e-01, -4.8079e-02,  3.3997e-02,  ...,  2.5077e-01,\n",
      "            3.0488e-01,  2.4669e-01]],\n",
      "\n",
      "         [[-2.8764e-01, -3.2015e-01, -6.7701e-02,  ...,  1.7714e-01,\n",
      "            2.5441e-01,  7.9316e-01],\n",
      "          [-3.5226e-01, -3.3793e-01, -4.0336e-02,  ...,  1.7574e-01,\n",
      "            2.2682e-01,  8.6452e-01],\n",
      "          [-3.8468e-01, -3.3981e-01, -4.2139e-02,  ...,  2.9656e-01,\n",
      "            2.8723e-01,  8.2249e-01],\n",
      "          ...,\n",
      "          [-3.5324e-01, -3.3982e-01, -3.2529e-02,  ...,  2.0075e-01,\n",
      "            2.8366e-01,  8.4825e-01],\n",
      "          [-3.0731e-01, -3.4309e-01,  3.0130e-02,  ...,  1.6142e-01,\n",
      "            2.5334e-01,  8.6146e-01],\n",
      "          [-3.3335e-01, -3.2825e-01, -3.4828e-02,  ...,  2.1294e-01,\n",
      "            2.5438e-01,  8.6695e-01]],\n",
      "\n",
      "         [[ 5.0291e-01,  2.1593e-01,  1.1861e-01,  ...,  5.4708e-01,\n",
      "            9.8017e-02,  1.9183e-01],\n",
      "          [ 5.3109e-01,  2.2751e-01,  1.8993e-01,  ...,  5.3172e-01,\n",
      "            8.9473e-02,  1.4855e-01],\n",
      "          [ 4.9233e-01,  2.5218e-01,  1.2958e-01,  ...,  4.9532e-01,\n",
      "            1.4980e-01,  1.8507e-01],\n",
      "          ...,\n",
      "          [ 5.2982e-01,  2.2991e-01,  9.5664e-02,  ...,  5.0323e-01,\n",
      "            1.9140e-01,  1.8294e-01],\n",
      "          [ 5.5536e-01,  2.3465e-01,  2.0863e-01,  ...,  5.1863e-01,\n",
      "            6.3550e-02,  2.0098e-01],\n",
      "          [ 5.3963e-01,  2.5594e-01,  1.6009e-01,  ...,  4.8351e-01,\n",
      "            1.4250e-01,  2.1503e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2447e-03,  7.4607e-02, -7.3284e-01,  ..., -2.0783e-01,\n",
      "           -1.0332e-01, -8.4207e-02],\n",
      "          [-3.6349e-02,  7.4382e-02, -7.0511e-01,  ..., -1.6709e-01,\n",
      "           -1.2927e-01, -1.0318e-01],\n",
      "          [-4.2756e-02,  4.8103e-02, -7.7343e-01,  ..., -2.0851e-01,\n",
      "           -1.4367e-01, -4.9006e-02],\n",
      "          ...,\n",
      "          [ 1.2920e-02,  1.0712e-01, -6.5554e-01,  ..., -1.8008e-01,\n",
      "           -9.3943e-02, -1.0459e-01],\n",
      "          [-2.9934e-02,  8.7717e-02, -6.6294e-01,  ..., -1.9865e-01,\n",
      "           -1.1043e-01, -1.2693e-01],\n",
      "          [-2.8334e-02,  3.5693e-02, -6.6122e-01,  ..., -1.2390e-01,\n",
      "           -1.1477e-01, -1.0164e-01]],\n",
      "\n",
      "         [[ 7.6285e-01,  3.2642e-01, -5.3226e-01,  ...,  2.7582e-01,\n",
      "           -3.0506e-01,  8.0649e-01],\n",
      "          [ 6.8033e-01,  3.8010e-01, -5.3659e-01,  ...,  2.9494e-01,\n",
      "           -3.2176e-01,  6.7100e-01],\n",
      "          [ 7.2697e-01,  3.4384e-01, -4.9619e-01,  ...,  3.6882e-01,\n",
      "           -3.4373e-01,  7.1831e-01],\n",
      "          ...,\n",
      "          [ 6.4242e-01,  2.7933e-01, -4.9368e-01,  ...,  2.9207e-01,\n",
      "           -2.7081e-01,  7.0032e-01],\n",
      "          [ 7.7674e-01,  3.2275e-01, -5.2082e-01,  ...,  2.8481e-01,\n",
      "           -3.7658e-01,  8.3382e-01],\n",
      "          [ 6.9339e-01,  3.1017e-01, -4.7465e-01,  ...,  2.7747e-01,\n",
      "           -3.0525e-01,  7.0726e-01]],\n",
      "\n",
      "         [[ 1.8829e-01, -1.0774e-01, -1.5077e-01,  ...,  1.2252e-01,\n",
      "            6.8064e-02,  5.6084e-01],\n",
      "          [ 1.5747e-01, -1.6667e-01, -1.9485e-01,  ...,  1.2461e-01,\n",
      "            5.3891e-02,  5.5640e-01],\n",
      "          [ 1.7344e-01, -1.7366e-01, -1.9416e-01,  ...,  1.3613e-01,\n",
      "            2.4058e-02,  6.0791e-01],\n",
      "          ...,\n",
      "          [ 1.8462e-01, -1.8023e-01, -1.5596e-01,  ...,  1.9903e-01,\n",
      "            6.1812e-02,  5.3013e-01],\n",
      "          [ 1.8569e-01, -2.2517e-01, -2.3591e-01,  ...,  2.2430e-01,\n",
      "           -2.6724e-02,  4.9891e-01],\n",
      "          [ 2.2950e-01, -1.7859e-01, -2.6322e-01,  ...,  1.5316e-01,\n",
      "            2.0320e-02,  6.0690e-01]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要进行残差连接？\n",
    "也许你已经注意到，前馈神经网络的输出`ffn_output 的形状： torch.Size([32, 50, 512])`与预处理之后的数据`x = preprocessor(input_ids)`、多头注意力的输出`拼接后的 output 的形状： torch.Size([32, 50, 512])`的形状一致。这让我们想到也许能够将其进行相加之类的操作。\n",
    "\n",
    "**（1）保留原始信息**\n",
    "- 多头注意力机制已经捕捉了序列中元素之间的关系。\n",
    "- 残差连接确保这些信息不会被前馈神经网络完全覆盖，保留原始特征。\n",
    "\n",
    "**（2）缓解梯度消失**\n",
    "- 深层网络中，梯度在反向传播时容易消失。\n",
    "- 残差连接提供了一条“捷径”，使梯度可以直接传播到浅层，缓解梯度消失问题。\n",
    "\n",
    "**（3）增强模型表达能力**\n",
    "- 前馈神经网络引入了非线性变换，增强了模型的表达能力。\n",
    "- 残差连接将这种非线性变换与原始特征结合，进一步提升模型性能。\n",
    "\n",
    "**（4）加速训练**\n",
    "- 残差连接使模型更容易优化，加速训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "norm1 = nn.LayerNorm(d_model)\n",
    "norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "x = norm1(x + projected_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在此之后，这个`x`再经过我们之前提到过的`Feed-Forward`得到的`ffn_output = ffn(projected_output)`进行残差链接，最后将`x`进行层归一化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "x = norm2(x + ffn_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 为什么要层归一化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，主要用于加速训练过程并提高模型的稳定性。以下是详细解释：\n",
    "\n",
    "**. 层归一化的作用**\n",
    "- **归一化特征**：\n",
    "  - 对每个样本的特征进行归一化，使其均值为0，方差为1。\n",
    "  - 减少内部协变量偏移（Internal Covariate Shift），使训练更稳定。\n",
    "- **加速收敛**：\n",
    "  - 归一化后的特征分布更稳定，有助于加速模型收敛。\n",
    "- **适用于不同任务**：\n",
    "  - 特别适合处理变长序列（如NLP任务）和小批量数据。\n",
    "\n",
    "**. 层归一化的公式**\n",
    "层归一化的计算公式如下：\n",
    "```python\n",
    "y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "```\n",
    "- **`x`**：输入特征。\n",
    "- **`mean`**：输入特征的均值。\n",
    "- **`var`**：输入特征的方差。\n",
    "- **`eps`**：一个小常数，用于数值稳定性（默认`1e-5`）。\n",
    "- **`gamma`**：可学习的缩放参数（权重）。\n",
    "- **`beta`**：可学习的偏移参数（偏置）。\n",
    "\n",
    "---\n",
    "\n",
    "**. 层归一化的特点**\n",
    "- **独立于批量大小**：\n",
    "  - 与批量归一化（Batch Normalization）不同，层归一化不依赖于批量大小，适合处理小批量或变长序列。\n",
    "- **逐样本归一化**：\n",
    "  - 对每个样本的特征进行归一化，而不是跨样本归一化。\n",
    "- **可学习的参数**：\n",
    "  - `gamma` 和 `beta` 是可学习的参数，允许模型调整归一化后的特征分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是`Encoder`中一个`EncoderLayer`的全部内容，为了获取`Encoder`，我们需要将`EncoderLayer`堆叠起来。你可以在`transformer_encoder.py`中找到完整的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Decoder 🐱 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. **掩码多头自注意力**：\n",
    "   - 捕捉已生成序列的内部关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 为什么就需要掩码了？之前的Encoder中的注意力不需要掩码呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **任务性质不同**\n",
    "- **Encoder**：\n",
    "  - 处理的是完整的输入序列（如源语言句子）\n",
    "  - 需要同时看到整个序列的所有信息\n",
    "  - 目标是捕捉序列中所有元素之间的关系\n",
    "\n",
    "- **Decoder**：\n",
    "  - 处理的是目标序列（如目标语言句子）\n",
    "  - 需要逐步生成序列，不能\"偷看\"未来信息\n",
    "  - 目标是基于已生成的部分序列和Encoder的输出来预测下一个元素\n",
    "\n",
    "##### 2. **掩码的作用**\n",
    "- **防止信息泄露**：\n",
    "  - 在训练时，Decoder会接收完整的目标序列\n",
    "  - 如果没有掩码，模型可能会直接\"看到\"未来的信息，导致训练作弊\n",
    "  - 掩码确保模型只能使用当前位置及之前的信息\n",
    "\n",
    "- **保持自回归性质**：\n",
    "  - 在推理时，Decoder需要逐个生成序列元素\n",
    "  - 掩码确保模型只能基于已生成的部分序列进行预测\n",
    "  - 这是序列生成任务（如机器翻译、文本生成）的基本要求\n",
    "\n",
    "##### 3. **具体实现**\n",
    "- **Encoder中的注意力**：\n",
    "  - 计算注意力分数时，所有位置之间都可以相互关注\n",
    "  - 不需要任何限制，因为整个输入序列是已知的\n",
    "\n",
    "- **Decoder中的掩码注意力**：\n",
    "  - 使用下三角掩码（`torch.tril`）\n",
    "  - 确保每个位置只能关注到它自身及之前的位置\n",
    "  - 未来位置的注意力分数被设置为`-inf`，在softmax后权重为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "Encoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "Decoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "mask 的形状： torch.Size([50, 50])\n",
      "mask 的值： tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "# Encoder注意力分数（无掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "# print(\"Encoder注意力分数 scores：\", scores)\n",
    "print(\"Encoder注意力分数 scores 的形状：\", scores.shape)\n",
    "# Decoder注意力分数（带掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # 下三角掩码\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))  # 应用掩码\n",
    "# print(\"Decoder注意力分数 scores：\", scores)\n",
    "print(\"Decoder注意力分数 scores 的形状：\", scores.shape)\n",
    "print(\"mask 的形状：\", mask.shape)\n",
    "print(\"mask 的值：\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 为什么需要下三角掩码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **自回归任务的性质**\n",
    "在自回归任务中，模型需要**逐步生成序列**，即每次只能基于已经生成的部分序列来预测下一个元素。例如：\n",
    "- 在文本生成中，模型只能基于已经生成的单词来预测下一个单词。\n",
    "- 在机器翻译中，模型只能基于已经生成的目标语言单词来预测下一个单词。\n",
    "\n",
    "如果模型能够“看到”未来的信息，它就会作弊，直接使用未来的信息来预测当前的位置，这会导致训练和推理不一致。\n",
    "\n",
    "\n",
    "##### 2. **下三角掩码的作用**\n",
    "下三角掩码的作用是**限制模型只能访问当前位置及之前的信息**，而不能访问未来的信息。具体来说：\n",
    "- **下三角部分**：值为1，表示允许模型访问这些位置的信息。\n",
    "- **上三角部分**：值为0，表示禁止模型访问这些位置的信息。\n",
    "\n",
    "通过将上三角部分的注意力分数设置为`-inf`，在softmax操作后，这些位置的权重会变为0，从而确保模型无法利用未来的信息。\n",
    "\n",
    "##### 3. **掩码的实现**\n",
    "在代码中，下三角掩码通常通过`torch.tril`函数生成：\n",
    "````python\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "````\n",
    "例如，对于一个长度为5的序列，掩码矩阵如下：\n",
    "````\n",
    "1 0 0 0 0\n",
    "1 1 0 0 0\n",
    "1 1 1 0 0\n",
    "1 1 1 1 0\n",
    "1 1 1 1 1\n",
    "````\n",
    "- 第1行：只能看到第1个位置。\n",
    "- 第2行：可以看到第1和第2个位置。\n",
    "- 第3行：可以看到第1、第2和第3个位置。\n",
    "- 以此类推。\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **为什么需要下三角掩码？**\n",
    "###### （1）**训练时防止信息泄露**\n",
    "- 在训练时，Decoder会接收完整的目标序列（如目标语言句子）。\n",
    "- 如果没有掩码，模型可能会直接“看到”未来的信息，导致训练作弊。\n",
    "- 掩码确保模型只能使用当前位置及之前的信息。\n",
    "\n",
    "###### （2）**推理时保持自回归性质**\n",
    "- 在推理时，Decoder需要逐个生成序列元素。\n",
    "- 掩码确保模型只能基于已生成的部分序列进行预测。\n",
    "- 这是序列生成任务（如机器翻译、文本生成）的基本要求。\n",
    "\n",
    "###### （3）**确保训练和推理一致性**\n",
    "- 训练时使用掩码，推理时也使用掩码，确保模型的行为一致。\n",
    "- 如果训练时不使用掩码，模型可能会学习到依赖未来信息的错误模式，导致推理时性能下降。\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **示例**\n",
    "假设我们有一个长度为3的序列，计算注意力分数时：\n",
    "- **无掩码**：模型可以看到所有位置的信息。\n",
    "  ````\n",
    "  scores = [[s11, s12, s13],\n",
    "            [s21, s22, s23],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "- **有掩码**：模型只能看到当前位置及之前的信息。\n",
    "  ````\n",
    "  scores = [[s11, -inf, -inf],\n",
    "            [s21, s22, -inf],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "在softmax后，`-inf`的位置权重为0，模型无法利用这些信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 6. **总结**\n",
    "| 特性                | 无掩码                  | 下三角掩码              |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| 信息访问            | 可以访问整个序列        | 只能访问当前位置及之前  |\n",
    "| 训练时              | 可能作弊，利用未来信息  | 防止信息泄露            |\n",
    "| 推理时              | 无法逐步生成序列        | 保持自回归性质          |\n",
    "| 适用场景            | Encoder                | Decoder                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 输入是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tgt: 目标序列的嵌入表示。也就是我们之前通过TransformerPreprocessor得到的被预处理的输入。\n",
    "2. tgt_mask: 下三角掩码，用于限制模型只能访问当前位置及之前的信息。\n",
    "3. memory: Encoder的输出，用于计算Encoder-Decoder Attention。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Output Layer 🐱 输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在《Attention is All You Need》论文中，输出层的主要意义是将解码器的输出转换为最终的预测结果。具体来说，输出层的作用包括：\n",
    "\n",
    "1. **线性变换**：将解码器的输出（通常是高维向量，在我们的例子中`输出: torch.Size([32, 50, 512])`）映射到词汇表大小的维度。这一步通过一个线性层（`nn.Linear`）实现，生成每个位置每个词的概率分数（logits）。\n",
    "\n",
    "2. **生成概率分布**：通过 Softmax 函数将 logits 转换为概率分布。这个概率分布表示每个位置每个词的概率，模型可以根据这个分布选择最可能的词作为输出。\n",
    "\n",
    "3. **序列生成**：在序列生成任务（如机器翻译、文本生成等）中，输出层的概率分布用于生成最终的序列。通常，模型会选择概率最高的词作为下一个词，逐步生成整个序列。\n",
    "\n",
    "总结来说，输出层的作用是将解码器的抽象表示转换为具体的词汇概率分布，从而生成最终的预测结果。这是 Transformer 模型完成序列生成任务的关键步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits 形状: torch.Size([32, 50, 10000])\n",
      "概率分布形状: torch.Size([32, 50, 10000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(OutputLayer, self).__init__()\n",
    "        # 线性层，将 d_model 维映射到词汇表大小\n",
    "        self.linear = nn.Linear(d_model, vocab_size) # 别忘了我们设置d_model为512，vocab_size为10000\n",
    "        # Softmax 函数，用于生成概率分布\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 线性变换，输出形状: [batch_size, seq_len, vocab_size]\n",
    "        logits = self.linear(x)\n",
    "        # Softmax 生成概率分布\n",
    "        probs = self.softmax(logits)\n",
    "        return logits, probs\n",
    "    \n",
    "\n",
    "# 初始化输出层\n",
    "output_layer = OutputLayer(d_model=512, vocab_size=vocab_size)\n",
    "\n",
    "# 解码器的输出，形状为 [32, 50, 512]\n",
    "decoder_output = torch.randn(32, 50, 512)\n",
    "\n",
    "# 前向传播\n",
    "logits, probs = output_layer(decoder_output)\n",
    "\n",
    "# 检查输出形状\n",
    "print(\"Logits 形状:\", logits.shape)  # 输出: torch.Size([32, 50, 10000])\n",
    "print(\"概率分布形状:\", probs.shape)  # 输出: torch.Size([32, 50, 10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`logits` 形状 `[32, 50, 10000]`**：\n",
    "  - `32` 是批次大小（batch size），表示同时处理 32 个样本。\n",
    "  - `50` 是序列长度（sequence length），表示每个样本有 50 个时间步或位置。\n",
    "  - `10000` 是词汇表大小（vocab size），表示每个位置有 10000 个可能的词。\n",
    "\n",
    "- **`probs` 形状 `[32, 50, 10000]`**：\n",
    "  - 这是 `logits` 经过 Softmax 后的概率分布，形状与 `logits` 相同。\n",
    "  - 每个位置的概率分布总和为 1，表示模型对每个位置预测的词的概率。\n",
    "\n",
    "### 为什么是这种形状？\n",
    "- 这种形状符合 Transformer 的输出设计，适用于序列生成任务（如机器翻译、文本生成等）。\n",
    "- 每个位置的概率分布可以用于选择最可能的词，逐步生成输出序列。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
