{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种革命性的深度学习模型架构，主要用于自然语言处理（NLP）任务。它由Google在2017年的论文《Attention is All You Need》中首次提出。以下是Transformer的核心特点：\n",
    "\n",
    "1. **自注意力机制（Self-Attention）**：\n",
    "   - 这是Transformer的核心创新\n",
    "   - 允许模型在处理每个词时关注输入序列中的所有词\n",
    "   - 能够捕捉长距离依赖关系\n",
    "\n",
    "2. **并行计算**：\n",
    "   - 与RNN不同，Transformer可以并行处理整个序列\n",
    "   - 大大提高了训练效率\n",
    "\n",
    "3. **编码器-解码器结构**：\n",
    "   - 编码器：将输入序列转换为一系列特征表示\n",
    "   - 解码器：根据编码器的输出生成目标序列\n",
    "\n",
    "4. **位置编码**：\n",
    "   - 由于Transformer没有循环结构，需要额外添加位置信息\n",
    "   - 通过正弦/余弦函数或学习得到的位置编码来实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型可以主要分为以下几个核心部分：\n",
    "\n",
    "1. **输入部分（Input Processing）**\n",
    "   - 词嵌入（Word Embedding）\n",
    "   - 位置编码（Positional Encoding）\n",
    "\n",
    "2. **编码器部分（Encoder）**\n",
    "   - 多头自注意力机制（Multi-Head Self-Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "3. **解码器部分（Decoder）**\n",
    "   - 掩码多头自注意力机制（Masked Multi-Head Self-Attention）\n",
    "   - 编码器-解码器注意力机制（Encoder-Decoder Attention）\n",
    "   - 前馈神经网络（Feed Forward Network）\n",
    "   - 残差连接和层归一化（Residual Connection & Layer Normalization）\n",
    "\n",
    "4. **输出部分（Output）**\n",
    "   - 线性变换（Linear Transformation）\n",
    "   - Softmax层\n",
    "\n",
    "5. **辅助组件**\n",
    "   - 注意力机制（Attention Mechanism）\n",
    "   - 位置前馈网络（Position-wise Feed Forward Network）\n",
    "   - 残差连接（Residual Connections）\n",
    "   - 层归一化（Layer Normalization）\n",
    "\n",
    "每个部分的具体作用：\n",
    "- **输入部分**：将离散的单词转换为连续的向量表示，并加入位置信息\n",
    "- **编码器**：提取输入序列的特征表示\n",
    "- **解码器**：根据编码器的输出和已生成的部分序列，预测下一个单词\n",
    "- **输出部分**：将解码器的输出转换为概率分布，用于预测下一个单词\n",
    "- **辅助组件**：帮助模型更好地训练和收敛\n",
    "\n",
    "这些部分共同构成了Transformer模型，使其能够有效地处理序列数据，并在各种NLP任务中取得优异的表现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Input Processing 🐱 输入处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 词嵌入（Word Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是nn.Embedding？**\n",
    "`nn.Embedding`是PyTorch中的一个模块，用于将离散的整数索引（通常是单词的索引）转换为连续的向量表示。它本质上是一个查找表，其中每个索引对应一个固定大小的向量。\n",
    "\n",
    "#### 2. **主要参数：**\n",
    "- `num_embeddings`：词汇表的大小，即有多少个不同的单词\n",
    "- `embedding_dim`：每个单词向量的维度\n",
    "- `padding_idx`（可选）：用于指定填充符号的索引，该索引对应的向量不会更新\n",
    "- `max_norm`（可选）：如果指定，会对向量进行归一化\n",
    "- `norm_type`（可选）：归一化的类型，默认是L2范数\n",
    "- `scale_grad_by_freq`（可选）：是否根据词频缩放梯度\n",
    "- `sparse`（可选）：是否使用稀疏梯度更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3. **独立使用示例：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入索引： tensor([2, 5, 1])\n",
      "输出向量：\n",
      " tensor([[-0.2993,  0.3595, -0.6111],\n",
      "        [-0.1720, -0.1778, -0.2709],\n",
      "        [-1.5210,  2.2073, -0.2865]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设我们有一个词汇表，包含10个单词\n",
    "vocab_size = 10\n",
    "# 每个单词用3维向量表示\n",
    "embedding_dim = 3\n",
    "\n",
    "# 创建Embedding层\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# 输入是一个包含单词索引的张量\n",
    "# 例如：[2, 5, 1] 表示一个包含3个单词的句子\n",
    "input_indices = torch.tensor([2, 5, 1])\n",
    "\n",
    "# 通过Embedding层获取对应的词向量\n",
    "output_vectors = embedding(input_indices)\n",
    "\n",
    "print(\"输入索引：\", input_indices)\n",
    "print(\"输出向量：\\n\", output_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **输出的解释**\n",
    "\n",
    "- 每个单词索引（如2, 5, 1）被转换为一个3维向量\n",
    "- 这些向量是随机初始化的，可以在训练过程中学习\n",
    "- `grad_fn`表示这些向量是可训练的，会随着模型训练而更新\n",
    "\n",
    "#### 5. **实际应用场景：**\n",
    "- 自然语言处理（NLP）中，用于将单词转换为向量\n",
    "- 推荐系统中，用于将用户ID或物品ID转换为向量\n",
    "- 任何需要将离散索引映射到连续向量的场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 位置编码 🐱 Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **什么是位置编码？**\n",
    "位置编码（Positional Encoding）是Transformer模型中用于为输入序列添加位置信息的一种方法。由于Transformer没有像RNN那样的循环结构，它需要额外的机制来理解单词在序列中的位置。\n",
    "\n",
    "#### 2. **为什么需要位置编码？**\n",
    "- **Transformer的局限性**：Transformer使用自注意力机制，可以并行处理整个序列，但无法直接获取序列中元素的位置信息\n",
    "- **保持顺序信息**：自然语言中，单词的顺序非常重要，位置编码帮助模型理解这种顺序\n",
    "- **捕捉相对位置**：位置编码的设计使得模型能够捕捉到元素之间的相对位置关系\n",
    "\n",
    "#### 3. **位置编码的公式：**\n",
    "位置编码使用正弦和余弦函数的组合：\n",
    "```\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "```\n",
    "其中：\n",
    "- `pos`：单词在序列中的位置\n",
    "- `i`：维度索引\n",
    "- `d_model`：模型的维度\n",
    "\n",
    "#### 4. **位置编码的特点：**\n",
    "- **周期性**：使用正弦和余弦函数，使得编码具有周期性\n",
    "- **可学习性**：虽然位置编码是固定的，但模型可以通过学习来利用这些信息\n",
    "- **相对位置**：不同位置之间的编码关系可以帮助模型理解相对位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. **独立使用示例：**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词向量形状： torch.Size([2, 10, 16])\n",
      "位置编码形状： torch.Size([1, 100, 16])\n",
      "添加位置编码后的形状： torch.Size([2, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 模型的维度，即每个词向量的长度\n",
    "# 这个值决定了位置编码和词嵌入的维度\n",
    "# 通常选择2的幂次方（如16, 32, 64, 128, 256, 512等）\n",
    "# 较大的维度可以捕捉更丰富的信息，但会增加计算量\n",
    "d_model = 16\n",
    "\n",
    "# 最大序列长度，即位置编码支持的最长序列\n",
    "# 这个值应该大于或等于实际输入序列的最大长度\n",
    "# 如果输入序列超过这个长度，位置编码将无法正确表示\n",
    "# 通常设置为一个足够大的值（如100, 200, 512, 1024等）\n",
    "max_len = 100\n",
    "\n",
    "# 批量大小，即一次处理的样本数量\n",
    "# 较大的批量大小可以提高训练效率，但需要更多内存\n",
    "# 通常根据GPU内存大小和模型复杂度来选择\n",
    "batch_size = 2\n",
    "\n",
    "# 序列长度，即每个样本的单词数量\n",
    "# 这个值应该小于或等于max_len\n",
    "# 如果序列长度不同，通常需要进行填充或截断\n",
    "# 在实际应用中，这个值会根据具体任务而变化\n",
    "seq_len = 10\n",
    "\n",
    "# 假设我们有一些随机生成的词向量\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 创建位置编码器\n",
    "pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# 添加位置编码\n",
    "output = pos_encoder(word_embeddings)\n",
    "\n",
    "print(\"原始词向量形状：\", word_embeddings.shape)\n",
    "print(\"位置编码形状：\", pos_encoder.pe.shape)\n",
    "print(\"添加位置编码后的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **输出解释：**\n",
    "\n",
    "```python\n",
    "原始词向量形状： torch.Size([2, 10, 16])\n",
    "位置编码形状： torch.Size([1, 100, 16])\n",
    "添加位置编码后的形状： torch.Size([2, 10, 16])\n",
    "```\n",
    "\n",
    "这些输出形状反映了Transformer模型中输入处理的不同阶段：\n",
    "\n",
    "1. **原始词向量形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小（batch_size），表示同时处理2个样本\n",
    "   - `10`：序列长度（seq_len），表示每个样本包含10个单词\n",
    "   - `16`：模型维度（d_model），表示每个单词用16维向量表示\n",
    "\n",
    "2. **位置编码形状：torch.Size([1, 100, 16])**\n",
    "   - `1`：表示位置编码是固定的，对所有样本都相同\n",
    "   - `100`：最大序列长度（max_len），表示位置编码支持的最长序列\n",
    "   - `16`：模型维度（d_model），与词向量维度一致，方便相加\n",
    "\n",
    "3. **添加位置编码后的形状：torch.Size([2, 10, 16])**\n",
    "   - `2`：批量大小保持不变\n",
    "   - `10`：序列长度保持不变\n",
    "   - `16`：模型维度保持不变\n",
    "\n",
    "**维度一致性的原因：**\n",
    "- 位置编码的维度`[1, 100, 16]`中，`1`表示位置编码是共享的，`100`是预先生成的最大长度，`16`与词向量维度一致\n",
    "- 在实际使用时，我们只取前`seq_len`个位置编码（`pos_encoder.pe[:, :seq_len, :]`），因此可以与词向量`[2, 10, 16]`直接相加\n",
    "- 相加操作利用了PyTorch的广播机制，将`[1, 10, 16]`的位置编码广播到`[2, 10, 16]`，与词向量逐元素相加\n",
    "\n",
    "这种设计确保了：\n",
    "1. 位置信息能够正确地添加到每个单词的向量表示中\n",
    "2. 不同样本可以共享相同的位置编码，提高效率\n",
    "3. 模型能够处理不同长度的序列，只要不超过最大长度`max_len`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerPreprocessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len):\n",
    "        super(TransformerPreprocessor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_len, d_model)\n",
    "        output = self.position_encoding(embeddings)  # (batch_size, seq_len, d_model)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        self.pe = self._generate_position_encoding()\n",
    "        \n",
    "    def _generate_position_encoding(self):\n",
    "        position = torch.arange(self.max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * \n",
    "                           -(math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "# 使用示例\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "output = preprocessor(input_ids)\n",
    "print(output.shape)  # 输出: torch.Size([32, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Encoder 🐱 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multi-Head Attention 🐱 多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多头注意力机制通过并行计算多个注意力头，捕捉输入序列中不同子空间的特征。每个注意力头独立计算注意力分数，然后将结果拼接起来，最后通过线性变换得到输出。\n",
    "\n",
    "多头注意力机制可以分为以下几个关键步骤：\n",
    "1. 线性变换：将输入映射为查询（Q）、键（K）、值（V）。\n",
    "2. 分割多头：将Q、K、V拆分为多个注意力头。\n",
    "3. 计算注意力分数：计算Q和K的点积，并进行缩放和softmax。\n",
    "4. 加权求和：使用注意力权重对V进行加权求和。\n",
    "5. 拼接多头：将多个注意力头的输出拼接回原始维度。\n",
    "6. 线性变换：对拼接后的结果进行线性变换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1.1 线性变换 Q K V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在多头注意力机制中，**线性变换**是将输入特征映射为查询（Q）、键（K）、值（V）的关键步骤。以下是详细解释：\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **线性变换的定义**\n",
    "线性变换是通过矩阵乘法将输入特征映射到新的特征空间。具体来说：\n",
    "- 输入：`x`，形状为`(batch_size, seq_len, d_model)`。\n",
    "- 输出：`Q`、`K`、`V`，形状仍为`(batch_size, seq_len, d_model)`，但特征表示已经不同。\n",
    "\n",
    "数学公式：\n",
    "```python\n",
    "Q = x · W_Q\n",
    "K = x · W_K\n",
    "V = x · W_V\n",
    "```\n",
    "其中：\n",
    "- `W_Q`、`W_K`、`W_V`是可学习的权重矩阵，形状为`(d_model, d_model)`。\n",
    "- `·`表示矩阵乘法。\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **线性变换的作用**\n",
    "- **特征空间的转换**：\n",
    "  - 输入特征`x`可能是词嵌入或位置编码后的表示，这些特征不一定适合直接用于计算注意力分数。\n",
    "  - 通过线性变换，将`x`映射到更适合计算注意力的特征空间。\n",
    "- **增加模型的表达能力**：\n",
    "  - 线性变换引入了可学习的参数，使模型能够根据任务需求动态调整Q、K、V的表示。\n",
    "  - 这样，模型可以捕捉输入序列中更复杂的依赖关系。\n",
    "- **分离不同的角色**：\n",
    "  - Q、K、V在注意力机制中扮演不同的角色：\n",
    "    - **Q（Query）**：表示当前需要关注的位置。\n",
    "    - **K（Key）**：表示其他位置的特征，用于与Q计算相似度。\n",
    "    - **V（Value）**：表示其他位置的实际信息，用于加权求和。\n",
    "  - 通过独立的线性变换，Q、K、V可以学习到不同的特征表示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))  # 随机生成输入\n",
    "x = preprocessor(input_ids)\n",
    "# print(\"输入 x:\\n\", x)\n",
    "print(\"输入 x 的形状：\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们定义三个线性变换层，分别用于生成Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "query = nn.Linear(d_model, d_model)  # 查询变换\n",
    "key = nn.Linear(d_model, d_model)    # 键变换\n",
    "value = nn.Linear(d_model, d_model)  # 值变换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过线性变换将输入`x`映射为Q、K、V："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[[ 0.2732, -0.7365, -0.1124,  ..., -2.2471,  0.5231,  0.0606],\n",
      "         [ 0.5000, -1.3261,  0.1891,  ..., -0.9225, -0.6997,  0.2025],\n",
      "         [ 0.9889, -0.9228,  0.2250,  ..., -0.3156, -0.2861, -0.3520],\n",
      "         ...,\n",
      "         [ 1.6322, -1.5255,  0.1453,  ..., -0.7419,  0.3134, -0.6718],\n",
      "         [-0.7001, -1.6303,  0.7533,  ..., -0.6353,  0.3861, -0.3624],\n",
      "         [ 0.2828, -0.3661, -0.1991,  ..., -0.6569,  0.0854,  0.7372]],\n",
      "\n",
      "        [[ 0.4600, -1.5325, -1.4598,  ..., -1.1366, -0.1702, -0.1107],\n",
      "         [ 0.2595, -1.1239, -1.2906,  ..., -1.5446,  0.4111, -1.0944],\n",
      "         [ 0.6114, -0.9706, -0.0660,  ..., -1.5545, -0.8888,  0.5346],\n",
      "         ...,\n",
      "         [-0.1139,  0.4386,  0.0652,  ..., -1.0135, -0.8894,  0.6492],\n",
      "         [-0.0807, -0.6225, -0.5950,  ...,  0.2132,  0.3429, -0.0104],\n",
      "         [ 0.4448, -0.6165, -0.5275,  ..., -0.9127, -0.7194,  0.4166]],\n",
      "\n",
      "        [[-0.1763, -0.8093, -0.4763,  ..., -1.0283, -1.1057, -0.2088],\n",
      "         [-0.7232, -0.2563,  0.3804,  ..., -1.3406, -0.4945, -0.4163],\n",
      "         [ 0.0707, -0.7439,  0.4636,  ..., -1.2199, -0.2354, -0.3464],\n",
      "         ...,\n",
      "         [ 0.9873, -0.1992, -0.2559,  ...,  0.1996,  0.3538, -0.0238],\n",
      "         [ 0.0810, -0.9203, -0.2543,  ..., -1.6015, -0.5012,  0.5286],\n",
      "         [-0.5723, -1.8040, -0.7579,  ...,  0.1981,  1.2107, -0.7263]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3431, -1.4633, -0.0795,  ..., -0.6479,  0.1715,  0.3289],\n",
      "         [-0.3397, -1.0336,  0.4499,  ..., -1.0626, -0.0890, -0.1895],\n",
      "         [-0.6877,  0.3425, -0.3833,  ..., -0.8700, -0.5917, -0.4135],\n",
      "         ...,\n",
      "         [ 0.9466, -0.2708,  0.3942,  ..., -1.3737, -0.9278, -0.4294],\n",
      "         [ 1.2488, -0.8833,  0.3545,  ..., -0.3175,  0.0570,  0.1000],\n",
      "         [-0.0196, -1.4166,  0.1613,  ...,  0.4956, -0.4468, -0.1252]],\n",
      "\n",
      "        [[ 0.4496,  1.2145, -0.2039,  ...,  0.0663, -1.7919, -0.5174],\n",
      "         [ 0.3321, -0.7121, -0.7975,  ..., -1.4467, -0.7612, -0.0758],\n",
      "         [-0.0721, -0.5508, -0.5617,  ..., -0.6794, -1.1602, -0.4728],\n",
      "         ...,\n",
      "         [ 0.7942, -0.5395, -0.8991,  ..., -2.1009, -0.5882, -0.3233],\n",
      "         [ 0.7871, -0.6202, -1.2191,  ..., -0.8463, -1.0027, -0.2391],\n",
      "         [ 0.3888, -0.2743, -0.5532,  ..., -0.6310,  0.6296, -0.2333]],\n",
      "\n",
      "        [[-0.5152, -1.3024, -1.2887,  ..., -0.5967, -0.9337, -0.5049],\n",
      "         [ 0.4441,  0.2748, -1.2391,  ..., -2.3803,  0.2357, -0.2319],\n",
      "         [ 0.5523, -0.0446, -0.2634,  ..., -1.0467, -0.1796,  0.8611],\n",
      "         ...,\n",
      "         [-0.1060,  0.4227, -0.7067,  ..., -0.5841, -0.5542, -0.2375],\n",
      "         [-0.5399,  0.1538, -0.7483,  ...,  0.2211,  0.2251, -1.5119],\n",
      "         [-0.2095, -1.5932, -0.6437,  ..., -0.6159, -0.1613, -0.1904]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Q 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "K:\n",
      " tensor([[[ 0.4407, -0.8426,  0.1613,  ...,  0.0867, -0.7430, -0.3447],\n",
      "         [-0.3466, -0.2798,  0.3221,  ...,  1.0570, -1.4463, -0.2273],\n",
      "         [-0.2780, -0.2950, -0.3136,  ...,  1.0181, -0.3185, -0.9697],\n",
      "         ...,\n",
      "         [ 0.2739,  0.1216, -0.3173,  ...,  0.5131,  0.2462, -0.1806],\n",
      "         [-0.6933, -0.4614,  0.3873,  ...,  1.5297,  0.0088,  0.3498],\n",
      "         [-0.9110,  0.0581, -0.3825,  ...,  0.0494, -0.5486, -0.8849]],\n",
      "\n",
      "        [[-0.2684, -1.1970, -0.1057,  ...,  0.4552, -0.1710, -1.2539],\n",
      "         [-0.4099,  0.2448, -0.0120,  ..., -0.5035, -0.8256,  0.3951],\n",
      "         [-0.9877,  0.3012, -0.2241,  ...,  0.7722, -0.2802, -0.1646],\n",
      "         ...,\n",
      "         [ 0.9227,  1.2152, -0.3863,  ...,  1.3888, -0.3753,  1.1442],\n",
      "         [-1.0734, -0.8536,  0.1022,  ...,  1.1274, -0.9362, -0.1856],\n",
      "         [-0.4376,  0.4295,  0.3881,  ...,  0.5318, -1.1281, -0.2414]],\n",
      "\n",
      "        [[-0.8070,  0.0226, -0.3058,  ...,  0.2682, -0.0497, -0.7331],\n",
      "         [-0.1451,  1.0135, -0.7184,  ..., -0.1229, -0.1649,  0.9159],\n",
      "         [-2.0772,  0.1679, -0.7206,  ...,  1.0303, -0.9125, -0.4684],\n",
      "         ...,\n",
      "         [ 0.5767,  0.5573,  0.4292,  ...,  0.2181,  0.0468,  1.1132],\n",
      "         [-0.7763, -0.2780, -0.1398,  ...,  1.9561,  0.1866,  0.4828],\n",
      "         [-0.9858, -0.6062, -0.0358,  ...,  0.3134,  0.7525,  0.0925]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2084, -0.3407, -0.9398,  ..., -0.1399,  0.2301, -0.3162],\n",
      "         [ 0.5919, -0.7896, -0.8726,  ..., -0.7519, -0.3388, -0.0701],\n",
      "         [-0.1143, -0.2115, -0.3799,  ...,  0.6539, -1.0873,  0.1686],\n",
      "         ...,\n",
      "         [-0.2226, -0.3161,  0.3714,  ...,  0.5049, -1.0189, -0.5034],\n",
      "         [-0.6028, -0.3618, -0.2179,  ...,  1.2761, -0.3524,  0.4147],\n",
      "         [-0.9888, -0.2097, -0.4930,  ...,  1.0856, -0.8986,  0.1818]],\n",
      "\n",
      "        [[-0.1562, -0.4658,  0.4305,  ...,  0.6039, -1.0596, -0.5439],\n",
      "         [ 0.0283, -0.9981,  0.9154,  ..., -0.0592, -0.4115, -0.4387],\n",
      "         [ 0.8779,  0.2775, -0.5220,  ...,  0.9660,  0.2945, -0.6118],\n",
      "         ...,\n",
      "         [-0.2136, -0.9377, -0.8647,  ...,  0.6108, -0.5921, -0.4469],\n",
      "         [-0.5646, -0.3424, -0.5914,  ..., -0.2038, -0.2141,  0.9225],\n",
      "         [-0.5180,  0.6432, -0.9695,  ...,  0.8906,  1.1095, -0.9033]],\n",
      "\n",
      "        [[-1.1553, -0.5439, -0.6425,  ..., -0.1878, -1.0846, -0.8226],\n",
      "         [-0.0700, -0.8311, -0.1222,  ...,  0.8891, -1.5817, -0.1885],\n",
      "         [-0.6901, -0.1305, -0.6135,  ...,  1.2117, -0.4119, -0.7193],\n",
      "         ...,\n",
      "         [ 0.0364, -0.3353,  0.0346,  ...,  0.8645, -0.4286,  0.0617],\n",
      "         [-2.0534,  0.1409, -0.9102,  ...,  0.8162,  0.2583,  0.9525],\n",
      "         [-0.0396, -0.3130,  0.1706,  ...,  0.5979, -0.2631, -0.2263]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "K 的形状： torch.Size([32, 50, 512])\n",
      "\n",
      "\n",
      "V:\n",
      " tensor([[[ 0.7828, -1.0593,  0.5050,  ...,  0.0952,  0.7534,  0.7800],\n",
      "         [ 0.1285, -0.1961,  0.3392,  ..., -0.2445,  0.2283,  0.1829],\n",
      "         [ 0.0526,  0.2388, -0.5361,  ..., -1.1301,  0.1213, -0.6222],\n",
      "         ...,\n",
      "         [-0.1283,  1.0007,  0.5090,  ..., -1.4504, -0.3247, -0.2891],\n",
      "         [ 0.1703,  0.2500,  0.6520,  ..., -0.8410,  0.0882,  0.2806],\n",
      "         [-0.0415, -0.4811,  1.4688,  ...,  0.4616, -1.3752,  0.0443]],\n",
      "\n",
      "        [[-0.1015,  0.3149,  1.0414,  ..., -0.4366,  0.0174, -0.1575],\n",
      "         [-0.4286, -0.9935,  1.5741,  ...,  0.1280, -0.1443, -0.4230],\n",
      "         [ 0.4901,  0.2654,  1.7171,  ..., -0.0482,  0.5634, -0.0963],\n",
      "         ...,\n",
      "         [-0.7812,  0.0087,  0.4904,  ..., -0.2979, -1.0543, -0.3421],\n",
      "         [ 0.6126,  0.0449,  0.3487,  ..., -0.9072,  0.1946,  1.0796],\n",
      "         [ 0.5995,  0.6841,  0.8175,  ..., -0.0868, -1.0435,  0.0135]],\n",
      "\n",
      "        [[-0.0909, -0.5033,  0.6990,  ..., -1.0297, -0.5779,  1.0394],\n",
      "         [ 0.3415,  0.9678,  1.6186,  ..., -0.9765, -0.1622, -0.4541],\n",
      "         [ 0.3480, -0.1887,  0.7344,  ..., -0.4235,  0.1910,  0.9819],\n",
      "         ...,\n",
      "         [ 0.0387,  1.3162,  1.0962,  ..., -0.8158,  0.6804, -0.5194],\n",
      "         [-0.6249,  1.1945,  0.5804,  ..., -0.1311, -0.1236,  0.0202],\n",
      "         [ 0.4356,  0.9637,  1.3349,  ..., -0.3008, -0.6640,  0.4200]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5090, -0.4509,  0.6133,  ..., -0.2396, -0.3768, -0.3506],\n",
      "         [-0.4328,  0.0244,  1.3979,  ..., -0.7090, -0.2627,  0.5556],\n",
      "         [-0.0936,  0.0509,  0.2043,  ..., -0.4554,  0.6306,  1.2950],\n",
      "         ...,\n",
      "         [-0.1333, -0.7664,  1.1590,  ..., -1.4069,  0.5106, -0.6985],\n",
      "         [-0.0448,  1.5581,  1.4130,  ..., -1.1807,  0.3351,  0.0089],\n",
      "         [-0.9264,  1.0530, -0.0376,  ..., -0.7651,  0.1754, -0.6820]],\n",
      "\n",
      "        [[ 0.8657, -1.0534,  0.4436,  ..., -1.3152, -0.1034,  0.5697],\n",
      "         [-0.7003, -0.8027,  0.9139,  ...,  0.9079,  0.1819, -0.4343],\n",
      "         [-0.6166, -0.8779,  0.8111,  ..., -0.7687, -0.2820,  0.3154],\n",
      "         ...,\n",
      "         [-0.1064,  0.6425,  1.3999,  ...,  0.1734, -0.9380,  1.0558],\n",
      "         [-0.2758,  0.2441,  0.5184,  ..., -0.7994, -0.3701, -0.0445],\n",
      "         [ 0.6347,  0.3344,  0.5597,  ...,  0.2573, -0.7914, -0.1394]],\n",
      "\n",
      "        [[ 0.8994, -1.1836,  0.2474,  ..., -0.8496,  0.4461,  0.5147],\n",
      "         [ 0.5387,  1.4580,  0.2105,  ...,  0.1276,  0.0712,  1.3083],\n",
      "         [-1.0350, -0.6528,  0.1274,  ..., -0.1709,  0.3680, -0.8239],\n",
      "         ...,\n",
      "         [ 0.3169, -0.0557,  1.3134,  ..., -0.4533,  0.5148,  0.3527],\n",
      "         [-0.2073,  0.9964,  0.6922,  ..., -0.8388,  0.2522, -0.0950],\n",
      "         [-0.3435,  1.2443,  0.2678,  ..., -0.0120,  0.8712, -0.1020]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "V 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = query(x)  # (batch_size, seq_len, d_model)\n",
    "K = key(x)    # (batch_size, seq_len, d_model)\n",
    "V = value(x)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K:\\n\", K)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V:\\n\", V)\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.2 分割多头 🐱 将 Q K V 分割为多个注意力头\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **分割多头的目的**\n",
    "- **并行计算**：通过将Q、K、V拆分为多个注意力头，可以并行计算多个注意力分数，提高计算效率。\n",
    "- **捕捉不同特征**：每个注意力头可以关注输入序列中的不同子空间，捕捉更丰富的特征。\n",
    "\n",
    "\n",
    "##### 2. **分割多头的实现**\n",
    "假设：\n",
    "- `d_model`：模型维度（例如512，如之前所示）。\n",
    "- `num_heads`：注意力头的数量（例如16）。\n",
    "- `head_dim`：每个注意力头的维度（`d_model // num_heads`，例如512 // 16 = 32）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Q, K, V 已经通过线性变换生成\n",
    "batch_size, seq_len, d_model = Q.shape\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "\n",
      "\n",
      "V 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 16\n",
    "head_dim = d_model // num_heads\n",
    "# 分割多头：将 d_model 维度拆分为 num_heads * head_dim\n",
    "#　将`num_heads`维度提到前面，方便后续并行计算。\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"\\n\")\n",
    "print(\"K 的形状：\", K.shape)\n",
    "print(\"\\n\")\n",
    "print(\"V 的形状：\", V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **代码解释**\n",
    "1. **`view`操作**：\n",
    "   - 将`d_model`维度拆分为`num_heads * head_dim`。\n",
    "   - 例如，如果`d_model=５１２`，`num_heads=１６`，则`head_dim=３２`。\n",
    "   - 结果形状为`(batch_size, seq_len, num_heads, head_dim)`。\n",
    "\n",
    "2. **`transpose`操作**：\n",
    "   - 将`num_heads`维度提到前面，方便后续并行计算。\n",
    "   - 结果形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.3 计算注意力分数 🐱 Q 与 K 的点积\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **计算注意力分数（点积）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **解释**：\n",
    "  - 计算Q和K的点积，得到注意力分数。\n",
    "  - `Q`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K`的形状为`(batch_size, num_heads, seq_len, head_dim)`。\n",
    "  - `K.transpose(-2, -1)`将K的最后两个维度转置，形状变为`(batch_size, num_heads, head_dim, seq_len)`。\n",
    "  - 点积结果`score`的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **2. 缩放**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缩放后的注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "scores = scores / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "print(\"缩放后的注意力分数 scores 的形状：\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 使用`sqrt(head_dim)`对点积结果进行缩放。\n",
    "  - 这是为了防止点积结果过大，导致softmax的梯度消失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重 attention 的形状： torch.Size([32, 16, 50, 50])\n",
      "注意力权重 attention 的值：\n",
      " tensor([[[[0.0085, 0.0292, 0.0221,  ..., 0.0115, 0.0254, 0.0184],\n",
      "          [0.0478, 0.0189, 0.0133,  ..., 0.0174, 0.0243, 0.0122],\n",
      "          [0.0212, 0.0160, 0.0088,  ..., 0.0244, 0.0285, 0.0274],\n",
      "          ...,\n",
      "          [0.0275, 0.0216, 0.0180,  ..., 0.0156, 0.0275, 0.0180],\n",
      "          [0.0279, 0.0138, 0.0177,  ..., 0.0118, 0.0167, 0.0285],\n",
      "          [0.0241, 0.0110, 0.0352,  ..., 0.0173, 0.0144, 0.0309]],\n",
      "\n",
      "         [[0.0064, 0.0199, 0.0203,  ..., 0.0281, 0.0179, 0.0117],\n",
      "          [0.0116, 0.0215, 0.0172,  ..., 0.0215, 0.0128, 0.0219],\n",
      "          [0.0113, 0.0325, 0.0139,  ..., 0.0271, 0.0225, 0.0201],\n",
      "          ...,\n",
      "          [0.0167, 0.0231, 0.0171,  ..., 0.0246, 0.0157, 0.0229],\n",
      "          [0.0057, 0.0168, 0.0130,  ..., 0.0310, 0.0223, 0.0487],\n",
      "          [0.0158, 0.0174, 0.0105,  ..., 0.0242, 0.0121, 0.0231]],\n",
      "\n",
      "         [[0.0202, 0.0288, 0.0119,  ..., 0.0343, 0.0358, 0.0204],\n",
      "          [0.0320, 0.0200, 0.0192,  ..., 0.0237, 0.0152, 0.0261],\n",
      "          [0.0236, 0.0257, 0.0153,  ..., 0.0330, 0.0315, 0.0235],\n",
      "          ...,\n",
      "          [0.0170, 0.0162, 0.0126,  ..., 0.0307, 0.0223, 0.0166],\n",
      "          [0.0339, 0.0156, 0.0180,  ..., 0.0097, 0.0199, 0.0081],\n",
      "          [0.0159, 0.0146, 0.0157,  ..., 0.0268, 0.0271, 0.0191]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0270, 0.0354, 0.0209,  ..., 0.0083, 0.0238, 0.0096],\n",
      "          [0.0234, 0.0235, 0.0259,  ..., 0.0085, 0.0200, 0.0177],\n",
      "          [0.0075, 0.0314, 0.0487,  ..., 0.0090, 0.0389, 0.0240],\n",
      "          ...,\n",
      "          [0.0138, 0.0133, 0.0152,  ..., 0.0123, 0.0149, 0.0105],\n",
      "          [0.0296, 0.0231, 0.0186,  ..., 0.0139, 0.0285, 0.0096],\n",
      "          [0.0164, 0.0177, 0.0148,  ..., 0.0283, 0.0308, 0.0242]],\n",
      "\n",
      "         [[0.0168, 0.0135, 0.0206,  ..., 0.0117, 0.0124, 0.0136],\n",
      "          [0.0076, 0.0193, 0.0118,  ..., 0.0209, 0.0167, 0.0096],\n",
      "          [0.0252, 0.0194, 0.0215,  ..., 0.0131, 0.0217, 0.0111],\n",
      "          ...,\n",
      "          [0.0101, 0.0210, 0.0145,  ..., 0.0096, 0.0112, 0.0127],\n",
      "          [0.0271, 0.0177, 0.0275,  ..., 0.0086, 0.0096, 0.0120],\n",
      "          [0.0134, 0.0138, 0.0219,  ..., 0.0078, 0.0138, 0.0198]],\n",
      "\n",
      "         [[0.0247, 0.0188, 0.0066,  ..., 0.0116, 0.0079, 0.0147],\n",
      "          [0.0175, 0.0204, 0.0093,  ..., 0.0103, 0.0078, 0.0222],\n",
      "          [0.0257, 0.0318, 0.0108,  ..., 0.0250, 0.0132, 0.0353],\n",
      "          ...,\n",
      "          [0.0253, 0.0158, 0.0199,  ..., 0.0222, 0.0131, 0.0129],\n",
      "          [0.0310, 0.0159, 0.0274,  ..., 0.0192, 0.0113, 0.0140],\n",
      "          [0.0083, 0.0188, 0.0091,  ..., 0.0171, 0.0105, 0.0133]]],\n",
      "\n",
      "\n",
      "        [[[0.0239, 0.0117, 0.0149,  ..., 0.0107, 0.0162, 0.0162],\n",
      "          [0.0214, 0.0213, 0.0237,  ..., 0.0128, 0.0124, 0.0140],\n",
      "          [0.0282, 0.0162, 0.0112,  ..., 0.0314, 0.0384, 0.0165],\n",
      "          ...,\n",
      "          [0.0264, 0.0153, 0.0200,  ..., 0.0378, 0.0534, 0.0186],\n",
      "          [0.0213, 0.0118, 0.0156,  ..., 0.0321, 0.0324, 0.0101],\n",
      "          [0.0322, 0.0128, 0.0146,  ..., 0.0236, 0.0377, 0.0166]],\n",
      "\n",
      "         [[0.0183, 0.0141, 0.0114,  ..., 0.0095, 0.0322, 0.0283],\n",
      "          [0.0177, 0.0174, 0.0182,  ..., 0.0236, 0.0193, 0.0226],\n",
      "          [0.0165, 0.0305, 0.0108,  ..., 0.0435, 0.0224, 0.0211],\n",
      "          ...,\n",
      "          [0.0099, 0.0104, 0.0126,  ..., 0.0094, 0.0259, 0.0164],\n",
      "          [0.0306, 0.0278, 0.0052,  ..., 0.0342, 0.0480, 0.0325],\n",
      "          [0.0261, 0.0281, 0.0101,  ..., 0.0222, 0.0176, 0.0209]],\n",
      "\n",
      "         [[0.0140, 0.0229, 0.0162,  ..., 0.0332, 0.0407, 0.0305],\n",
      "          [0.0166, 0.0249, 0.0266,  ..., 0.0266, 0.0276, 0.0316],\n",
      "          [0.0150, 0.0147, 0.0164,  ..., 0.0167, 0.0549, 0.0181],\n",
      "          ...,\n",
      "          [0.0289, 0.0259, 0.0401,  ..., 0.0195, 0.0146, 0.0229],\n",
      "          [0.0189, 0.0153, 0.0204,  ..., 0.0176, 0.0152, 0.0242],\n",
      "          [0.0225, 0.0291, 0.0215,  ..., 0.0202, 0.0217, 0.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0158, 0.0215, 0.0093,  ..., 0.0051, 0.0363, 0.0242],\n",
      "          [0.0195, 0.0286, 0.0179,  ..., 0.0067, 0.0189, 0.0215],\n",
      "          [0.0111, 0.0398, 0.0163,  ..., 0.0116, 0.0251, 0.0191],\n",
      "          ...,\n",
      "          [0.0166, 0.0253, 0.0506,  ..., 0.0094, 0.0168, 0.0151],\n",
      "          [0.0136, 0.0382, 0.0155,  ..., 0.0052, 0.0294, 0.0081],\n",
      "          [0.0159, 0.0395, 0.0283,  ..., 0.0276, 0.0287, 0.0168]],\n",
      "\n",
      "         [[0.0143, 0.0076, 0.0105,  ..., 0.0189, 0.0132, 0.0077],\n",
      "          [0.0226, 0.0367, 0.0075,  ..., 0.0266, 0.0135, 0.0134],\n",
      "          [0.0142, 0.0391, 0.0086,  ..., 0.0200, 0.0145, 0.0154],\n",
      "          ...,\n",
      "          [0.0253, 0.0323, 0.0201,  ..., 0.0193, 0.0109, 0.0182],\n",
      "          [0.0250, 0.0124, 0.0155,  ..., 0.0279, 0.0058, 0.0123],\n",
      "          [0.0269, 0.0119, 0.0166,  ..., 0.0211, 0.0045, 0.0152]],\n",
      "\n",
      "         [[0.0094, 0.0174, 0.0184,  ..., 0.0092, 0.0152, 0.0240],\n",
      "          [0.0187, 0.0148, 0.0178,  ..., 0.0104, 0.0134, 0.0135],\n",
      "          [0.0135, 0.0325, 0.0174,  ..., 0.0108, 0.0169, 0.0151],\n",
      "          ...,\n",
      "          [0.0152, 0.0280, 0.0188,  ..., 0.0144, 0.0154, 0.0240],\n",
      "          [0.0182, 0.0129, 0.0187,  ..., 0.0119, 0.0145, 0.0121],\n",
      "          [0.0133, 0.0238, 0.0120,  ..., 0.0141, 0.0182, 0.0145]]],\n",
      "\n",
      "\n",
      "        [[[0.0176, 0.0272, 0.0085,  ..., 0.0205, 0.0260, 0.0470],\n",
      "          [0.0196, 0.0164, 0.0071,  ..., 0.0177, 0.0190, 0.0205],\n",
      "          [0.0145, 0.0148, 0.0047,  ..., 0.0190, 0.0244, 0.0275],\n",
      "          ...,\n",
      "          [0.0147, 0.0128, 0.0141,  ..., 0.0185, 0.0172, 0.0050],\n",
      "          [0.0162, 0.0134, 0.0065,  ..., 0.0195, 0.0163, 0.0106],\n",
      "          [0.0293, 0.0176, 0.0107,  ..., 0.0144, 0.0164, 0.0323]],\n",
      "\n",
      "         [[0.0077, 0.0058, 0.0222,  ..., 0.0140, 0.0201, 0.0344],\n",
      "          [0.0185, 0.0248, 0.0140,  ..., 0.0402, 0.0233, 0.0272],\n",
      "          [0.0101, 0.0113, 0.0125,  ..., 0.0188, 0.0205, 0.0165],\n",
      "          ...,\n",
      "          [0.0087, 0.0094, 0.0090,  ..., 0.0219, 0.0339, 0.0188],\n",
      "          [0.0114, 0.0128, 0.0104,  ..., 0.0237, 0.0445, 0.0135],\n",
      "          [0.0070, 0.0088, 0.0135,  ..., 0.0092, 0.0241, 0.0092]],\n",
      "\n",
      "         [[0.0116, 0.0318, 0.0132,  ..., 0.0257, 0.0179, 0.0088],\n",
      "          [0.0112, 0.0306, 0.0145,  ..., 0.0278, 0.0349, 0.0127],\n",
      "          [0.0215, 0.0152, 0.0247,  ..., 0.0225, 0.0214, 0.0229],\n",
      "          ...,\n",
      "          [0.0105, 0.0182, 0.0183,  ..., 0.0135, 0.0147, 0.0121],\n",
      "          [0.0271, 0.0151, 0.0263,  ..., 0.0129, 0.0171, 0.0128],\n",
      "          [0.0146, 0.0281, 0.0232,  ..., 0.0406, 0.0372, 0.0235]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0113, 0.0171, 0.0160,  ..., 0.0137, 0.0131, 0.0148],\n",
      "          [0.0193, 0.0153, 0.0243,  ..., 0.0209, 0.0248, 0.0154],\n",
      "          [0.0186, 0.0158, 0.0194,  ..., 0.0180, 0.0204, 0.0223],\n",
      "          ...,\n",
      "          [0.0418, 0.0252, 0.0360,  ..., 0.0156, 0.0325, 0.0179],\n",
      "          [0.0321, 0.0259, 0.0315,  ..., 0.0223, 0.0140, 0.0078],\n",
      "          [0.0245, 0.0170, 0.0165,  ..., 0.0226, 0.0122, 0.0098]],\n",
      "\n",
      "         [[0.0093, 0.0252, 0.0131,  ..., 0.0117, 0.0300, 0.0113],\n",
      "          [0.0109, 0.0142, 0.0156,  ..., 0.0095, 0.0265, 0.0160],\n",
      "          [0.0214, 0.0196, 0.0156,  ..., 0.0065, 0.0423, 0.0088],\n",
      "          ...,\n",
      "          [0.0125, 0.0205, 0.0132,  ..., 0.0039, 0.0276, 0.0125],\n",
      "          [0.0194, 0.0204, 0.0092,  ..., 0.0227, 0.0171, 0.0240],\n",
      "          [0.0124, 0.0128, 0.0155,  ..., 0.0054, 0.0145, 0.0089]],\n",
      "\n",
      "         [[0.0128, 0.0153, 0.0075,  ..., 0.0164, 0.0134, 0.0160],\n",
      "          [0.0177, 0.0259, 0.0166,  ..., 0.0163, 0.0236, 0.0216],\n",
      "          [0.0166, 0.0140, 0.0266,  ..., 0.0156, 0.0135, 0.0303],\n",
      "          ...,\n",
      "          [0.0140, 0.0117, 0.0173,  ..., 0.0091, 0.0272, 0.0212],\n",
      "          [0.0128, 0.0290, 0.0122,  ..., 0.0177, 0.0160, 0.0210],\n",
      "          [0.0196, 0.0153, 0.0120,  ..., 0.0113, 0.0234, 0.0113]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0155, 0.0143, 0.0246,  ..., 0.0172, 0.0180, 0.0226],\n",
      "          [0.0206, 0.0227, 0.0257,  ..., 0.0187, 0.0305, 0.0160],\n",
      "          [0.0133, 0.0123, 0.0138,  ..., 0.0124, 0.0228, 0.0176],\n",
      "          ...,\n",
      "          [0.0198, 0.0218, 0.0246,  ..., 0.0256, 0.0208, 0.0157],\n",
      "          [0.0432, 0.0175, 0.0169,  ..., 0.0140, 0.0094, 0.0340],\n",
      "          [0.0509, 0.0102, 0.0314,  ..., 0.0251, 0.0208, 0.0121]],\n",
      "\n",
      "         [[0.0270, 0.0232, 0.0217,  ..., 0.0233, 0.0201, 0.0161],\n",
      "          [0.0130, 0.0204, 0.0212,  ..., 0.0153, 0.0208, 0.0083],\n",
      "          [0.0459, 0.0228, 0.0363,  ..., 0.0079, 0.0162, 0.0143],\n",
      "          ...,\n",
      "          [0.0265, 0.0162, 0.0426,  ..., 0.0090, 0.0178, 0.0144],\n",
      "          [0.0099, 0.0211, 0.0249,  ..., 0.0170, 0.0178, 0.0172],\n",
      "          [0.0191, 0.0120, 0.0215,  ..., 0.0135, 0.0318, 0.0138]],\n",
      "\n",
      "         [[0.0251, 0.0302, 0.0190,  ..., 0.0126, 0.0165, 0.0263],\n",
      "          [0.0162, 0.0128, 0.0180,  ..., 0.0115, 0.0288, 0.0442],\n",
      "          [0.0361, 0.0398, 0.0374,  ..., 0.0246, 0.0341, 0.0152],\n",
      "          ...,\n",
      "          [0.0197, 0.0097, 0.0181,  ..., 0.0078, 0.0227, 0.0157],\n",
      "          [0.0209, 0.0174, 0.0269,  ..., 0.0114, 0.0245, 0.0150],\n",
      "          [0.0209, 0.0202, 0.0217,  ..., 0.0200, 0.0171, 0.0186]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0191, 0.0133, 0.0349,  ..., 0.0104, 0.0052, 0.0102],\n",
      "          [0.0246, 0.0089, 0.0541,  ..., 0.0235, 0.0046, 0.0109],\n",
      "          [0.0447, 0.0152, 0.0611,  ..., 0.0144, 0.0137, 0.0216],\n",
      "          ...,\n",
      "          [0.0181, 0.0324, 0.0337,  ..., 0.0234, 0.0155, 0.0151],\n",
      "          [0.0135, 0.0133, 0.0269,  ..., 0.0202, 0.0072, 0.0107],\n",
      "          [0.0285, 0.0232, 0.0163,  ..., 0.0133, 0.0154, 0.0129]],\n",
      "\n",
      "         [[0.0320, 0.0262, 0.0084,  ..., 0.0274, 0.0224, 0.0211],\n",
      "          [0.0303, 0.0163, 0.0092,  ..., 0.0325, 0.0238, 0.0193],\n",
      "          [0.0283, 0.0180, 0.0109,  ..., 0.0313, 0.0152, 0.0196],\n",
      "          ...,\n",
      "          [0.0121, 0.0118, 0.0096,  ..., 0.0085, 0.0327, 0.0140],\n",
      "          [0.0157, 0.0174, 0.0089,  ..., 0.0157, 0.0182, 0.0122],\n",
      "          [0.0171, 0.0123, 0.0175,  ..., 0.0116, 0.0484, 0.0130]],\n",
      "\n",
      "         [[0.0175, 0.0117, 0.0172,  ..., 0.0171, 0.0234, 0.0259],\n",
      "          [0.0201, 0.0269, 0.0230,  ..., 0.0077, 0.0183, 0.0203],\n",
      "          [0.0341, 0.0216, 0.0163,  ..., 0.0057, 0.0081, 0.0135],\n",
      "          ...,\n",
      "          [0.0166, 0.0528, 0.0389,  ..., 0.0149, 0.0157, 0.0198],\n",
      "          [0.0472, 0.0311, 0.0141,  ..., 0.0072, 0.0174, 0.0158],\n",
      "          [0.0321, 0.0221, 0.0348,  ..., 0.0106, 0.0223, 0.0433]]],\n",
      "\n",
      "\n",
      "        [[[0.0169, 0.0133, 0.0451,  ..., 0.0103, 0.0154, 0.0274],\n",
      "          [0.0210, 0.0151, 0.0338,  ..., 0.0246, 0.0199, 0.0297],\n",
      "          [0.0183, 0.0136, 0.0179,  ..., 0.0236, 0.0196, 0.0221],\n",
      "          ...,\n",
      "          [0.0221, 0.0124, 0.0307,  ..., 0.0146, 0.0165, 0.0149],\n",
      "          [0.0312, 0.0155, 0.0242,  ..., 0.0300, 0.0180, 0.0105],\n",
      "          [0.0223, 0.0134, 0.0146,  ..., 0.0157, 0.0114, 0.0195]],\n",
      "\n",
      "         [[0.0195, 0.0281, 0.0205,  ..., 0.0136, 0.0134, 0.0263],\n",
      "          [0.0200, 0.0175, 0.0256,  ..., 0.0227, 0.0145, 0.0235],\n",
      "          [0.0189, 0.0158, 0.0293,  ..., 0.0128, 0.0070, 0.0366],\n",
      "          ...,\n",
      "          [0.0279, 0.0257, 0.0168,  ..., 0.0223, 0.0123, 0.0327],\n",
      "          [0.0351, 0.0258, 0.0192,  ..., 0.0160, 0.0152, 0.0261],\n",
      "          [0.0109, 0.0185, 0.0156,  ..., 0.0262, 0.0097, 0.0206]],\n",
      "\n",
      "         [[0.0021, 0.0145, 0.0157,  ..., 0.0335, 0.0338, 0.0299],\n",
      "          [0.0135, 0.0221, 0.0221,  ..., 0.0290, 0.0190, 0.0397],\n",
      "          [0.0075, 0.0217, 0.0267,  ..., 0.0207, 0.0125, 0.0102],\n",
      "          ...,\n",
      "          [0.0047, 0.0082, 0.0312,  ..., 0.0188, 0.0081, 0.0202],\n",
      "          [0.0156, 0.0154, 0.0365,  ..., 0.0224, 0.0303, 0.0359],\n",
      "          [0.0090, 0.0163, 0.0144,  ..., 0.0179, 0.0221, 0.0279]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0161, 0.0308, 0.0208,  ..., 0.0073, 0.0303, 0.0426],\n",
      "          [0.0242, 0.0254, 0.0160,  ..., 0.0177, 0.0279, 0.0224],\n",
      "          [0.0212, 0.0294, 0.0441,  ..., 0.0260, 0.0393, 0.0211],\n",
      "          ...,\n",
      "          [0.0226, 0.0285, 0.0240,  ..., 0.0100, 0.0090, 0.0096],\n",
      "          [0.0193, 0.0365, 0.0336,  ..., 0.0113, 0.0223, 0.0424],\n",
      "          [0.0237, 0.0792, 0.0501,  ..., 0.0128, 0.0138, 0.0187]],\n",
      "\n",
      "         [[0.0123, 0.0150, 0.0139,  ..., 0.0100, 0.0202, 0.0158],\n",
      "          [0.0107, 0.0145, 0.0173,  ..., 0.0110, 0.0121, 0.0251],\n",
      "          [0.0120, 0.0104, 0.0153,  ..., 0.0220, 0.0304, 0.0199],\n",
      "          ...,\n",
      "          [0.0089, 0.0098, 0.0168,  ..., 0.0107, 0.0093, 0.0148],\n",
      "          [0.0102, 0.0150, 0.0131,  ..., 0.0158, 0.0183, 0.0170],\n",
      "          [0.0066, 0.0138, 0.0259,  ..., 0.0091, 0.0341, 0.0236]],\n",
      "\n",
      "         [[0.0310, 0.0049, 0.0090,  ..., 0.0109, 0.0136, 0.0079],\n",
      "          [0.0292, 0.0097, 0.0107,  ..., 0.0114, 0.0237, 0.0076],\n",
      "          [0.0208, 0.0310, 0.0177,  ..., 0.0116, 0.0191, 0.0130],\n",
      "          ...,\n",
      "          [0.0283, 0.0198, 0.0200,  ..., 0.0159, 0.0230, 0.0096],\n",
      "          [0.0391, 0.0077, 0.0219,  ..., 0.0151, 0.0143, 0.0110],\n",
      "          [0.0314, 0.0120, 0.0308,  ..., 0.0141, 0.0159, 0.0222]]],\n",
      "\n",
      "\n",
      "        [[[0.0151, 0.0353, 0.0236,  ..., 0.0192, 0.0184, 0.0196],\n",
      "          [0.0138, 0.0134, 0.0114,  ..., 0.0166, 0.0213, 0.0203],\n",
      "          [0.0163, 0.0137, 0.0359,  ..., 0.0177, 0.0182, 0.0263],\n",
      "          ...,\n",
      "          [0.0232, 0.0113, 0.0457,  ..., 0.0188, 0.0184, 0.0221],\n",
      "          [0.0057, 0.0073, 0.0369,  ..., 0.0147, 0.0207, 0.0261],\n",
      "          [0.0275, 0.0203, 0.0243,  ..., 0.0262, 0.0230, 0.0292]],\n",
      "\n",
      "         [[0.0147, 0.0179, 0.0256,  ..., 0.0108, 0.0160, 0.0244],\n",
      "          [0.0102, 0.0131, 0.0387,  ..., 0.0096, 0.0162, 0.0223],\n",
      "          [0.0129, 0.0169, 0.0249,  ..., 0.0177, 0.0188, 0.0479],\n",
      "          ...,\n",
      "          [0.0116, 0.0177, 0.0324,  ..., 0.0080, 0.0224, 0.0237],\n",
      "          [0.0209, 0.0242, 0.0252,  ..., 0.0202, 0.0283, 0.0241],\n",
      "          [0.0095, 0.0172, 0.0148,  ..., 0.0154, 0.0127, 0.0487]],\n",
      "\n",
      "         [[0.0167, 0.0134, 0.0252,  ..., 0.0184, 0.0209, 0.0159],\n",
      "          [0.0095, 0.0198, 0.0322,  ..., 0.0111, 0.0403, 0.0114],\n",
      "          [0.0172, 0.0199, 0.0245,  ..., 0.0135, 0.0188, 0.0154],\n",
      "          ...,\n",
      "          [0.0139, 0.0195, 0.0319,  ..., 0.0136, 0.0223, 0.0352],\n",
      "          [0.0202, 0.0236, 0.0117,  ..., 0.0122, 0.0243, 0.0202],\n",
      "          [0.0129, 0.0242, 0.0266,  ..., 0.0131, 0.0268, 0.0187]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0095, 0.0380, 0.0222,  ..., 0.0358, 0.0337, 0.0106],\n",
      "          [0.0243, 0.0353, 0.0171,  ..., 0.0236, 0.0172, 0.0136],\n",
      "          [0.0103, 0.0281, 0.0210,  ..., 0.0132, 0.0124, 0.0092],\n",
      "          ...,\n",
      "          [0.0163, 0.0195, 0.0327,  ..., 0.0278, 0.0229, 0.0179],\n",
      "          [0.0141, 0.0129, 0.0182,  ..., 0.0093, 0.0172, 0.0123],\n",
      "          [0.0172, 0.0281, 0.0245,  ..., 0.0072, 0.0070, 0.0099]],\n",
      "\n",
      "         [[0.0152, 0.0376, 0.0127,  ..., 0.0286, 0.0338, 0.0087],\n",
      "          [0.0085, 0.0130, 0.0228,  ..., 0.0196, 0.0356, 0.0142],\n",
      "          [0.0076, 0.0152, 0.0121,  ..., 0.0179, 0.0283, 0.0074],\n",
      "          ...,\n",
      "          [0.0177, 0.0131, 0.0118,  ..., 0.0150, 0.0095, 0.0178],\n",
      "          [0.0105, 0.0121, 0.0080,  ..., 0.0033, 0.0295, 0.0093],\n",
      "          [0.0147, 0.0113, 0.0123,  ..., 0.0090, 0.0150, 0.0076]],\n",
      "\n",
      "         [[0.0248, 0.0434, 0.0198,  ..., 0.0196, 0.0261, 0.0175],\n",
      "          [0.0183, 0.0255, 0.0085,  ..., 0.0159, 0.0503, 0.0400],\n",
      "          [0.0259, 0.0152, 0.0222,  ..., 0.0085, 0.0398, 0.0272],\n",
      "          ...,\n",
      "          [0.0278, 0.0466, 0.0160,  ..., 0.0305, 0.0393, 0.0189],\n",
      "          [0.0240, 0.0474, 0.0310,  ..., 0.0111, 0.0108, 0.0201],\n",
      "          [0.0233, 0.0365, 0.0156,  ..., 0.0152, 0.0270, 0.0319]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "print(\"注意力权重 attention 的形状：\", attention.shape)\n",
    "print(\"注意力权重 attention 的值：\\n\", attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解释**：\n",
    "  - 对最后一个维度（`seq_len`）进行softmax，得到归一化的注意力权重。\n",
    "  - 注意力权重的形状为`(batch_size, num_heads, seq_len, seq_len)`。\n",
    "\n",
    "**注意力权重用于衡量输入序列中每个位置对其他位置的重要性，并指导模型如何聚合信息。**\n",
    "\n",
    "---\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 加权求和 🐱 使用注意力权重对 V 进行加权求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权求和后的输出 output 的形状： torch.Size([32, 16, 50, 32])\n",
      "加权求和后的输出 output 的值：\n",
      " tensor([[[[-2.5685e-01,  2.1289e-01,  9.5923e-01,  ...,  2.0610e-01,\n",
      "           -3.9970e-03, -3.8988e-01],\n",
      "          [-1.5795e-01,  2.0845e-01,  9.9624e-01,  ...,  1.9469e-01,\n",
      "           -1.6897e-01, -2.5943e-01],\n",
      "          [-2.0496e-01,  2.4569e-01,  9.6513e-01,  ...,  1.4012e-01,\n",
      "           -8.9897e-02, -3.0672e-01],\n",
      "          ...,\n",
      "          [-2.7828e-01,  2.2217e-01,  9.6377e-01,  ...,  1.3426e-01,\n",
      "           -1.3023e-01, -2.8205e-01],\n",
      "          [-2.0562e-01,  2.3800e-01,  1.0627e+00,  ...,  8.7397e-02,\n",
      "           -1.1728e-01, -2.5728e-01],\n",
      "          [-1.8584e-01,  2.3244e-01,  9.1740e-01,  ...,  7.9465e-02,\n",
      "           -1.0504e-01, -3.3069e-01]],\n",
      "\n",
      "         [[ 4.2216e-01, -2.6826e-01,  6.3927e-01,  ...,  8.8094e-02,\n",
      "           -8.8057e-01, -2.2187e-01],\n",
      "          [ 4.1354e-01, -2.5237e-01,  6.8748e-01,  ...,  6.5499e-02,\n",
      "           -8.6878e-01, -2.3861e-01],\n",
      "          [ 3.8318e-01, -2.6173e-01,  7.0321e-01,  ...,  9.1930e-02,\n",
      "           -8.9859e-01, -2.5034e-01],\n",
      "          ...,\n",
      "          [ 3.7606e-01, -3.1200e-01,  6.7121e-01,  ...,  8.3386e-02,\n",
      "           -8.1014e-01, -2.3932e-01],\n",
      "          [ 4.6315e-01, -3.2392e-01,  6.0597e-01,  ...,  1.2781e-01,\n",
      "           -8.8862e-01, -1.7605e-01],\n",
      "          [ 4.1194e-01, -2.8649e-01,  6.3075e-01,  ...,  6.9519e-02,\n",
      "           -8.5338e-01, -2.4529e-01]],\n",
      "\n",
      "         [[-2.0167e-01,  9.6693e-02, -1.2442e-01,  ...,  6.4278e-02,\n",
      "            4.9329e-02,  3.5285e-01],\n",
      "          [-1.9077e-01,  1.7278e-01, -1.7874e-01,  ...,  1.3778e-02,\n",
      "            1.0242e-01,  4.0495e-01],\n",
      "          [-1.9823e-01,  1.8276e-01, -1.7645e-01,  ...,  7.4920e-02,\n",
      "            9.5494e-02,  3.9294e-01],\n",
      "          ...,\n",
      "          [-7.1625e-02,  1.8523e-01, -1.7954e-01,  ...,  3.9847e-02,\n",
      "            9.6464e-02,  4.1258e-01],\n",
      "          [-1.5694e-01,  7.2809e-02, -1.4399e-01,  ...,  4.5156e-02,\n",
      "            1.2754e-01,  3.5630e-01],\n",
      "          [-1.0350e-01,  1.7671e-01, -2.2653e-01,  ...,  3.3513e-02,\n",
      "            1.2526e-01,  3.7952e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0860e-01, -2.8813e-01, -1.6780e-01,  ...,  1.6449e-01,\n",
      "            6.9312e-01, -6.3473e-01],\n",
      "          [-1.3981e-01, -2.8655e-01, -1.7827e-01,  ...,  1.4621e-01,\n",
      "            6.7505e-01, -6.6647e-01],\n",
      "          [-1.4215e-01, -1.8675e-01, -1.4856e-01,  ...,  1.6850e-01,\n",
      "            6.5321e-01, -6.8849e-01],\n",
      "          ...,\n",
      "          [-1.7622e-01, -2.5086e-01, -1.6879e-01,  ...,  1.9605e-01,\n",
      "            6.0929e-01, -6.1344e-01],\n",
      "          [-1.5796e-01, -2.2018e-01, -1.9138e-01,  ...,  1.9719e-01,\n",
      "            6.5194e-01, -6.6032e-01],\n",
      "          [-1.5880e-01, -2.5479e-01, -2.5204e-01,  ...,  1.7904e-01,\n",
      "            6.3605e-01, -6.9197e-01]],\n",
      "\n",
      "         [[-2.1434e-01,  1.8705e-01,  3.8059e-01,  ...,  7.8993e-02,\n",
      "            3.0539e-01,  1.5601e-01],\n",
      "          [-1.4874e-01,  2.2523e-01,  3.6295e-01,  ...,  7.6283e-02,\n",
      "            2.5670e-01,  1.0224e-01],\n",
      "          [-1.8289e-01,  1.3489e-01,  3.2699e-01,  ...,  1.1153e-01,\n",
      "            1.6623e-01,  1.4362e-01],\n",
      "          ...,\n",
      "          [-2.2453e-01,  1.0458e-01,  3.6705e-01,  ...,  1.3448e-01,\n",
      "            2.1226e-01,  9.2466e-02],\n",
      "          [-1.0561e-01,  1.8315e-01,  3.2076e-01,  ...,  1.4436e-01,\n",
      "            2.4711e-01,  9.1492e-02],\n",
      "          [-2.5534e-01,  1.7424e-01,  4.0024e-01,  ...,  7.2531e-02,\n",
      "            2.1797e-01,  1.6525e-01]],\n",
      "\n",
      "         [[-1.4683e-01,  6.0333e-01,  4.9256e-01,  ..., -1.9312e-01,\n",
      "           -3.9690e-01, -1.1445e-01],\n",
      "          [-8.1546e-02,  5.7491e-01,  4.4145e-01,  ..., -1.8892e-01,\n",
      "           -4.6270e-01, -3.4303e-02],\n",
      "          [-8.0075e-02,  6.2224e-01,  4.3780e-01,  ..., -1.2761e-01,\n",
      "           -4.8061e-01,  1.2829e-02],\n",
      "          ...,\n",
      "          [-9.8069e-02,  5.3785e-01,  4.3584e-01,  ..., -2.3383e-01,\n",
      "           -4.5259e-01, -1.1748e-02],\n",
      "          [-1.0205e-01,  5.6508e-01,  4.4833e-01,  ..., -2.2838e-01,\n",
      "           -4.2540e-01, -2.8523e-03],\n",
      "          [-6.1153e-02,  5.2691e-01,  4.8126e-01,  ..., -2.0460e-01,\n",
      "           -5.1167e-01, -1.0293e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6706e-01,  2.0914e-01,  8.9832e-01,  ...,  2.4990e-01,\n",
      "           -2.9379e-01, -3.5280e-01],\n",
      "          [-2.0457e-01,  1.8040e-01,  8.2777e-01,  ...,  1.7973e-01,\n",
      "           -2.5880e-01, -3.7211e-01],\n",
      "          [-1.2792e-01,  2.0480e-01,  8.9634e-01,  ...,  2.2777e-01,\n",
      "           -2.4682e-01, -3.6394e-01],\n",
      "          ...,\n",
      "          [-1.6029e-01,  1.9481e-01,  9.0077e-01,  ...,  1.7759e-01,\n",
      "           -2.7444e-01, -3.3811e-01],\n",
      "          [-1.5964e-01,  1.7924e-01,  9.0473e-01,  ...,  2.0116e-01,\n",
      "           -3.0745e-01, -3.5553e-01],\n",
      "          [-8.2357e-02,  2.5032e-01,  8.8453e-01,  ...,  1.2522e-01,\n",
      "           -3.1891e-01, -3.3529e-01]],\n",
      "\n",
      "         [[ 4.1507e-01, -3.5616e-01,  7.4239e-01,  ..., -2.5951e-02,\n",
      "           -8.7293e-01, -2.0573e-01],\n",
      "          [ 4.0594e-01, -4.0373e-01,  7.5340e-01,  ..., -3.3307e-02,\n",
      "           -9.0156e-01, -1.0878e-01],\n",
      "          [ 4.3113e-01, -3.9668e-01,  6.7381e-01,  ..., -6.6358e-02,\n",
      "           -9.1148e-01, -6.0904e-02],\n",
      "          ...,\n",
      "          [ 4.0203e-01, -3.8734e-01,  7.1959e-01,  ..., -2.4582e-02,\n",
      "           -8.9661e-01, -2.1635e-01],\n",
      "          [ 4.5677e-01, -4.0643e-01,  6.5363e-01,  ..., -3.6022e-02,\n",
      "           -8.7760e-01, -9.7152e-02],\n",
      "          [ 4.3028e-01, -4.0273e-01,  7.0041e-01,  ..., -3.6453e-02,\n",
      "           -8.9015e-01, -1.4087e-01]],\n",
      "\n",
      "         [[-2.9751e-02,  3.1563e-01, -1.9582e-01,  ..., -2.2159e-01,\n",
      "            4.9953e-02,  4.1859e-01],\n",
      "          [-6.7019e-03,  2.7742e-01, -2.2280e-01,  ..., -2.3161e-01,\n",
      "           -3.6572e-02,  3.7283e-01],\n",
      "          [ 3.1898e-02,  2.6199e-01, -2.4526e-01,  ..., -2.1022e-01,\n",
      "           -2.6511e-02,  3.9680e-01],\n",
      "          ...,\n",
      "          [-1.0879e-01,  2.2918e-01, -1.8695e-01,  ..., -1.9041e-01,\n",
      "           -8.6010e-02,  3.4346e-01],\n",
      "          [-7.5313e-03,  2.5177e-01, -1.9110e-01,  ..., -2.4522e-01,\n",
      "           -3.1009e-02,  4.4764e-01],\n",
      "          [ 8.0391e-02,  2.1162e-01, -2.5945e-01,  ..., -2.4141e-01,\n",
      "           -5.0738e-02,  3.8636e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0961e-01, -1.0693e-01, -3.3139e-01,  ...,  1.5944e-01,\n",
      "            6.4702e-01, -5.6677e-01],\n",
      "          [ 1.5260e-01, -5.5092e-02, -2.6990e-01,  ...,  1.7983e-01,\n",
      "            6.1750e-01, -5.6538e-01],\n",
      "          [ 7.3889e-02, -1.4399e-01, -3.6056e-01,  ...,  3.9985e-02,\n",
      "            6.6766e-01, -5.8551e-01],\n",
      "          ...,\n",
      "          [ 7.4576e-02, -1.9133e-01, -3.0938e-01,  ...,  1.1472e-01,\n",
      "            5.9841e-01, -6.2079e-01],\n",
      "          [ 7.3613e-02, -1.7030e-01, -3.8696e-01,  ...,  9.6773e-02,\n",
      "            7.0530e-01, -6.3274e-01],\n",
      "          [ 5.3463e-02, -1.9832e-01, -3.6154e-01,  ...,  3.7969e-02,\n",
      "            6.7266e-01, -6.3103e-01]],\n",
      "\n",
      "         [[-8.6990e-02,  2.3834e-01,  3.3600e-01,  ..., -2.2903e-01,\n",
      "            3.4336e-01, -1.4554e-01],\n",
      "          [-5.7906e-02,  2.2896e-01,  3.4338e-01,  ..., -2.6726e-01,\n",
      "            2.3940e-01, -1.8681e-02],\n",
      "          [-2.1304e-02,  2.4540e-01,  3.6761e-01,  ..., -2.8597e-01,\n",
      "            2.7306e-01, -1.5421e-01],\n",
      "          ...,\n",
      "          [-3.8452e-02,  2.0119e-01,  3.2426e-01,  ..., -2.3282e-01,\n",
      "            2.2900e-01, -3.5649e-02],\n",
      "          [-1.2889e-01,  1.9709e-01,  3.8792e-01,  ..., -2.5748e-01,\n",
      "            2.3416e-01, -1.2453e-02],\n",
      "          [-1.1576e-01,  1.9637e-01,  3.5179e-01,  ..., -2.2480e-01,\n",
      "            2.8932e-01, -2.1326e-02]],\n",
      "\n",
      "         [[-1.7286e-01,  6.9480e-01,  4.2228e-01,  ..., -3.7533e-01,\n",
      "           -4.0458e-01, -8.3042e-02],\n",
      "          [-1.8979e-01,  6.8179e-01,  4.0787e-01,  ..., -3.0819e-01,\n",
      "           -2.4097e-01,  4.5637e-03],\n",
      "          [-2.9407e-01,  6.6005e-01,  4.7441e-01,  ..., -4.2891e-01,\n",
      "           -3.6859e-01, -1.0402e-01],\n",
      "          ...,\n",
      "          [-2.2997e-01,  6.6336e-01,  5.0423e-01,  ..., -3.3904e-01,\n",
      "           -3.9203e-01, -1.0151e-01],\n",
      "          [-2.4279e-01,  6.1845e-01,  4.2643e-01,  ..., -4.2140e-01,\n",
      "           -4.3924e-01, -1.7778e-01],\n",
      "          [-2.0339e-01,  6.9591e-01,  4.7759e-01,  ..., -3.8836e-01,\n",
      "           -3.8855e-01, -1.1773e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.1545e-01,  3.5136e-01,  1.0583e+00,  ...,  3.1006e-01,\n",
      "           -1.9753e-01, -4.2155e-01],\n",
      "          [-6.6401e-02,  3.4579e-01,  9.9144e-01,  ...,  3.0454e-01,\n",
      "           -2.0565e-01, -4.3174e-01],\n",
      "          [-8.1478e-02,  3.0416e-01,  1.0031e+00,  ...,  3.0063e-01,\n",
      "           -2.1957e-01, -3.5795e-01],\n",
      "          ...,\n",
      "          [-1.7352e-01,  2.2784e-01,  1.0177e+00,  ...,  2.3419e-01,\n",
      "           -2.1382e-01, -4.0827e-01],\n",
      "          [-1.4067e-01,  2.5938e-01,  9.8145e-01,  ...,  2.7540e-01,\n",
      "           -2.4710e-01, -3.5129e-01],\n",
      "          [-1.5055e-01,  3.3420e-01,  1.0366e+00,  ...,  2.6082e-01,\n",
      "           -2.2391e-01, -3.6484e-01]],\n",
      "\n",
      "         [[ 4.4103e-01, -3.1265e-01,  6.3650e-01,  ...,  5.5403e-02,\n",
      "           -8.7760e-01, -1.4740e-01],\n",
      "          [ 4.4166e-01, -3.6037e-01,  6.6028e-01,  ...,  4.7527e-02,\n",
      "           -8.4256e-01, -2.0881e-01],\n",
      "          [ 3.8699e-01, -3.1846e-01,  7.6115e-01,  ...,  1.4373e-02,\n",
      "           -8.1642e-01, -2.3801e-01],\n",
      "          ...,\n",
      "          [ 4.1028e-01, -3.6608e-01,  7.2648e-01,  ..., -5.6450e-02,\n",
      "           -8.1203e-01, -2.2942e-01],\n",
      "          [ 4.2109e-01, -3.4204e-01,  6.8523e-01,  ..., -3.8349e-03,\n",
      "           -8.3907e-01, -2.3749e-01],\n",
      "          [ 4.1323e-01, -3.3202e-01,  6.9825e-01,  ..., -1.8592e-02,\n",
      "           -8.1825e-01, -1.7522e-01]],\n",
      "\n",
      "         [[-9.0357e-02,  3.0973e-01, -6.6289e-02,  ..., -1.8268e-01,\n",
      "            9.8618e-02,  3.2411e-01],\n",
      "          [-6.0065e-02,  3.4551e-01, -4.4769e-02,  ..., -1.8280e-01,\n",
      "            9.1178e-02,  4.1757e-01],\n",
      "          [-9.1314e-02,  2.7673e-01, -5.3904e-02,  ..., -1.8149e-01,\n",
      "            5.6835e-02,  4.2151e-01],\n",
      "          ...,\n",
      "          [-8.1633e-02,  2.6021e-01, -9.0261e-02,  ..., -1.9616e-01,\n",
      "            8.5134e-02,  4.1517e-01],\n",
      "          [-1.2287e-01,  2.8068e-01, -6.8214e-02,  ..., -1.6601e-01,\n",
      "            5.7833e-02,  4.0921e-01],\n",
      "          [-9.8445e-02,  2.7404e-01, -5.6366e-02,  ..., -1.8286e-01,\n",
      "            7.2730e-02,  4.3041e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7482e-02, -3.1465e-01, -4.6566e-01,  ...,  5.4758e-02,\n",
      "            7.1475e-01, -7.3443e-01],\n",
      "          [ 2.2021e-02, -3.0985e-01, -4.2542e-01,  ...,  6.7902e-02,\n",
      "            7.5891e-01, -6.8790e-01],\n",
      "          [ 1.1219e-02, -2.7107e-01, -4.3020e-01,  ...,  7.6267e-02,\n",
      "            7.0870e-01, -7.0745e-01],\n",
      "          ...,\n",
      "          [-6.3673e-02, -2.9515e-01, -3.6359e-01,  ...,  9.8140e-02,\n",
      "            7.1208e-01, -6.4176e-01],\n",
      "          [-3.8451e-02, -2.8472e-01, -3.6958e-01,  ...,  8.9122e-02,\n",
      "            7.1110e-01, -6.4395e-01],\n",
      "          [ 2.1755e-02, -2.9756e-01, -3.6009e-01,  ...,  5.2262e-02,\n",
      "            7.0269e-01, -6.2142e-01]],\n",
      "\n",
      "         [[-1.5185e-01,  3.2731e-02,  3.0902e-01,  ..., -2.0707e-01,\n",
      "            1.2010e-01,  4.2452e-02],\n",
      "          [-1.7912e-01,  3.3426e-02,  3.4538e-01,  ..., -1.7451e-01,\n",
      "            1.6106e-01,  2.1228e-02],\n",
      "          [-1.1555e-01,  5.1503e-02,  3.2970e-01,  ..., -2.3009e-01,\n",
      "            9.0867e-02,  7.0930e-02],\n",
      "          ...,\n",
      "          [-1.2077e-01,  4.6061e-02,  3.8664e-01,  ..., -1.8455e-01,\n",
      "            9.7508e-02,  8.5031e-02],\n",
      "          [-1.4090e-01,  1.9111e-02,  3.5663e-01,  ..., -2.1597e-01,\n",
      "            1.0060e-01,  7.0536e-02],\n",
      "          [-1.3503e-01,  2.3722e-02,  3.7780e-01,  ..., -2.2253e-01,\n",
      "            7.8388e-02,  1.2233e-01]],\n",
      "\n",
      "         [[-2.4405e-01,  8.0898e-01,  5.1592e-01,  ..., -4.6152e-01,\n",
      "           -4.1128e-01, -1.8346e-01],\n",
      "          [-2.1371e-01,  7.1067e-01,  5.2376e-01,  ..., -4.3488e-01,\n",
      "           -3.4273e-01, -1.0672e-01],\n",
      "          [-1.9148e-01,  7.5754e-01,  5.5013e-01,  ..., -4.0088e-01,\n",
      "           -2.9579e-01, -3.1242e-02],\n",
      "          ...,\n",
      "          [-1.4084e-01,  7.4415e-01,  5.4743e-01,  ..., -4.2493e-01,\n",
      "           -3.0614e-01, -9.9925e-03],\n",
      "          [-1.6112e-01,  6.9608e-01,  5.7811e-01,  ..., -4.1064e-01,\n",
      "           -2.9159e-01, -9.2888e-02],\n",
      "          [-1.6573e-01,  7.4022e-01,  5.9271e-01,  ..., -4.6262e-01,\n",
      "           -3.1870e-01, -4.6466e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.4783e-01,  2.2130e-01,  8.7541e-01,  ...,  4.4041e-02,\n",
      "           -6.6280e-02, -3.7516e-01],\n",
      "          [-2.6647e-01,  1.5849e-01,  8.0659e-01,  ...,  9.6528e-02,\n",
      "           -1.5526e-01, -4.9179e-01],\n",
      "          [-3.0330e-01,  1.5264e-01,  8.8541e-01,  ...,  1.8581e-02,\n",
      "           -1.1387e-01, -4.0157e-01],\n",
      "          ...,\n",
      "          [-3.0151e-01,  1.5661e-01,  9.0433e-01,  ...,  6.0227e-02,\n",
      "           -9.6992e-02, -3.3875e-01],\n",
      "          [-2.8923e-01,  1.6234e-01,  8.2743e-01,  ...,  6.4833e-02,\n",
      "           -9.5778e-02, -3.9276e-01],\n",
      "          [-2.9842e-01,  1.9929e-01,  7.7920e-01,  ...,  6.5958e-02,\n",
      "           -9.0857e-02, -3.8313e-01]],\n",
      "\n",
      "         [[ 4.4612e-01, -3.1270e-01,  7.8783e-01,  ..., -4.2755e-02,\n",
      "           -8.7977e-01, -3.5330e-01],\n",
      "          [ 4.8982e-01, -2.6864e-01,  7.3849e-01,  ..., -1.4360e-02,\n",
      "           -9.1930e-01, -3.6172e-01],\n",
      "          [ 4.0324e-01, -2.5278e-01,  8.6665e-01,  ..., -1.1608e-02,\n",
      "           -8.1170e-01, -3.0151e-01],\n",
      "          ...,\n",
      "          [ 4.2068e-01, -3.1670e-01,  8.1172e-01,  ..., -4.7451e-02,\n",
      "           -8.2552e-01, -3.8609e-01],\n",
      "          [ 4.5798e-01, -3.1070e-01,  7.0271e-01,  ..., -3.3653e-02,\n",
      "           -9.2807e-01, -3.9379e-01],\n",
      "          [ 4.7310e-01, -3.8069e-01,  7.4140e-01,  ..., -8.1832e-02,\n",
      "           -9.0935e-01, -3.9416e-01]],\n",
      "\n",
      "         [[-1.0484e-01,  1.2039e-01, -2.2797e-01,  ..., -6.7350e-02,\n",
      "            8.1777e-02,  3.6664e-01],\n",
      "          [-1.3085e-01,  1.1152e-01, -1.8502e-01,  ..., -1.2706e-01,\n",
      "            9.5339e-02,  4.2054e-01],\n",
      "          [-1.0061e-01,  8.9890e-02, -1.3122e-01,  ..., -4.5677e-02,\n",
      "            1.0511e-01,  3.6187e-01],\n",
      "          ...,\n",
      "          [-4.9095e-02,  1.0889e-01, -2.6286e-01,  ..., -1.1059e-01,\n",
      "            3.9749e-02,  3.5202e-01],\n",
      "          [-1.1922e-01,  8.9187e-03, -1.6500e-01,  ..., -7.0208e-02,\n",
      "            6.2529e-02,  3.2524e-01],\n",
      "          [-1.9719e-01,  4.5010e-03, -1.8872e-01,  ..., -7.7475e-02,\n",
      "            1.2967e-01,  3.3088e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5804e-01, -2.2324e-01, -2.3674e-01,  ...,  1.7547e-01,\n",
      "            7.2364e-01, -5.3188e-01],\n",
      "          [ 9.5838e-03, -2.1627e-01, -1.7975e-01,  ...,  1.4567e-01,\n",
      "            8.4261e-01, -5.9241e-01],\n",
      "          [ 6.1427e-02, -2.1735e-01, -2.4032e-01,  ...,  1.0369e-01,\n",
      "            7.2425e-01, -5.3240e-01],\n",
      "          ...,\n",
      "          [ 1.3114e-01, -1.6299e-01, -2.4489e-01,  ...,  1.2466e-01,\n",
      "            6.8245e-01, -4.8748e-01],\n",
      "          [ 1.1800e-01, -2.1688e-01, -2.1940e-01,  ...,  1.1712e-01,\n",
      "            7.8208e-01, -5.3550e-01],\n",
      "          [ 1.2383e-01, -1.8318e-01, -2.3170e-01,  ...,  1.1762e-01,\n",
      "            7.3130e-01, -5.1781e-01]],\n",
      "\n",
      "         [[-2.3081e-01,  2.9740e-01,  2.4207e-01,  ..., -2.2626e-01,\n",
      "            3.3069e-01, -1.8991e-01],\n",
      "          [-2.2386e-01,  2.3119e-01,  2.3066e-01,  ..., -1.5187e-01,\n",
      "            2.5583e-01, -1.0662e-01],\n",
      "          [-2.8152e-01,  2.5130e-01,  2.5532e-01,  ..., -2.0251e-01,\n",
      "            3.0305e-01, -1.6991e-01],\n",
      "          ...,\n",
      "          [-2.1104e-01,  9.2761e-02,  2.4226e-01,  ..., -1.6277e-01,\n",
      "            1.8245e-01, -1.3426e-01],\n",
      "          [-1.8080e-01,  9.5139e-02,  2.6482e-01,  ..., -1.3196e-01,\n",
      "            1.8666e-01, -1.2067e-01],\n",
      "          [-1.8623e-01,  1.6982e-01,  3.2467e-01,  ..., -1.4644e-01,\n",
      "            1.1167e-01, -4.6721e-02]],\n",
      "\n",
      "         [[-2.0198e-01,  5.5055e-01,  4.1276e-01,  ..., -3.2676e-01,\n",
      "           -2.6656e-01, -1.1648e-01],\n",
      "          [-2.6099e-01,  4.8776e-01,  4.3851e-01,  ..., -2.7932e-01,\n",
      "           -3.2868e-01, -8.2238e-02],\n",
      "          [-2.5734e-01,  4.4857e-01,  4.2398e-01,  ..., -2.4061e-01,\n",
      "           -2.9365e-01, -1.3804e-03],\n",
      "          ...,\n",
      "          [-2.6844e-01,  5.8446e-01,  4.3667e-01,  ..., -3.2901e-01,\n",
      "           -2.6607e-01, -8.1299e-02],\n",
      "          [-1.8876e-01,  4.6643e-01,  3.9455e-01,  ..., -2.9895e-01,\n",
      "           -2.9141e-01, -9.3820e-02],\n",
      "          [-1.2867e-01,  5.0511e-01,  3.9104e-01,  ..., -2.6505e-01,\n",
      "           -2.1469e-01, -1.4579e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3995e-01,  1.2811e-01,  9.6553e-01,  ..., -9.3916e-02,\n",
      "           -1.9008e-01, -4.4400e-01],\n",
      "          [-1.3741e-01,  1.7752e-01,  1.0041e+00,  ..., -1.4188e-01,\n",
      "           -1.8137e-01, -4.2375e-01],\n",
      "          [-1.6300e-01,  2.2968e-01,  1.0362e+00,  ..., -8.2986e-02,\n",
      "           -1.9580e-01, -3.7576e-01],\n",
      "          ...,\n",
      "          [-1.9221e-01,  2.0507e-01,  1.0078e+00,  ..., -1.1503e-01,\n",
      "           -2.4319e-01, -4.0246e-01],\n",
      "          [-8.4622e-02,  1.7689e-01,  9.7646e-01,  ..., -8.0186e-02,\n",
      "           -2.5066e-01, -4.0725e-01],\n",
      "          [-1.9203e-01,  2.2294e-01,  1.0245e+00,  ..., -1.3576e-01,\n",
      "           -1.9799e-01, -3.7612e-01]],\n",
      "\n",
      "         [[ 4.6003e-01, -3.9894e-01,  7.4315e-01,  ...,  3.4223e-02,\n",
      "           -8.8711e-01, -2.5284e-01],\n",
      "          [ 4.1840e-01, -3.7989e-01,  7.7282e-01,  ...,  7.4827e-02,\n",
      "           -8.6484e-01, -2.2424e-01],\n",
      "          [ 4.6077e-01, -4.1636e-01,  7.1695e-01,  ...,  4.9675e-03,\n",
      "           -8.8561e-01, -2.4654e-01],\n",
      "          ...,\n",
      "          [ 4.8263e-01, -3.7442e-01,  7.3062e-01,  ...,  2.1968e-02,\n",
      "           -8.4366e-01, -2.4257e-01],\n",
      "          [ 5.0728e-01, -3.7213e-01,  8.0431e-01,  ...,  4.8461e-02,\n",
      "           -9.0051e-01, -2.7458e-01],\n",
      "          [ 4.8720e-01, -3.9595e-01,  7.8604e-01,  ...,  2.5606e-02,\n",
      "           -8.7233e-01, -3.0161e-01]],\n",
      "\n",
      "         [[-2.1114e-01,  1.4965e-01, -1.3444e-02,  ..., -1.9564e-01,\n",
      "            3.0743e-02,  3.7213e-01],\n",
      "          [-2.3090e-01,  1.2825e-01, -1.4543e-02,  ..., -1.9815e-01,\n",
      "            1.3240e-02,  3.3348e-01],\n",
      "          [-1.2647e-01,  9.4607e-02,  6.0326e-02,  ..., -2.0107e-01,\n",
      "            5.6115e-02,  3.2556e-01],\n",
      "          ...,\n",
      "          [-9.2420e-02,  9.9624e-02,  3.5181e-02,  ..., -1.9748e-01,\n",
      "            7.5506e-02,  2.8734e-01],\n",
      "          [-2.9447e-01,  5.9259e-02,  2.5187e-02,  ..., -9.4554e-02,\n",
      "           -3.5350e-02,  2.7859e-01],\n",
      "          [-2.1321e-01,  7.0383e-02,  3.0346e-02,  ..., -1.4795e-01,\n",
      "            2.6476e-02,  3.4334e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6312e-01, -2.4937e-01, -3.6826e-01,  ...,  6.1705e-02,\n",
      "            7.8776e-01, -5.6044e-01],\n",
      "          [ 8.6574e-02, -3.2196e-01, -3.6298e-01,  ...,  7.8686e-02,\n",
      "            7.3961e-01, -6.0091e-01],\n",
      "          [ 5.4508e-02, -3.1974e-01, -3.2399e-01,  ...,  7.6480e-02,\n",
      "            7.4069e-01, -5.6144e-01],\n",
      "          ...,\n",
      "          [ 6.6941e-02, -3.3891e-01, -2.7884e-01,  ...,  9.9536e-02,\n",
      "            7.7621e-01, -6.1312e-01],\n",
      "          [ 1.3979e-01, -3.2599e-01, -3.2652e-01,  ...,  8.2160e-02,\n",
      "            7.5530e-01, -5.5216e-01],\n",
      "          [ 1.6856e-01, -3.4396e-01, -2.7308e-01,  ...,  1.1569e-01,\n",
      "            7.2181e-01, -5.6035e-01]],\n",
      "\n",
      "         [[-1.8459e-01,  3.1986e-01,  3.6795e-01,  ..., -1.4595e-01,\n",
      "            2.0731e-01,  3.0574e-02],\n",
      "          [-1.5798e-01,  2.9209e-01,  3.2392e-01,  ..., -1.3779e-01,\n",
      "            2.0939e-01,  5.1260e-02],\n",
      "          [-1.7983e-01,  3.3738e-01,  3.3068e-01,  ..., -1.5311e-01,\n",
      "            2.2144e-01,  4.1133e-02],\n",
      "          ...,\n",
      "          [-1.3377e-01,  3.2995e-01,  3.7384e-01,  ..., -9.9732e-02,\n",
      "            2.3051e-01,  2.4321e-02],\n",
      "          [-2.3419e-01,  2.7679e-01,  3.4613e-01,  ..., -6.2235e-02,\n",
      "            2.1407e-01,  3.0827e-02],\n",
      "          [-2.3120e-01,  2.1397e-01,  3.1228e-01,  ..., -1.0048e-01,\n",
      "            1.4865e-01,  4.4151e-02]],\n",
      "\n",
      "         [[-1.6172e-01,  6.4654e-01,  6.3144e-01,  ..., -3.8963e-01,\n",
      "           -4.0522e-01, -1.1118e-01],\n",
      "          [-1.6234e-01,  6.2082e-01,  6.5971e-01,  ..., -3.3709e-01,\n",
      "           -4.5437e-01, -1.0079e-01],\n",
      "          [-1.9264e-01,  6.9616e-01,  6.2586e-01,  ..., -3.0312e-01,\n",
      "           -4.4898e-01, -9.2496e-02],\n",
      "          ...,\n",
      "          [-9.0497e-02,  6.7889e-01,  5.8288e-01,  ..., -3.2947e-01,\n",
      "           -5.0290e-01, -1.7064e-01],\n",
      "          [-1.5863e-01,  6.6012e-01,  5.7042e-01,  ..., -3.5823e-01,\n",
      "           -4.8018e-01, -9.1851e-02],\n",
      "          [-1.3449e-01,  6.9575e-01,  5.3680e-01,  ..., -2.8461e-01,\n",
      "           -4.9254e-01, -1.0818e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2300e-01,  9.0715e-02,  9.7519e-01,  ...,  6.9565e-02,\n",
      "           -3.1552e-01, -3.9906e-01],\n",
      "          [-1.8976e-01, -1.7226e-02,  9.4027e-01,  ...,  1.0016e-01,\n",
      "           -3.0126e-01, -4.5305e-01],\n",
      "          [-2.3626e-01,  4.1882e-02,  9.6637e-01,  ...,  6.6836e-02,\n",
      "           -2.7630e-01, -4.4719e-01],\n",
      "          ...,\n",
      "          [-2.0609e-01,  1.0078e-02,  9.5070e-01,  ...,  5.2304e-02,\n",
      "           -2.0221e-01, -3.3942e-01],\n",
      "          [-2.5270e-01,  6.4902e-02,  9.8572e-01,  ...,  9.6146e-02,\n",
      "           -2.3242e-01, -3.9549e-01],\n",
      "          [-1.2013e-01,  8.2494e-02,  9.2677e-01,  ...,  6.6785e-02,\n",
      "           -2.3743e-01, -3.4690e-01]],\n",
      "\n",
      "         [[ 4.5087e-01, -3.0596e-01,  6.3330e-01,  ...,  1.3529e-02,\n",
      "           -9.1401e-01, -2.1114e-01],\n",
      "          [ 4.4589e-01, -3.5227e-01,  6.0317e-01,  ...,  5.7300e-02,\n",
      "           -9.3721e-01, -3.4348e-01],\n",
      "          [ 4.2348e-01, -3.1248e-01,  6.0882e-01,  ...,  2.7520e-02,\n",
      "           -9.1143e-01, -3.2526e-01],\n",
      "          ...,\n",
      "          [ 4.5369e-01, -3.9737e-01,  6.2716e-01,  ...,  1.1240e-01,\n",
      "           -9.9740e-01, -3.3685e-01],\n",
      "          [ 5.0700e-01, -3.3654e-01,  5.6406e-01,  ...,  8.0173e-03,\n",
      "           -9.5163e-01, -2.6764e-01],\n",
      "          [ 4.5203e-01, -3.2457e-01,  5.8379e-01,  ..., -2.1327e-02,\n",
      "           -9.1571e-01, -3.2562e-01]],\n",
      "\n",
      "         [[-4.9511e-02,  2.5359e-01, -2.3241e-01,  ..., -3.0347e-01,\n",
      "            8.5884e-02,  2.4258e-01],\n",
      "          [-8.0923e-02,  2.3184e-01, -2.1775e-01,  ..., -2.4105e-01,\n",
      "            1.1644e-01,  2.6999e-01],\n",
      "          [-7.2074e-02,  2.4718e-01, -1.4061e-01,  ..., -2.8259e-01,\n",
      "            1.2942e-01,  2.9242e-01],\n",
      "          ...,\n",
      "          [-1.1579e-01,  1.9054e-01, -2.7609e-01,  ..., -2.8504e-01,\n",
      "            6.2496e-02,  2.2618e-01],\n",
      "          [-8.6253e-02,  2.1572e-01, -2.0532e-01,  ..., -2.7222e-01,\n",
      "            3.9658e-02,  2.8306e-01],\n",
      "          [-1.0291e-01,  1.9679e-01, -2.1070e-01,  ..., -2.6861e-01,\n",
      "            3.0442e-02,  2.3633e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6853e-02, -1.0686e-01, -2.0093e-01,  ...,  4.7329e-03,\n",
      "            5.7791e-01, -6.6707e-01],\n",
      "          [-4.6748e-03, -1.2624e-01, -2.0898e-01,  ..., -1.6834e-02,\n",
      "            5.4014e-01, -6.5119e-01],\n",
      "          [-1.4966e-02, -8.6583e-02, -1.3729e-01,  ..., -2.0645e-02,\n",
      "            6.1714e-01, -6.5493e-01],\n",
      "          ...,\n",
      "          [ 3.7105e-02, -8.4055e-02, -1.7475e-01,  ...,  2.3482e-02,\n",
      "            5.5717e-01, -6.7472e-01],\n",
      "          [ 1.3245e-02, -1.5471e-01, -2.6010e-01,  ..., -1.3391e-02,\n",
      "            6.3356e-01, -6.4015e-01],\n",
      "          [-4.8377e-02, -1.1252e-01, -1.2134e-01,  ..., -9.7105e-04,\n",
      "            6.2465e-01, -7.0339e-01]],\n",
      "\n",
      "         [[-1.3265e-01,  3.4864e-01,  4.5029e-01,  ..., -1.7350e-01,\n",
      "            1.8600e-01, -1.0498e-01],\n",
      "          [-1.4478e-01,  2.7475e-01,  4.9977e-01,  ..., -4.9085e-02,\n",
      "            1.4560e-01, -1.4060e-01],\n",
      "          [-1.7070e-01,  3.0291e-01,  4.9248e-01,  ..., -1.0824e-01,\n",
      "            9.4012e-02, -6.6261e-02],\n",
      "          ...,\n",
      "          [-1.1191e-01,  2.9840e-01,  4.9705e-01,  ..., -3.2487e-02,\n",
      "            1.0404e-01, -3.6698e-02],\n",
      "          [-1.1640e-01,  3.4803e-01,  5.0017e-01,  ..., -4.2659e-02,\n",
      "            1.0294e-01, -8.1928e-02],\n",
      "          [-7.9029e-02,  3.1599e-01,  5.2573e-01,  ..., -4.1890e-02,\n",
      "            1.5436e-01, -8.4232e-02]],\n",
      "\n",
      "         [[-1.4926e-01,  6.9440e-01,  6.2663e-01,  ..., -4.0245e-01,\n",
      "           -1.7974e-01, -9.6852e-02],\n",
      "          [-6.1079e-02,  6.9405e-01,  6.7810e-01,  ..., -4.1723e-01,\n",
      "           -1.0521e-01, -1.2514e-01],\n",
      "          [-1.5549e-01,  7.2389e-01,  6.4629e-01,  ..., -4.2208e-01,\n",
      "           -1.9352e-01, -2.0250e-01],\n",
      "          ...,\n",
      "          [-1.0546e-01,  7.3994e-01,  6.6910e-01,  ..., -4.5348e-01,\n",
      "           -9.5893e-02, -5.1776e-02],\n",
      "          [-1.8423e-01,  6.5658e-01,  6.7638e-01,  ..., -3.9958e-01,\n",
      "           -1.3714e-01, -5.8987e-02],\n",
      "          [-9.6958e-02,  6.7424e-01,  6.2961e-01,  ..., -4.8276e-01,\n",
      "           -1.4668e-01, -1.0310e-01]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "print(\"加权求和后的输出 output 的形状：\", output.shape)\n",
    "print(\"加权求和后的输出 output 的值：\\n\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.5 拼接多头 🐱 将多个注意力头的输出拼接回原始维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. 拼接多头的作用**\n",
    "- **恢复原始维度**：\n",
    "  - 在分割多头时，我们将`d_model`拆分为`num_heads * head_dim`。\n",
    "  - 拼接多头的作用是将多个注意力头的输出拼接回`d_model`维度。\n",
    "- **生成最终输出**：\n",
    "  - 拼接后的输出形状为`(batch_size, seq_len, d_model)`，可以直接用于后续的计算。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状： torch.Size([32, 16, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "# output 是加权求和的结果，形状为 (batch_size, num_heads, seq_len, head_dim)\n",
    "batch_size, num_heads, seq_len, head_dim = output.shape\n",
    "print(\"output 的形状：\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# 1. 转置：将 num_heads 维度移到后面\n",
    "output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# 2. 拼接：将 num_heads 和 head_dim 合并为 d_model\n",
    "output = output.reshape(batch_size, seq_len, -1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"拼接后的 output 的形状：\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.1.6 线性变换 🐱 将拼接后的输出映射回原始维度\n",
    "\n",
    "这部分用于将之前获得的拼接结果用线性变换层映射到另外一个特征空间。这也可以用于适应下一部分``Feed-Forward Network``的输入维度。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性变换后的 output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 定义线性变换层\n",
    "output_projection = nn.Linear(d_model, d_model)\n",
    "\n",
    "# 线性变换\n",
    "projected_output = output_projection(output)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"线性变换后的 output 的形状：\", projected_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Feed-Forward Network 🐱 前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **特征转换**：\n",
    "  - 将多头注意力机制的输出进一步映射到更高维的特征空间。\n",
    "  - 通过非线性激活函数（如ReLU）引入非线性变换。\n",
    "- **独立处理**：\n",
    "  - 对序列中的每个位置独立处理，不依赖其他位置的信息。\n",
    "- **增强表达能力**：\n",
    "  - 通过多层全连接网络增强模型的表达能力。\n",
    "\n",
    "前馈神经网络通常由两层全连接层组成：\n",
    "1. **第一层**：\n",
    "   - 输入维度：`d_model`\n",
    "   - 输出维度：`d_ff`（通常为`4 * d_model`）\n",
    "   - 激活函数：ReLU\n",
    "2. **第二层**：\n",
    "   - 输入维度：`d_ff`\n",
    "   - 输出维度：`d_model`\n",
    "   - 无激活函数\n",
    "\n",
    "很像autoencoder的结构不是吗🐱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  # 第一层全连接\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)  # 第二层全连接\n",
    "        self.activation = nn.ReLU()  # 激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = self.linear1(x)  # (batch_size, seq_len, d_ff)\n",
    "        x = self.activation(x)  # 非线性变换\n",
    "        x = self.linear2(x)  # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projected_output 的形状： torch.Size([32, 50, 512])\n",
      "ffn_output 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_ff = 2048  # 通常为 4 * d_model\n",
    "\n",
    "# 我们已经获得了projected_output，形状为 (batch_size, seq_len, d_model)\n",
    "print(\"projected_output 的形状：\", projected_output.shape)\n",
    "# 前馈神经网络\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "ffn_output = ffn(projected_output)\n",
    "\n",
    "print(\"ffn_output 的形状：\", ffn_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 Residual Connection & Layer Normalization 🐱 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 为什么要进行残差连接？\n",
    "也许你已经注意到，前馈神经网络的输出`ffn_output 的形状： torch.Size([32, 50, 512])`与预处理之后的数据`x = preprocessor(input_ids)`、多头注意力的输出`拼接后的 output 的形状： torch.Size([32, 50, 512])`的形状一致。这让我们想到也许能够将其进行相加之类的操作。\n",
    "\n",
    "**（1）保留原始信息**\n",
    "- 多头注意力机制已经捕捉了序列中元素之间的关系。\n",
    "- 残差连接确保这些信息不会被前馈神经网络完全覆盖，保留原始特征。\n",
    "\n",
    "**（2）缓解梯度消失**\n",
    "- 深层网络中，梯度在反向传播时容易消失。\n",
    "- 残差连接提供了一条“捷径”，使梯度可以直接传播到浅层，缓解梯度消失问题。\n",
    "\n",
    "**（3）增强模型表达能力**\n",
    "- 前馈神经网络引入了非线性变换，增强了模型的表达能力。\n",
    "- 残差连接将这种非线性变换与原始特征结合，进一步提升模型性能。\n",
    "\n",
    "**（4）加速训练**\n",
    "- 残差连接使模型更容易优化，加速训练过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "norm1 = nn.LayerNorm(d_model)\n",
    "norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "x = norm1(x + projected_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在此之后，这个`x`再经过我们之前提到过的`Feed-Forward`得到的`ffn_output = ffn(projected_output)`进行残差链接，最后将`x`进行层归一化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "残差连接和层归一化后的 x 的形状： torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "x = norm2(x + ffn_output)\n",
    "\n",
    "print(\"残差连接和层归一化后的 x 的形状：\", x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 为什么要层归一化？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（Layer Normalization）是一种用于神经网络中的归一化技术，主要用于加速训练过程并提高模型的稳定性。以下是详细解释：\n",
    "\n",
    "**. 层归一化的作用**\n",
    "- **归一化特征**：\n",
    "  - 对每个样本的特征进行归一化，使其均值为0，方差为1。\n",
    "  - 减少内部协变量偏移（Internal Covariate Shift），使训练更稳定。\n",
    "- **加速收敛**：\n",
    "  - 归一化后的特征分布更稳定，有助于加速模型收敛。\n",
    "- **适用于不同任务**：\n",
    "  - 特别适合处理变长序列（如NLP任务）和小批量数据。\n",
    "\n",
    "**. 层归一化的公式**\n",
    "层归一化的计算公式如下：\n",
    "```python\n",
    "y = (x - mean) / sqrt(var + eps) * gamma + beta\n",
    "```\n",
    "- **`x`**：输入特征。\n",
    "- **`mean`**：输入特征的均值。\n",
    "- **`var`**：输入特征的方差。\n",
    "- **`eps`**：一个小常数，用于数值稳定性（默认`1e-5`）。\n",
    "- **`gamma`**：可学习的缩放参数（权重）。\n",
    "- **`beta`**：可学习的偏移参数（偏置）。\n",
    "\n",
    "---\n",
    "\n",
    "**. 层归一化的特点**\n",
    "- **独立于批量大小**：\n",
    "  - 与批量归一化（Batch Normalization）不同，层归一化不依赖于批量大小，适合处理小批量或变长序列。\n",
    "- **逐样本归一化**：\n",
    "  - 对每个样本的特征进行归一化，而不是跨样本归一化。\n",
    "- **可学习的参数**：\n",
    "  - `gamma` 和 `beta` 是可学习的参数，允许模型调整归一化后的特征分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是`Encoder`中一个`EncoderLayer`的全部内容，为了获取`Encoder`，我们需要将`EncoderLayer`堆叠起来。你可以在`transformer_encoder.py`中找到完整的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Decoder 🐱 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. **掩码多头自注意力**：\n",
    "   - 捕捉已生成序列的内部关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 为什么就需要掩码了？之前的Encoder中的注意力不需要掩码呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **任务性质不同**\n",
    "- **Encoder**：\n",
    "  - 处理的是完整的输入序列（如源语言句子）\n",
    "  - 需要同时看到整个序列的所有信息\n",
    "  - 目标是捕捉序列中所有元素之间的关系\n",
    "\n",
    "- **Decoder**：\n",
    "  - 处理的是目标序列（如目标语言句子）\n",
    "  - 需要逐步生成序列，不能\"偷看\"未来信息\n",
    "  - 目标是基于已生成的部分序列和Encoder的输出来预测下一个元素\n",
    "\n",
    "##### 2. **掩码的作用**\n",
    "- **防止信息泄露**：\n",
    "  - 在训练时，Decoder会接收完整的目标序列\n",
    "  - 如果没有掩码，模型可能会直接\"看到\"未来的信息，导致训练作弊\n",
    "  - 掩码确保模型只能使用当前位置及之前的信息\n",
    "\n",
    "- **保持自回归性质**：\n",
    "  - 在推理时，Decoder需要逐个生成序列元素\n",
    "  - 掩码确保模型只能基于已生成的部分序列进行预测\n",
    "  - 这是序列生成任务（如机器翻译、文本生成）的基本要求\n",
    "\n",
    "##### 3. **具体实现**\n",
    "- **Encoder中的注意力**：\n",
    "  - 计算注意力分数时，所有位置之间都可以相互关注\n",
    "  - 不需要任何限制，因为整个输入序列是已知的\n",
    "\n",
    "- **Decoder中的掩码注意力**：\n",
    "  - 使用下三角掩码（`torch.tril`）\n",
    "  - 确保每个位置只能关注到它自身及之前的位置\n",
    "  - 未来位置的注意力分数被设置为`-inf`，在softmax后权重为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状： torch.Size([32, 16, 50, 32])\n",
      "K 的形状： torch.Size([32, 16, 50, 32])\n",
      "Encoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "Decoder注意力分数 scores 的形状： torch.Size([32, 16, 50, 50])\n",
      "mask 的形状： torch.Size([50, 50])\n",
      "mask 的值： tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "print(\"Q 的形状：\", Q.shape)\n",
    "print(\"K 的形状：\", K.shape)\n",
    "# Encoder注意力分数（无掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "# print(\"Encoder注意力分数 scores：\", scores)\n",
    "print(\"Encoder注意力分数 scores 的形状：\", scores.shape)\n",
    "# Decoder注意力分数（带掩码）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / sqrt(head_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))  # 下三角掩码\n",
    "scores = scores.masked_fill(mask == 0, float('-inf'))  # 应用掩码\n",
    "# print(\"Decoder注意力分数 scores：\", scores)\n",
    "print(\"Decoder注意力分数 scores 的形状：\", scores.shape)\n",
    "print(\"mask 的形状：\", mask.shape)\n",
    "print(\"mask 的值：\", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 为什么需要下三角掩码？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. **自回归任务的性质**\n",
    "在自回归任务中，模型需要**逐步生成序列**，即每次只能基于已经生成的部分序列来预测下一个元素。例如：\n",
    "- 在文本生成中，模型只能基于已经生成的单词来预测下一个单词。\n",
    "- 在机器翻译中，模型只能基于已经生成的目标语言单词来预测下一个单词。\n",
    "\n",
    "如果模型能够“看到”未来的信息，它就会作弊，直接使用未来的信息来预测当前的位置，这会导致训练和推理不一致。\n",
    "\n",
    "\n",
    "##### 2. **下三角掩码的作用**\n",
    "下三角掩码的作用是**限制模型只能访问当前位置及之前的信息**，而不能访问未来的信息。具体来说：\n",
    "- **下三角部分**：值为1，表示允许模型访问这些位置的信息。\n",
    "- **上三角部分**：值为0，表示禁止模型访问这些位置的信息。\n",
    "\n",
    "通过将上三角部分的注意力分数设置为`-inf`，在softmax操作后，这些位置的权重会变为0，从而确保模型无法利用未来的信息。\n",
    "\n",
    "##### 3. **掩码的实现**\n",
    "在代码中，下三角掩码通常通过`torch.tril`函数生成：\n",
    "````python\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "````\n",
    "例如，对于一个长度为5的序列，掩码矩阵如下：\n",
    "````\n",
    "1 0 0 0 0\n",
    "1 1 0 0 0\n",
    "1 1 1 0 0\n",
    "1 1 1 1 0\n",
    "1 1 1 1 1\n",
    "````\n",
    "- 第1行：只能看到第1个位置。\n",
    "- 第2行：可以看到第1和第2个位置。\n",
    "- 第3行：可以看到第1、第2和第3个位置。\n",
    "- 以此类推。\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **为什么需要下三角掩码？**\n",
    "###### （1）**训练时防止信息泄露**\n",
    "- 在训练时，Decoder会接收完整的目标序列（如目标语言句子）。\n",
    "- 如果没有掩码，模型可能会直接“看到”未来的信息，导致训练作弊。\n",
    "- 掩码确保模型只能使用当前位置及之前的信息。\n",
    "\n",
    "###### （2）**推理时保持自回归性质**\n",
    "- 在推理时，Decoder需要逐个生成序列元素。\n",
    "- 掩码确保模型只能基于已生成的部分序列进行预测。\n",
    "- 这是序列生成任务（如机器翻译、文本生成）的基本要求。\n",
    "\n",
    "###### （3）**确保训练和推理一致性**\n",
    "- 训练时使用掩码，推理时也使用掩码，确保模型的行为一致。\n",
    "- 如果训练时不使用掩码，模型可能会学习到依赖未来信息的错误模式，导致推理时性能下降。\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **示例**\n",
    "假设我们有一个长度为3的序列，计算注意力分数时：\n",
    "- **无掩码**：模型可以看到所有位置的信息。\n",
    "  ````\n",
    "  scores = [[s11, s12, s13],\n",
    "            [s21, s22, s23],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "- **有掩码**：模型只能看到当前位置及之前的信息。\n",
    "  ````\n",
    "  scores = [[s11, -inf, -inf],\n",
    "            [s21, s22, -inf],\n",
    "            [s31, s32, s33]]\n",
    "  ````\n",
    "在softmax后，`-inf`的位置权重为0，模型无法利用这些信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 6. **总结**\n",
    "| 特性                | 无掩码                  | 下三角掩码              |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| 信息访问            | 可以访问整个序列        | 只能访问当前位置及之前  |\n",
    "| 训练时              | 可能作弊，利用未来信息  | 防止信息泄露            |\n",
    "| 推理时              | 无法逐步生成序列        | 保持自回归性质          |\n",
    "| 适用场景            | Encoder                | Decoder                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 输入是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tgt: 目标序列的嵌入表示。也就是我们之前通过TransformerPreprocessor得到的被预处理的输入。\n",
    "2. tgt_mask: 下三角掩码，用于限制模型只能访问当前位置及之前的信息。\n",
    "3. memory: Encoder的输出，用于计算Encoder-Decoder Attention。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
