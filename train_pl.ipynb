{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"datasets/\"):\n",
    "    with zipfile.ZipFile(\"Multi30K.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# 1. 初始化GPT2 Tokenizer\n",
    "en_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "de_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 添加特殊标记\n",
    "special_tokens = {\"bos_token\": \"<sos>\", \"eos_token\": \"<eos>\", \"pad_token\": \"<pad>\"}\n",
    "en_tokenizer.add_special_tokens(special_tokens)\n",
    "de_tokenizer.add_special_tokens(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English batch shape: torch.Size([32, 33])\n",
      "German batch shape: torch.Size([32, 55])\n",
      "English batch example (tokens): tensor([   32,  2415,  2832,  1223,   284,   257,  4675, 26960, 12049,   287,\n",
      "          257,  2330, 33323,    13, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259])\n",
      "German batch example (tokens): tensor([   36,   500, 39313,    84,   302, 30830,   304,  7749, 10255,   304,\n",
      "         7749,   356,    72, 39683,   368, 21039, 33255,   307,    74,   293,\n",
      "          312,   316,   268, 15195, 39683,   268,    74,  9116,    77,   301,\n",
      "         1754,  2123,  9776,    13, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259])\n",
      "Decoded English: A woman hands something to a street performer dressed in a white gown.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Decoded German: Eine Frau reicht einem mit einem weißem Umhang bekleideten Straßenkünstler etwas.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 1. 自定义数据集\n",
    "class Multi30KDataset(Dataset):\n",
    "    def __init__(self, en_path, de_path, en_tokenizer, de_tokenizer):\n",
    "        self.en_sentences = self._read_file(en_path)\n",
    "        self.de_sentences = self._read_file(de_path)\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        assert len(self.en_sentences) == len(self.de_sentences), \"数据不匹配！\"\n",
    "\n",
    "    def _read_file(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_encoded = self.en_tokenizer(\n",
    "            self.en_sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        de_encoded = self.de_tokenizer(\n",
    "            self.de_sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return en_encoded, de_encoded\n",
    "\n",
    "# 2. 定义collate_fn\n",
    "def collate_fn(batch):\n",
    "    en_batch, de_batch = zip(*batch)\n",
    "    en_batch = pad_sequence(en_batch, batch_first=True, padding_value=en_tokenizer.pad_token_id)\n",
    "    de_batch = pad_sequence(de_batch, batch_first=True, padding_value=de_tokenizer.pad_token_id)\n",
    "    return en_batch, de_batch\n",
    "\n",
    "# 3. 定义LightningDataModule\n",
    "class Multi30KDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, en_file_path, de_file_path, en_tokenizer, de_tokenizer, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.en_file_path = en_file_path\n",
    "        self.de_file_path = de_file_path\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = Multi30KDataset(self.en_file_path, self.de_file_path, self.en_tokenizer, self.de_tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 4. 初始化数据模块\n",
    "en_file_path = 'datasets/train/train.en'\n",
    "de_file_path = 'datasets/train/train.de'\n",
    "batch_size = 32\n",
    "\n",
    "data_module = Multi30KDataModule(en_file_path, de_file_path, en_tokenizer, de_tokenizer, batch_size)\n",
    "\n",
    "# 5. 测试数据加载器\n",
    "data_module.setup()\n",
    "for en_batch, de_batch in data_module.train_dataloader():\n",
    "    print(\"English batch shape:\", en_batch.shape)\n",
    "    print(\"German batch shape:\", de_batch.shape)\n",
    "    print(\"English batch example (tokens):\", en_batch[0])\n",
    "    print(\"German batch example (tokens):\", de_batch[0])\n",
    "    print(\"Decoded English:\", en_tokenizer.decode(en_batch[0]))\n",
    "    print(\"Decoded German:\", de_tokenizer.decode(de_batch[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pl_transformer(pl.LightningDataModule)\n",
    "    def __init__(self, en_tokenizer, de_tokenizer, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        self.batch_size = batch_size"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
