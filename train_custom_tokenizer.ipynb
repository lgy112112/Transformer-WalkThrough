{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"datasets/\"):\n",
    "    with zipfile.ZipFile(\"Multi30K.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_4975/31629097.py\", line 67, in collate_fn\n    en_batch = pad_sequence(en_batch, batch_first=True, padding_value=en_tokenizer.pad_token_id)\nAttributeError: 'SimpleTokenizer' object has no attribute 'pad_token_id'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 5. æµ‹è¯•æ•°æ®åŠ è½½å™¨\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m en_batch, de_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, en_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGerman batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, de_batch\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_4975/31629097.py\", line 67, in collate_fn\n    en_batch = pad_sequence(en_batch, batch_first=True, padding_value=en_tokenizer.pad_token_id)\nAttributeError: 'SimpleTokenizer' object has no attribute 'pad_token_id'\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformer_preprocess import TransformerPreprocessor\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # ç®€å•çš„åˆ†è¯é€»è¾‘ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸º token IDs\n",
    "        tokens = text.split()\n",
    "        return [hash(token) % self.vocab_size for token in tokens]\n",
    "\n",
    "    def __call__(self, text, return_tensors=\"pt\", padding=False, truncation=True, add_special_tokens=True):\n",
    "        token_ids = self.tokenize(text)\n",
    "        if return_tensors == \"pt\":\n",
    "            return {\"input_ids\": torch.tensor([token_ids])}\n",
    "        return {\"input_ids\": token_ids}\n",
    "\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨å’Œ TransformerPreprocessor\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "max_seq_len = 100\n",
    "\n",
    "en_tokenizer = SimpleTokenizer(vocab_size)\n",
    "de_tokenizer = SimpleTokenizer(vocab_size)\n",
    "preprocessor = TransformerPreprocessor(vocab_size, d_model, max_seq_len)\n",
    "\n",
    "class Multi30KDataset(Dataset):\n",
    "    def __init__(self, en_path, de_path, en_tokenizer, de_tokenizer):\n",
    "        self.en_sentences = self._read_file(en_path)\n",
    "        self.de_sentences = self._read_file(de_path)\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        assert len(self.en_sentences) == len(self.de_sentences), \"æ•°æ®ä¸åŒ¹é…ï¼\"\n",
    "\n",
    "    def _read_file(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en_encoded = self.en_tokenizer(\n",
    "            self.en_sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        de_encoded = self.de_tokenizer(\n",
    "            self.de_sentences[idx],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "        return en_encoded, de_encoded\n",
    "\n",
    "\n",
    "# 3. å®šä¹‰collate_fn\n",
    "def collate_fn(batch):\n",
    "    en_batch, de_batch = zip(*batch)\n",
    "    en_batch = pad_sequence(en_batch, batch_first=True, padding_value=en_tokenizer.pad_token_id)\n",
    "    de_batch = pad_sequence(de_batch, batch_first=True, padding_value=de_tokenizer.pad_token_id)\n",
    "    return en_batch, de_batch\n",
    "\n",
    "# 4. åˆå§‹åŒ–æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "en_file_path = 'datasets/train/train.en'\n",
    "de_file_path = 'datasets/train/train.de'\n",
    "\n",
    "dataset = Multi30KDataset(en_file_path, de_file_path, en_tokenizer, de_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, pin_memory=True, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 5. æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
    "for en_batch, de_batch in dataloader:\n",
    "    print(\"English batch shape:\", en_batch.shape)\n",
    "    print(\"German batch shape:\", de_batch.shape)\n",
    "    print(\"English batch example (tokens):\", en_batch[0])\n",
    "    print(\"German batch example (tokens):\", de_batch[0])\n",
    "    print(\"Decoded English:\", en_tokenizer.decode(en_batch[0], skip_special_tokens=False))\n",
    "    print(\"Decoded German:\", de_tokenizer.decode(de_batch[0], skip_special_tokens=False))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer initialized.\n",
      "Source padding index: 0, Target padding index: 0\n",
      "Vocabulary size: 119547\n"
     ]
    }
   ],
   "source": [
    "from transformer import Transformer\n",
    "\n",
    "# 1. Transformer æ¨¡å‹å‚æ•°\n",
    "vocab_size = len(en_tokenizer)\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "d_ff = 2048\n",
    "max_seq_len = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# 2. å¡«å……å€¼ç´¢å¼•\n",
    "src_pad_idx = en_tokenizer.pad_token_id\n",
    "tgt_pad_idx = de_tokenizer.pad_token_id\n",
    "\n",
    "# 3. åˆå§‹åŒ– Transformer\n",
    "transformer = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "\n",
    "# æ‰“å°éªŒè¯\n",
    "print(f\"Transformer initialized.\")\n",
    "print(f\"Source padding index: {src_pad_idx}, Target padding index: {tgt_pad_idx}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)  # å¿½ç•¥å¡«å……æ ‡è®°çš„æŸå¤±\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=0.0001)\n",
    "num_epochs = 5\n",
    "\n",
    "# 2. å®šä¹‰è®­ç»ƒå‡½æ•°\n",
    "def train_epoch(transformer, dataloader, criterion, optimizer, device):\n",
    "    transformer.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # ä¿®æ­£åçš„ç”Ÿæˆæ©ç \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_target = tgt[:, 1:]\n",
    "\n",
    "        # æ„é€ æ©ç \n",
    "        src_mask = transformer.make_src_mask(src, src_pad_idx)\n",
    "        tgt_mask = transformer.make_trg_mask(tgt_input, tgt_pad_idx)  # ä¿®æ­£ä¸º tgt_input\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # è°ƒæ•´è¾“å‡ºå½¢çŠ¶ä»¥è®¡ç®—æŸå¤±\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        tgt_target = tgt_target.reshape(-1)\n",
    "\n",
    "        # è®¡ç®—æŸå¤±\n",
    "        loss = criterion(output, tgt_target)\n",
    "\n",
    "        # åå‘ä¼ æ’­ä¸ä¼˜åŒ–\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # åœ¨tqdmè¿›åº¦æ¡ä¸­æ˜¾ç¤ºå½“å‰batchçš„loss\n",
    "        progress_bar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 3. å®šä¹‰è®­ç»ƒä¸»å¾ªç¯\n",
    "def train_model(transformer, dataloader, num_epochs, device, pretrain=None):\n",
    "    if pretrain:\n",
    "        transformer.load_state_dict(torch.load(pretrain))\n",
    "        print(f\"Loaded pre-trained model from {pretrain}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        epoch_loss = train_epoch(transformer, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 4. å¼€å§‹è®­ç»ƒ\n",
    "transformer = transformer.to(device)\n",
    "pretrain_path = 'transformer.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/454 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:29<00:00,  5.05it/s, batch_loss=3.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 5.1123\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:31<00:00,  4.97it/s, batch_loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 3.1154\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:31<00:00,  4.94it/s, batch_loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 2.4791\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:31<00:00,  4.94it/s, batch_loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 2.0926\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454/454 [01:31<00:00,  4.96it/s, batch_loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 1.8131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(transformer, dataloader, num_epochs, device, pretrain=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: A group of children outside a building includes a boy jumping in the grass.\n",
      "Target Sentence: [CLS] Eine Gruppe Kinder vor einem GebÃ¤ude, darunter ein Junge, der im Gras springt. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted Sentence: Eine Gruppe Kinder vor einem GebÃ¤ude, der ein Junge spring der im Gras springt. [SEP].eee [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ä¿®æ­£æ‹¼å†™é”™è¯¯\n",
    "transformer.load_state_dict(torch.load(\"transformer.pth\", map_location=torch.device('cpu')))\n",
    "transformer.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # ä¿®æ­£åçš„ç”Ÿæˆæ©ç \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_target = tgt[:, 1:]\n",
    "\n",
    "        # æ„é€ æ©ç \n",
    "        src_mask = transformer.make_src_mask(src, src_pad_idx)\n",
    "        tgt_mask = transformer.make_trg_mask(tgt_input, tgt_pad_idx)  # ä¿®æ­£ä¸º tgt_input\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # è·å–é¢„æµ‹ç»“æœ\n",
    "        output = output.argmax(dim=-1)\n",
    "\n",
    "        # æ˜¾ç¤ºåŸå§‹å¥å­å’Œé¢„æµ‹å¥å­\n",
    "        print(\"Source Sentence:\", en_tokenizer.decode(src[0], skip_special_tokens=True, clear_special_tokens=True))\n",
    "        print(\"Target Sentence:\", de_tokenizer.decode(tgt[0], skip_special_tokens=False, clear_special_tokens=True))\n",
    "        print(\"Predicted Sentence:\", de_tokenizer.decode(output[0], skip_special_tokens=False, clear_special_tokens=True))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m src \u001b[38;5;241m=\u001b[39m en_tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# æ„é€ ç›®æ ‡è¾“å…¥å’Œç›®æ ‡æ©ç \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tgt_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mde_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ç›®æ ‡è¾“å…¥ä»¥BOSå¼€å§‹\u001b[39;00m\n\u001b[1;32m      9\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mmake_src_mask(src, src_pad_idx)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# é€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "sentence = \"A woman is walking on the street.\"\n",
    "transformer.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
    "with torch.no_grad():\n",
    "    # å°†è‡ªå®šä¹‰å¥å­è¿›è¡Œç¼–ç \n",
    "    src = en_tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
    "    \n",
    "    # æ„é€ ç›®æ ‡è¾“å…¥å’Œç›®æ ‡æ©ç \n",
    "    tgt_input = torch.tensor([[de_tokenizer.bos_token_id]], device=device)  # ç›®æ ‡è¾“å…¥ä»¥BOSå¼€å§‹\n",
    "    src_mask = transformer.make_src_mask(src, src_pad_idx)\n",
    "    \n",
    "    # é€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—\n",
    "    for _ in range(100):  # å‡è®¾æœ€å¤§ç”Ÿæˆé•¿åº¦ä¸º100\n",
    "        tgt_mask = transformer.make_trg_mask(tgt_input, tgt_pad_idx)\n",
    "        output = transformer(src, tgt_input, src_mask, tgt_mask)\n",
    "        next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        tgt_input = torch.cat([tgt_input, next_token], dim=-1)\n",
    "        if next_token.item() == de_tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # è·å–é¢„æµ‹ç»“æœ\n",
    "    output_sentence = de_tokenizer.decode(tgt_input[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # æ˜¾ç¤ºåŸå§‹å¥å­å’Œé¢„æµ‹å¥å­\n",
    "    print(\"Source Sentence:\", sentence)\n",
    "    print(\"Predicted Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "è§‚å¯ŸğŸ‘€ï¼š\n",
    "\n",
    "1. é¢„æµ‹çš„å¥å­çš„é¦–å­—æ¯æ€»æ˜¯ç¼ºå°‘ï¼Œæˆ‘çŒœæµ‹æ˜¯GPT2åˆ†è¯å™¨çš„ç¼˜æ•…\n",
    "2. ç¿»è¯‘çš„å¥å­æœ«å°¾æ€»ä¼šæœ‰ä¹±ç ä¸€æ ·çš„å¹»è§‰ï¼Œå¯èƒ½ä¹Ÿæ˜¯åˆ†è¯å™¨ï¼Ÿ\n",
    "\n",
    "æˆ‘å°†ä½¿ç”¨BERTåˆ†è¯å™¨å°è¯•\n",
    "\n",
    "ä½¿ç”¨BERTä¹‹åï¼Œå¥å­å¼€å¤´çš„é¦–å­—æ¯çš„ç¼ºå°‘çš„æƒ…å†µè§£å†³äº†\n",
    "\n",
    "ä½†æ˜¯æœ«å°¾æ€»ä¼šå‡ºç°ä¹±ç ï¼Œå¹¶ä¸”ä½¿ç”¨è‡ªå®šä¹‰source sentenceçš„æ—¶å€™æ€»ä¼šå‡ºç°å¼€å¤´çš„ `##er` è¿™æ˜¯GPT2åˆ†è¯å™¨çš„ç‰¹å¾ï¼Œå¾ˆå¥‡æ€ª\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
